# LangChain Academy æ¨¡å—è¯¦ç»†æ–‡æ¡£

## ğŸ“š æ¨¡å—è¯¦ç»†åˆ†æ

### Module-0: åŸºç¡€è®¾ç½® (Basics)

#### ğŸ¯ å­¦ä¹ ç›®æ ‡
- æŒæ¡ LangChain å’Œ OpenAI API çš„åŸºç¡€é…ç½®
- ç†è§£ Chat Models çš„æ ¸å¿ƒæ¦‚å¿µ
- å­¦ä¹ æ¶ˆæ¯ç±»å‹çš„ä½¿ç”¨æ–¹æ³•
- é…ç½® Tavily æœç´¢å·¥å…·

#### ğŸ“ æ–‡ä»¶ç»“æ„
```
module-0/
â””â”€â”€ basics.ipynb          # åŸºç¡€æ¦‚å¿µå’Œé…ç½®æ•™ç¨‹
```

#### ğŸ”§ æ ¸å¿ƒæŠ€æœ¯è¦ç‚¹
```python
# Chat Model åˆå§‹åŒ–
from langchain_openai import ChatOpenAI

# æ”¯æŒä»£ç†é…ç½®
proxy_url = os.getenv("PROXY")  
llm = ChatOpenAI(
    model="gpt-4o",  # æˆ– gpt-3.5-turbo
    openai_proxy=proxy_url if proxy_url else None
)

# æ¶ˆæ¯ç±»å‹ä½¿ç”¨
from langchain_core.messages import HumanMessage, AIMessage
messages = [HumanMessage(content="Hello, world!")]
response = llm.invoke(messages)
```

#### ğŸ“‹ å­¦ä¹ æ£€æŸ¥æ¸…å•
- [ ] æˆåŠŸé…ç½® OpenAI API å¯†é’¥
- [ ] ç†è§£ HumanMessage å’Œ AIMessage çš„å·®å¼‚
- [ ] æŒæ¡ Chat Model çš„åŸºæœ¬è°ƒç”¨æ–¹å¼
- [ ] é…ç½® Tavily æœç´¢å·¥å…· (ä¸º Module-4 åšå‡†å¤‡)

---

### Module-1: LangGraph å…¥é—¨ (Introduction to LangGraph)

#### ğŸ¯ å­¦ä¹ ç›®æ ‡
- æŒæ¡ StateGraph çš„æ„å»ºå’Œæ‰§è¡Œ
- ç†è§£èŠ‚ç‚¹(Node)å’Œè¾¹(Edge)çš„æ¦‚å¿µ
- å­¦ä¹ æ¡ä»¶è¾¹(Conditional Edge)çš„ä½¿ç”¨
- å®ç°åŸºç¡€ä»£ç†å’Œè®°å¿†ä»£ç†

#### ğŸ“ æ–‡ä»¶ç»“æ„
```
module-1/
â”œâ”€â”€ simple-graph.ipynb     # æœ€ç®€å•çš„3èŠ‚ç‚¹å›¾ç¤ºä¾‹
â”œâ”€â”€ chain.ipynb           # é“¾å¼å›¾ç»“æ„
â”œâ”€â”€ router.ipynb          # è·¯ç”±å›¾å®ç°
â”œâ”€â”€ agent.ipynb           # åŸºç¡€ä»£ç†æ„å»º
â”œâ”€â”€ agent-memory.ipynb    # å¸¦è®°å¿†çš„ä»£ç†
â”œâ”€â”€ deployment.ipynb      # éƒ¨ç½²ç›¸å…³
â”œâ”€â”€ agent_graphs.py       # ä¸»è¦çš„å›¾å®šä¹‰æ–‡ä»¶
â”œâ”€â”€ langgraph.json        # LangGraphé…ç½®æ–‡ä»¶
â””â”€â”€ studio/               # Studioå®è·µé¡¹ç›®
    â”œâ”€â”€ agent.py          # Reactä»£ç†å®ç°
    â”œâ”€â”€ router.py         # è·¯ç”±é€»è¾‘
    â””â”€â”€ simple.py         # ç®€å•å›¾ç¤ºä¾‹
```

#### ğŸ”§ æ ¸å¿ƒæŠ€æœ¯è¦ç‚¹

##### 1. çŠ¶æ€å®šä¹‰
```python
from typing import TypedDict, Annotated
from langchain_core.messages import BaseMessage
from langgraph.graph.message import add

class MessagesState(TypedDict):
    messages: Annotated[list[BaseMessage], add]  # addä½œä¸ºreducer
```

##### 2. å›¾æ„å»ºæ¨¡å¼
```python
from langgraph.graph import StateGraph

# åˆ›å»ºå›¾æ„å»ºå™¨
builder = StateGraph(MessagesState)

# æ·»åŠ èŠ‚ç‚¹
builder.add_node("assistant", assistant_node)
builder.add_node("tools", tool_node)

# æ·»åŠ è¾¹
builder.add_edge("__start__", "assistant")
builder.add_conditional_edges(
    "assistant",
    should_continue,  # æ¡ä»¶å‡½æ•°
    {
        "tools": "tools",
        "__end__": "__end__"
    }
)

# ç¼–è¯‘å›¾
graph = builder.compile()
```

##### 3. è®°å¿†æœºåˆ¶
```python
from langgraph.checkpoint.memory import MemorySaver

# æ·»åŠ è®°å¿†åŠŸèƒ½
memory = MemorySaver()
graph_with_memory = builder.compile(checkpointer=memory)

# ä½¿ç”¨æ—¶æŒ‡å®šçº¿ç¨‹ID
config = {"configurable": {"thread_id": "conversation-1"}}
result = graph_with_memory.invoke(input_data, config)
```

#### ğŸ“Š å®è·µé¡¹ç›®

##### React ä»£ç† (studio/agent.py)
```python
# å·¥å…·å®šä¹‰
def add(a: int, b: int) -> int:
    """Add two integers."""
    return a + b

def multiply(a: int, b: int) -> int:
    """Multiply two integers."""
    return a * b

def divide(a: int, b: int) -> float:
    """Divide two integers."""
    return a / b

tools = [add, multiply, divide]

# ä»£ç†èŠ‚ç‚¹
def assistant(state: MessagesState):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

def should_continue(state: MessagesState) -> Literal["tools", "__end__"]:
    messages = state["messages"]
    last_message = messages[-1]
    if last_message.tool_calls:
        return "tools"
    return "__end__"
```

#### ğŸ“‹ å­¦ä¹ æ£€æŸ¥æ¸…å•
- [ ] ç†è§£ StateGraph çš„åŸºæœ¬ç»“æ„
- [ ] æŒæ¡èŠ‚ç‚¹å’Œè¾¹çš„æ·»åŠ æ–¹æ³•
- [ ] å®ç°æ¡ä»¶è¾¹çš„é€»è¾‘åˆ¤æ–­
- [ ] æˆåŠŸæ„å»ºå¸¦å·¥å…·çš„ React ä»£ç†
- [ ] å®ç°è®°å¿†åŠŸèƒ½å¹¶æµ‹è¯•å¤šè½®å¯¹è¯
- [ ] ä½¿ç”¨ LangGraph Studio å¯è§†åŒ–å›¾æ‰§è¡Œ

---

### Module-2: çŠ¶æ€ç®¡ç†å’Œå†…å­˜ (State Management & Memory)

#### ğŸ¯ å­¦ä¹ ç›®æ ‡
- æŒæ¡ä¸åŒçš„çŠ¶æ€å®šä¹‰æ–¹å¼
- ç†è§£çŠ¶æ€çº¦å‡å™¨(Reducer)çš„ä½œç”¨
- å­¦ä¹ æ¶ˆæ¯ä¿®å‰ªå’Œè¿‡æ»¤æŠ€æœ¯
- å®ç°å¤–éƒ¨è®°å¿†å­˜å‚¨

#### ğŸ“ æ–‡ä»¶ç»“æ„
```
module-2/
â”œâ”€â”€ state-schema.ipynb             # çŠ¶æ€æ¨¡å¼å®šä¹‰æ–¹æ³•
â”œâ”€â”€ state-reducers.ipynb          # çŠ¶æ€çº¦å‡å™¨ä½¿ç”¨
â”œâ”€â”€ multiple-schemas.ipynb        # å¤šçŠ¶æ€æ¨¡å¼ç®¡ç†
â”œâ”€â”€ trim-filter-messages.ipynb   # æ¶ˆæ¯ä¿®å‰ªå’Œè¿‡æ»¤
â”œâ”€â”€ chatbot-summarization.ipynb  # èŠå¤©æœºå™¨äººæ‘˜è¦
â”œâ”€â”€ chatbot-external-memory.ipynb # å¤–éƒ¨è®°å¿†å­˜å‚¨
â”œâ”€â”€ state_db/
â”‚   â””â”€â”€ example.db               # SQLiteç¤ºä¾‹æ•°æ®åº“
â””â”€â”€ studio/
    â”œâ”€â”€ chatbot.py               # å®Œæ•´èŠå¤©æœºå™¨äººå®ç°
    â”œâ”€â”€ langgraph.json
    â””â”€â”€ requirements.txt
```

#### ğŸ”§ æ ¸å¿ƒæŠ€æœ¯è¦ç‚¹

##### 1. ä¸‰ç§çŠ¶æ€å®šä¹‰æ–¹å¼
```python
# æ–¹å¼1: TypedDict (ç®€å•å¿«é€Ÿ)
from typing import TypedDict
class TypedDictState(TypedDict):
    messages: list[BaseMessage]
    user_info: str

# æ–¹å¼2: Dataclass (ç»“æ„åŒ–)
from dataclasses import dataclass
@dataclass
class DataclassState:
    messages: list[BaseMessage]
    user_info: str = ""

# æ–¹å¼3: Pydantic (ç±»å‹éªŒè¯)
from pydantic import BaseModel, Field
class PydanticState(BaseModel):
    messages: list[BaseMessage] = Field(default_factory=list)
    user_info: str = Field(default="", description="ç”¨æˆ·ä¿¡æ¯")
```

##### 2. çŠ¶æ€çº¦å‡å™¨
```python
from typing import Annotated
from langchain_core.messages import BaseMessage
from langgraph.graph.message import add

def add_messages(left: list, right: list) -> list:
    """è‡ªå®šä¹‰æ¶ˆæ¯æ·»åŠ é€»è¾‘"""
    return left + right

class MessagesState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    
# å†…ç½®çº¦å‡å™¨
from langgraph.graph.message import add
messages: Annotated[list[BaseMessage], add]  # é»˜è®¤è¿½åŠ 
```

##### 3. æ¶ˆæ¯ä¿®å‰ªç­–ç•¥
```python
from langchain_core.messages.utils import trim_messages

def trim_messages_node(state: MessagesState):
    """ä¿®å‰ªæ¶ˆæ¯ä»¥æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦"""
    trimmed = trim_messages(
        state["messages"],
        max_tokens=1000,           # æœ€å¤§tokenæ•°
        strategy="last",           # ä¿ç•™æœ€æ–°çš„æ¶ˆæ¯
        token_counter=len,         # tokenè®¡æ•°å‡½æ•°
        include_system=True,       # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
    )
    return {"messages": trimmed}
```

##### 4. å¤–éƒ¨è®°å¿†å­˜å‚¨
```python
import sqlite3
from datetime import datetime

class ExternalMemory:
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.init_db()
    
    def init_db(self):
        conn = sqlite3.connect(self.db_path)
        conn.execute("""
            CREATE TABLE IF NOT EXISTS memories (
                id INTEGER PRIMARY KEY,
                user_id TEXT,
                content TEXT,
                timestamp DATETIME,
                memory_type TEXT
            )
        """)
        conn.commit()
        conn.close()
    
    def save_memory(self, user_id: str, content: str, memory_type: str = "conversation"):
        conn = sqlite3.connect(self.db_path)
        conn.execute(
            "INSERT INTO memories (user_id, content, timestamp, memory_type) VALUES (?, ?, ?, ?)",
            (user_id, content, datetime.now(), memory_type)
        )
        conn.commit()
        conn.close()
```

#### ğŸ“Š å®è·µé¡¹ç›®ç¤ºä¾‹

##### å¸¦æ‘˜è¦çš„èŠå¤©æœºå™¨äºº
```python
def summarize_conversation(state: MessagesState):
    """å¯¹é•¿å¯¹è¯è¿›è¡Œæ‘˜è¦"""
    messages = state["messages"]
    if len(messages) > 10:  # è¶…è¿‡10æ¡æ¶ˆæ¯æ—¶è§¦å‘æ‘˜è¦
        conversation_text = "\n".join([m.content for m in messages])
        summary_prompt = f"è¯·æ€»ç»“ä»¥ä¸‹å¯¹è¯çš„è¦ç‚¹:\n{conversation_text}"
        summary = llm.invoke([AIMessage(content=summary_prompt)])
        
        # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œæ‘˜è¦ï¼Œæ¸…é™¤å†å²
        return {
            "messages": [
                messages[0],  # ç³»ç»Ÿæ¶ˆæ¯
                AIMessage(content=f"å¯¹è¯æ‘˜è¦: {summary.content}")
            ]
        }
    return {"messages": messages}
```

#### ğŸ“‹ å­¦ä¹ æ£€æŸ¥æ¸…å•
- [ ] ç†è§£ä¸‰ç§çŠ¶æ€å®šä¹‰æ–¹å¼çš„ä¼˜ç¼ºç‚¹
- [ ] æŒæ¡è‡ªå®šä¹‰çŠ¶æ€çº¦å‡å™¨çš„å®ç°
- [ ] å®ç°æ¶ˆæ¯ä¿®å‰ªå’Œè¿‡æ»¤é€»è¾‘
- [ ] æ„å»ºå¸¦æ‘˜è¦åŠŸèƒ½çš„èŠå¤©æœºå™¨äºº
- [ ] é›†æˆå¤–éƒ¨æ•°æ®åº“è¿›è¡Œè®°å¿†å­˜å‚¨
- [ ] æµ‹è¯•ä¸åŒçŠ¶æ€æ¨¡å¼çš„æ€§èƒ½å·®å¼‚

---

### Module-3: äººæœºäº¤äº’å¾ªç¯ (Human-in-the-Loop)

#### ğŸ¯ å­¦ä¹ ç›®æ ‡
- æŒæ¡æ–­ç‚¹(Breakpoint)çš„è®¾ç½®å’Œä½¿ç”¨
- å®ç°åŠ¨æ€æ–­ç‚¹æ§åˆ¶
- å­¦ä¹ çŠ¶æ€ç¼–è¾‘å’Œäººå·¥åé¦ˆ
- ç†è§£æµå¼å¤„ç†ä¸­æ–­æœºåˆ¶
- æŒæ¡æ—¶é—´æ—…è¡Œ(Time Travel)è°ƒè¯•

#### ğŸ“ æ–‡ä»¶ç»“æ„
```
module-3/
â”œâ”€â”€ breakpoints.ipynb              # åŸºç¡€æ–­ç‚¹åŠŸèƒ½
â”œâ”€â”€ dynamic-breakpoints.ipynb     # åŠ¨æ€æ–­ç‚¹å®ç°
â”œâ”€â”€ edit-state-human-feedback.ipynb # çŠ¶æ€ç¼–è¾‘å’Œäººå·¥åé¦ˆ
â”œâ”€â”€ streaming-interruption.ipynb  # æµå¼å¤„ç†ä¸­æ–­
â”œâ”€â”€ time-travel.ipynb            # æ—¶é—´æ—…è¡Œè°ƒè¯•
â””â”€â”€ studio/
    â”œâ”€â”€ agent.py                 # æ”¯æŒæ–­ç‚¹çš„ä»£ç†
    â”œâ”€â”€ dynamic_breakpoints.py   # åŠ¨æ€æ–­ç‚¹æ§åˆ¶
    â”œâ”€â”€ langgraph.json
    â””â”€â”€ requirements.txt
```

#### ğŸ”§ æ ¸å¿ƒæŠ€æœ¯è¦ç‚¹

##### 1. åŸºç¡€æ–­ç‚¹è®¾ç½®
```python
from langgraph.checkpoint.memory import MemorySaver

# åœ¨ç‰¹å®šèŠ‚ç‚¹å‰è®¾ç½®æ–­ç‚¹
graph = builder.compile(
    checkpointer=MemorySaver(),
    interrupt_before=["human_feedback"],  # åœ¨äººå·¥åé¦ˆèŠ‚ç‚¹å‰æš‚åœ
    interrupt_after=["critical_decision"] # åœ¨å…³é”®å†³ç­–èŠ‚ç‚¹åæš‚åœ
)

# æ‰§è¡Œåˆ°æ–­ç‚¹
config = {"configurable": {"thread_id": "conversation-1"}}
result = graph.invoke(input_data, config)

# æ¢å¤æ‰§è¡Œ
result = graph.invoke(None, config)  # ä¼ å…¥Noneç»§ç»­æ‰§è¡Œ
```

##### 2. åŠ¨æ€æ–­ç‚¹æ§åˆ¶
```python
def should_interrupt(state: MessagesState) -> bool:
    """åŠ¨æ€åˆ¤æ–­æ˜¯å¦éœ€è¦ä¸­æ–­"""
    last_message = state["messages"][-1]
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦äººå·¥å®¡æ ¸
    if "sensitive" in last_message.content.lower():
        return True
    
    # æ£€æŸ¥ç½®ä¿¡åº¦
    if hasattr(last_message, 'confidence') and last_message.confidence < 0.7:
        return True
    
    return False

def conditional_interrupt_node(state: MessagesState):
    """æ¡ä»¶ä¸­æ–­èŠ‚ç‚¹"""
    if should_interrupt(state):
        # è¿”å›ç‰¹æ®ŠçŠ¶æ€æŒ‡ç¤ºéœ€è¦ä¸­æ–­
        return {"needs_human_review": True, "messages": state["messages"]}
    return {"messages": state["messages"]}
```

##### 3. çŠ¶æ€ç¼–è¾‘å’Œäººå·¥åé¦ˆ
```python
# è·å–å½“å‰çŠ¶æ€
current_state = graph.get_state(config)
print(f"å½“å‰çŠ¶æ€: {current_state.values}")

# ç¼–è¾‘çŠ¶æ€
new_state = {
    "messages": current_state.values["messages"] + [
        HumanMessage(content="äººå·¥ä¿®æ­£: è¯·é‡æ–°ç”Ÿæˆæ›´å‡†ç¡®çš„å›ç­”")
    ]
}

# æ›´æ–°çŠ¶æ€
graph.update_state(config, new_state)

# ç»§ç»­æ‰§è¡Œ
result = graph.invoke(None, config)
```

##### 4. æµå¼å¤„ç†ä¸­æ–­
```python
def streaming_with_interrupt(graph, input_data, config):
    """æ”¯æŒä¸­æ–­çš„æµå¼å¤„ç†"""
    for chunk in graph.stream(input_data, config):
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸­æ–­
        if should_interrupt_streaming(chunk):
            print("æ£€æµ‹åˆ°éœ€è¦äººå·¥å¹²é¢„ï¼Œæš‚åœæµå¼å¤„ç†...")
            break
        
        # å¤„ç†æ•°æ®å—
        yield chunk
        
        # æ£€æŸ¥ç”¨æˆ·è¾“å…¥
        if user_requested_stop():
            print("ç”¨æˆ·è¯·æ±‚åœæ­¢ï¼Œä¸­æ–­å¤„ç†...")
            break

def user_requested_stop():
    """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦è¯·æ±‚åœæ­¢ (ç®€åŒ–ç¤ºä¾‹)"""
    # å®é™…å®ç°ä¸­å¯èƒ½æ£€æŸ¥æ–‡ä»¶ã€æ•°æ®åº“æˆ–æ¶ˆæ¯é˜Ÿåˆ—
    return False
```

##### 5. æ—¶é—´æ—…è¡Œè°ƒè¯•
```python
# è·å–çŠ¶æ€å†å²
state_history = graph.get_state_history(config)

print("çŠ¶æ€å†å²:")
for i, state_snapshot in enumerate(state_history):
    print(f"æ­¥éª¤ {i}: {state_snapshot.config}")
    print(f"å€¼: {state_snapshot.values}")
    print(f"ä¸‹ä¸€æ­¥: {state_snapshot.next}")
    print("---")

# å›æ»šåˆ°ç‰¹å®šçŠ¶æ€
target_config = list(state_history)[2].config  # å›æ»šåˆ°ç¬¬3ä¸ªçŠ¶æ€
graph.update_state(target_config, values=None)  # å›æ»šæ“ä½œ

# ä»å›æ»šç‚¹ç»§ç»­æ‰§è¡Œ
result = graph.invoke(None, target_config)
```

#### ğŸ“Š å®è·µé¡¹ç›®ç¤ºä¾‹

##### æ™ºèƒ½å®¢æœä»£ç† (æ”¯æŒäººå·¥ä»‹å…¥)
```python
class CustomerServiceAgent:
    def __init__(self):
        self.graph = self.build_graph()
    
    def build_graph(self):
        builder = StateGraph(MessagesState)
        
        # æ·»åŠ èŠ‚ç‚¹
        builder.add_node("analyze_query", self.analyze_query)
        builder.add_node("auto_respond", self.auto_respond)
        builder.add_node("human_handoff", self.human_handoff)
        builder.add_node("quality_check", self.quality_check)
        
        # æ·»åŠ è¾¹å’Œæ¡ä»¶è¾¹
        builder.add_edge("__start__", "analyze_query")
        builder.add_conditional_edges(
            "analyze_query",
            self.should_handoff_to_human,
            {
                "human": "human_handoff",
                "auto": "auto_respond"
            }
        )
        builder.add_edge("auto_respond", "quality_check")
        builder.add_conditional_edges(
            "quality_check",
            self.quality_check_passed,
            {
                "pass": "__end__",
                "review": "human_handoff"
            }
        )
        
        return builder.compile(
            checkpointer=MemorySaver(),
            interrupt_before=["human_handoff"]  # äººå·¥ä»‹å…¥å‰æš‚åœ
        )
    
    def should_handoff_to_human(self, state: MessagesState):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦äººå·¥ä»‹å…¥"""
        query = state["messages"][-1].content
        
        # å¤æ‚æŸ¥è¯¢æ£€æµ‹
        if len(query.split()) > 50:
            return "human"
        
        # æƒ…æ„Ÿåˆ†æ - æ£€æµ‹è´Ÿé¢æƒ…ç»ª
        if any(word in query.lower() for word in ["angry", "frustrated", "complaint"]):
            return "human"
        
        # æŠ€æœ¯é—®é¢˜æ£€æµ‹
        if any(word in query.lower() for word in ["bug", "error", "not working"]):
            return "human"
        
        return "auto"
```

#### ğŸ“‹ å­¦ä¹ æ£€æŸ¥æ¸…å•
- [ ] æŒæ¡åŸºç¡€æ–­ç‚¹çš„è®¾ç½®å’Œæ¢å¤
- [ ] å®ç°åŠ¨æ€æ–­ç‚¹é€»è¾‘
- [ ] å­¦ä¼šçŠ¶æ€ç¼–è¾‘å’Œæ›´æ–°æ“ä½œ
- [ ] æ„å»ºæ”¯æŒäººå·¥ä»‹å…¥çš„å·¥ä½œæµ
- [ ] å®ç°æµå¼å¤„ç†çš„ä¸­æ–­æœºåˆ¶
- [ ] æŒæ¡æ—¶é—´æ—…è¡Œè°ƒè¯•æŠ€æœ¯
- [ ] æµ‹è¯•å¤æ‚çš„äººæœºäº¤äº’åœºæ™¯

---

### Module-4: å¹¶è¡Œå¤„ç†å’Œå­å›¾ (Parallelization & Sub-graphs)

#### ğŸ¯ å­¦ä¹ ç›®æ ‡
- æŒæ¡ Map-Reduce è®¾è®¡æ¨¡å¼
- å®ç°ä»»åŠ¡å¹¶è¡ŒåŒ–å¤„ç†
- å­¦ä¹ å­å›¾çš„æ„å»ºå’Œç®¡ç†
- æ„å»ºå¤æ‚çš„ç ”ç©¶åŠ©æ‰‹åº”ç”¨

#### ğŸ“ æ–‡ä»¶ç»“æ„
```
module-4/
â”œâ”€â”€ map-reduce.ipynb           # Map-Reduceæ¨¡å¼å®ç°
â”œâ”€â”€ parallelization.ipynb     # å¹¶è¡Œå¤„ç†æŠ€æœ¯
â”œâ”€â”€ sub-graph.ipynb          # å­å›¾æ„å»ºå’Œç®¡ç†
â”œâ”€â”€ research-assistant.ipynb # ç ”ç©¶åŠ©æ‰‹ç»¼åˆåº”ç”¨
â””â”€â”€ studio/
    â”œâ”€â”€ map_reduce.py         # Map-Reduceå®ç°
    â”œâ”€â”€ parallelization.py    # å¹¶è¡Œå¤„ç†é€»è¾‘
    â”œâ”€â”€ sub_graphs.py         # å­å›¾å®šä¹‰
    â”œâ”€â”€ research_assistant.py # å®Œæ•´ç ”ç©¶åŠ©æ‰‹
    â”œâ”€â”€ langgraph.json
    â””â”€â”€ requirements.txt
```

#### ğŸ”§ æ ¸å¿ƒæŠ€æœ¯è¦ç‚¹

##### 1. Map-Reduce æ¨¡å¼
```python
from functools import reduce
from concurrent.futures import ThreadPoolExecutor

class MapReduceState(TypedDict):
    inputs: list[str]           # è¾“å…¥æ•°æ®åˆ—è¡¨
    mapped_results: list[dict]  # Mapé˜¶æ®µç»“æœ
    final_result: dict          # Reduceé˜¶æ®µç»“æœ

def map_step(state: MapReduceState):
    """Mapé˜¶æ®µï¼šå¹¶è¡Œå¤„ç†æ¯ä¸ªè¾“å…¥"""
    inputs = state["inputs"]
    
    def process_single_input(input_item):
        # å¯¹å•ä¸ªè¾“å…¥è¿›è¡Œå¤„ç†
        return {
            "input": input_item,
            "processed": llm.invoke([HumanMessage(content=f"åˆ†æ: {input_item}")]).content,
            "timestamp": datetime.now().isoformat()
        }
    
    # å¹¶è¡Œå¤„ç†
    with ThreadPoolExecutor(max_workers=4) as executor:
        mapped_results = list(executor.map(process_single_input, inputs))
    
    return {"mapped_results": mapped_results}

def reduce_step(state: MapReduceState):
    """Reduceé˜¶æ®µï¼šåˆå¹¶æ‰€æœ‰ç»“æœ"""
    results = state["mapped_results"]
    
    # åˆå¹¶æ‰€æœ‰åˆ†æç»“æœ
    combined_analysis = "\n".join([r["processed"] for r in results])
    
    # ç”Ÿæˆæœ€ç»ˆæ€»ç»“
    summary_prompt = f"è¯·æ€»ç»“ä»¥ä¸‹åˆ†æç»“æœ:\n{combined_analysis}"
    final_summary = llm.invoke([HumanMessage(content=summary_prompt)]).content
    
    return {
        "final_result": {
            "summary": final_summary,
            "individual_results": results,
            "total_processed": len(results)
        }
    }
```

##### 2. å¹¶è¡ŒèŠ‚ç‚¹æ‰§è¡Œ
```python
from langgraph.graph import StateGraph

def build_parallel_graph():
    builder = StateGraph(MapReduceState)
    
    # æ·»åŠ èŠ‚ç‚¹
    builder.add_node("input_preparation", prepare_inputs)
    builder.add_node("map_process", map_step)
    builder.add_node("reduce_process", reduce_step)
    
    # ä¸²è¡Œæ‰§è¡Œ
    builder.add_edge("__start__", "input_preparation")
    builder.add_edge("input_preparation", "map_process")
    builder.add_edge("map_process", "reduce_process")
    builder.add_edge("reduce_process", "__end__")
    
    return builder.compile()

# ä½¿ç”¨å¹¶è¡Œå¤„ç†å·¥å…·
from langgraph.prebuilt import ToolNode
from concurrent.futures import as_completed

def parallel_tool_execution(state: MessagesState):
    """å¹¶è¡Œæ‰§è¡Œå¤šä¸ªå·¥å…·"""
    tools_to_execute = state.get("pending_tools", [])
    
    results = []
    with ThreadPoolExecutor(max_workers=3) as executor:
        # æäº¤æ‰€æœ‰å·¥å…·æ‰§è¡Œä»»åŠ¡
        future_to_tool = {
            executor.submit(execute_tool, tool): tool 
            for tool in tools_to_execute
        }
        
        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_tool):
            tool = future_to_tool[future]
            try:
                result = future.result()
                results.append({"tool": tool, "result": result})
            except Exception as exc:
                results.append({"tool": tool, "error": str(exc)})
    
    return {"tool_results": results}
```

##### 3. å­å›¾æ„å»ºå’Œç»„åˆ
```python
def build_analysis_subgraph():
    """æ„å»ºåˆ†æå­å›¾"""
    sub_builder = StateGraph(AnalysisState)
    
    sub_builder.add_node("data_extraction", extract_data)
    sub_builder.add_node("data_analysis", analyze_data)
    sub_builder.add_node("result_formatting", format_results)
    
    sub_builder.add_edge("__start__", "data_extraction")
    sub_builder.add_edge("data_extraction", "data_analysis")
    sub_builder.add_edge("data_analysis", "result_formatting")
    sub_builder.add_edge("result_formatting", "__end__")
    
    return sub_builder.compile()

def build_main_graph():
    """æ„å»ºä¸»å›¾ï¼Œé›†æˆå¤šä¸ªå­å›¾"""
    main_builder = StateGraph(MainState)
    
    # åˆ›å»ºå­å›¾å®ä¾‹
    analysis_subgraph = build_analysis_subgraph()
    
    # æ·»åŠ å­å›¾ä½œä¸ºèŠ‚ç‚¹
    main_builder.add_node("analysis", analysis_subgraph)
    main_builder.add_node("preprocessing", preprocess_data)
    main_builder.add_node("postprocessing", postprocess_results)
    
    main_builder.add_edge("__start__", "preprocessing")
    main_builder.add_edge("preprocessing", "analysis")
    main_builder.add_edge("analysis", "postprocessing")
    main_builder.add_edge("postprocessing", "__end__")
    
    return main_builder.compile()
```

##### 4. ç ”ç©¶åŠ©æ‰‹åº”ç”¨æ¶æ„
```python
class ResearchAssistant:
    """å¤šä»£ç†ç ”ç©¶åŠ©æ‰‹ç³»ç»Ÿ"""
    
    def __init__(self):
        self.graph = self.build_research_graph()
    
    def build_research_graph(self):
        builder = StateGraph(ResearchState)
        
        # ç ”ç©¶è§„åˆ’é˜¶æ®µ
        builder.add_node("plan_research", self.plan_research)
        builder.add_node("create_analysts", self.create_analysts)
        
        # å¹¶è¡Œç ”ç©¶é˜¶æ®µ
        builder.add_node("conduct_interviews", self.conduct_interviews)
        builder.add_node("gather_information", self.gather_information)
        
        # ç»“æœåˆæˆé˜¶æ®µ
        builder.add_node("synthesize_findings", self.synthesize_findings)
        builder.add_node("generate_report", self.generate_report)
        
        # è¿æ¥èŠ‚ç‚¹
        builder.add_edge("__start__", "plan_research")
        builder.add_edge("plan_research", "create_analysts")
        
        # å¹¶è¡Œæ‰§è¡Œç ”ç©¶ä»»åŠ¡
        builder.add_edge("create_analysts", "conduct_interviews")
        builder.add_edge("create_analysts", "gather_information")
        
        # ç­‰å¾…å¹¶è¡Œä»»åŠ¡å®Œæˆååˆæˆç»“æœ
        builder.add_edge(["conduct_interviews", "gather_information"], "synthesize_findings")
        builder.add_edge("synthesize_findings", "generate_report")
        builder.add_edge("generate_report", "__end__")
        
        return builder.compile()
    
    def create_analysts(self, state: ResearchState):
        """åˆ›å»ºAIåˆ†æå¸ˆå›¢é˜Ÿ"""
        research_topic = state["research_topic"]
        
        # åˆ†è§£ç ”ç©¶ä¸»é¢˜ä¸ºå­ä¸»é¢˜
        subtopics_prompt = f"""
        å°†ç ”ç©¶ä¸»é¢˜ "{research_topic}" åˆ†è§£ä¸º3-5ä¸ªå…·ä½“çš„å­ä¸»é¢˜ï¼Œ
        æ¯ä¸ªå­ä¸»é¢˜éœ€è¦ä¸€ä¸ªä¸“é—¨çš„åˆ†æå¸ˆæ¥ç ”ç©¶ã€‚
        """
        
        subtopics_response = llm.invoke([HumanMessage(content=subtopics_prompt)])
        subtopics = self.parse_subtopics(subtopics_response.content)
        
        # ä¸ºæ¯ä¸ªå­ä¸»é¢˜åˆ›å»ºä¸“é—¨çš„åˆ†æå¸ˆ
        analysts = []
        for subtopic in subtopics:
            analyst = {
                "id": f"analyst_{len(analysts)}",
                "specialization": subtopic,
                "instructions": f"ä½ æ˜¯ä¸“é—¨ç ”ç©¶ {subtopic} çš„AIåˆ†æå¸ˆ...",
                "tools": ["web_search", "document_analysis", "data_extraction"]
            }
            analysts.append(analyst)
        
        return {"analysts": analysts, "subtopics": subtopics}
    
    def conduct_interviews(self, state: ResearchState):
        """å¹¶è¡Œè¿›è¡Œä¸“å®¶è®¿è°ˆ"""
        analysts = state["analysts"]
        
        def interview_expert(analyst):
            """å•ä¸ªåˆ†æå¸ˆè¿›è¡Œä¸“å®¶è®¿è°ˆ"""
            interview_prompt = f"""
            ä½œä¸º {analyst['specialization']} çš„ä¸“ä¸šåˆ†æå¸ˆï¼Œ
            è¯·å°±è¿™ä¸ªä¸»é¢˜è¿›è¡Œæ·±å…¥çš„ä¸“å®¶çº§åˆ†æ...
            """
            
            expert_response = llm.invoke([HumanMessage(content=interview_prompt)])
            
            return {
                "analyst_id": analyst["id"],
                "specialization": analyst["specialization"],
                "interview_result": expert_response.content,
                "confidence_score": 0.85  # æ¨¡æ‹Ÿç½®ä¿¡åº¦è¯„åˆ†
            }
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰è®¿è°ˆ
        with ThreadPoolExecutor(max_workers=len(analysts)) as executor:
            interview_results = list(executor.map(interview_expert, analysts))
        
        return {"interview_results": interview_results}
```

#### ğŸ“Š æ ¸å¿ƒåº”ç”¨ï¼šæ™ºèƒ½ç ”ç©¶åŠ©æ‰‹

##### åŠŸèƒ½ç‰¹æ€§
1. **åŠ¨æ€å›¢é˜Ÿç”Ÿæˆ**: æ ¹æ®ç ”ç©¶ä¸»é¢˜è‡ªåŠ¨ç”ŸæˆAIåˆ†æå¸ˆå›¢é˜Ÿ
2. **å¹¶è¡Œä¿¡æ¯æ”¶é›†**: å¤šä¸ªåˆ†æå¸ˆåŒæ—¶è¿›è¡Œç ”ç©¶
3. **ä¸“å®¶çº§è®¿è°ˆ**: æ¯ä¸ªåˆ†æå¸ˆä¸ä¸“å®¶AIè¿›è¡Œæ·±åº¦å¯¹è¯
4. **æ™ºèƒ½åˆæˆ**: å°†å¤šä¸ªç ”ç©¶ç»“æœåˆæˆä¸ºç»Ÿä¸€æŠ¥å‘Š
5. **å¯å®šåˆ¶è¾“å‡º**: æ”¯æŒå¤šç§æŠ¥å‘Šæ ¼å¼å’Œè¯¦ç»†ç¨‹åº¦

##### ä½¿ç”¨ç¤ºä¾‹
```python
# å¯åŠ¨ç ”ç©¶åŠ©æ‰‹
research_topic = "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„åº”ç”¨ç°çŠ¶å’Œæœªæ¥è¶‹åŠ¿"
config = {"configurable": {"thread_id": "research_session_1"}}

result = research_assistant.invoke({
    "research_topic": research_topic,
    "output_format": "detailed_report",
    "required_sections": ["current_state", "challenges", "opportunities", "future_trends"]
}, config)

print(result["final_report"])
```

#### ğŸ“‹ å­¦ä¹ æ£€æŸ¥æ¸…å•
- [ ] ç†è§£ Map-Reduce æ¨¡å¼çš„åŸç†å’Œåº”ç”¨
- [ ] å®ç°ä»»åŠ¡çš„å¹¶è¡ŒåŒ–å¤„ç†
- [ ] æŒæ¡å­å›¾çš„æ„å»ºå’Œç»„åˆæŠ€æœ¯
- [ ] æ„å»ºå®Œæ•´çš„ç ”ç©¶åŠ©æ‰‹åº”ç”¨
- [ ] æµ‹è¯•å¹¶è¡Œå¤„ç†çš„æ€§èƒ½ä¼˜åŠ¿
- [ ] ä¼˜åŒ–å¤šä»£ç†åä½œçš„æ•ˆç‡
- [ ] å¤„ç†å¹¶å‘æ‰§è¡Œä¸­çš„é”™è¯¯å’Œå¼‚å¸¸

---

### Module-5: é•¿æœŸè®°å¿†ç³»ç»Ÿ (Long-term Memory)

#### ğŸ¯ å­¦ä¹ ç›®æ ‡
- å®ç°è¯­ä¹‰è®°å¿†å­˜å‚¨å’Œæ£€ç´¢
- æŒæ¡ç”¨æˆ·æ¡£æ¡ˆ(Profile)ç®¡ç†
- å­¦ä¹ è®°å¿†é›†åˆ(Collection)çš„ä½¿ç”¨
- æ„å»ºå…·æœ‰é•¿æœŸè®°å¿†çš„æ™ºèƒ½ä»£ç†

#### ğŸ“ æ–‡ä»¶ç»“æ„
```
module-5/
â”œâ”€â”€ memory_store.ipynb              # è®°å¿†å­˜å‚¨åŸºç¡€
â”œâ”€â”€ memoryschema_profile.ipynb     # ç”¨æˆ·æ¡£æ¡ˆè®°å¿†æ¨¡å¼
â”œâ”€â”€ memoryschema_collection.ipynb  # é›†åˆè®°å¿†æ¨¡å¼
â”œâ”€â”€ memory_agent.ipynb             # è®°å¿†ä»£ç†å®ç°
â””â”€â”€ studio/
    â”œâ”€â”€ memory_store.py             # è®°å¿†å­˜å‚¨å®ç°
    â”œâ”€â”€ memoryschema_profile.py     # æ¡£æ¡ˆæ¨¡å¼å®šä¹‰
    â”œâ”€â”€ memoryschema_collection.py  # é›†åˆæ¨¡å¼å®šä¹‰
    â”œâ”€â”€ memory_agent.py             # å®Œæ•´è®°å¿†ä»£ç†
    â”œâ”€â”€ configuration.py            # é…ç½®ç®¡ç†
    â”œâ”€â”€ langgraph.json
    â””â”€â”€ requirements.txt
```

#### ğŸ”§ æ ¸å¿ƒæŠ€æœ¯è¦ç‚¹

##### 1. è®°å¿†å­˜å‚¨åŸºç¡€æ¶æ„
```python
from langchain_core.memory import BaseMemory
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
import json
from datetime import datetime
from typing import Dict, List, Any

class SemanticMemoryStore:
    """è¯­ä¹‰è®°å¿†å­˜å‚¨ç³»ç»Ÿ"""
    
    def __init__(self, embedding_model="text-embedding-ada-002"):
        self.embeddings = OpenAIEmbeddings(model=embedding_model)
        self.vector_store = None
        self.metadata_store = {}  # å­˜å‚¨è®°å¿†çš„å…ƒæ•°æ®
        
    def initialize_store(self, memory_documents: List[str] = None):
        """åˆå§‹åŒ–å‘é‡å­˜å‚¨"""
        if memory_documents:
            self.vector_store = FAISS.from_texts(
                memory_documents, 
                self.embeddings
            )
        else:
            # åˆ›å»ºç©ºçš„å‘é‡å­˜å‚¨
            self.vector_store = FAISS.from_texts(
                ["åˆå§‹åŒ–æ–‡æ¡£"], 
                self.embeddings
            )
    
    def save_memory(self, content: str, memory_type: str, metadata: Dict[str, Any] = None):
        """ä¿å­˜è®°å¿†åˆ°å‘é‡å­˜å‚¨"""
        memory_id = f"memory_{datetime.now().timestamp()}"
        
        # ä¿å­˜åˆ°å‘é‡å­˜å‚¨
        self.vector_store.add_texts(
            [content],
            metadatas=[{
                "memory_id": memory_id,
                "memory_type": memory_type,
                "timestamp": datetime.now().isoformat(),
                **(metadata or {})
            }]
        )
        
        # ä¿å­˜è¯¦ç»†å…ƒæ•°æ®
        self.metadata_store[memory_id] = {
            "content": content,
            "memory_type": memory_type,
            "timestamp": datetime.now().isoformat(),
            "metadata": metadata or {}
        }
        
        return memory_id
    
    def retrieve_memories(self, query: str, k: int = 5, memory_type: str = None):
        """æ£€ç´¢ç›¸å…³è®°å¿†"""
        # è¯­ä¹‰æœç´¢
        results = self.vector_store.similarity_search_with_score(query, k=k)
        
        # è¿‡æ»¤è®°å¿†ç±»å‹
        if memory_type:
            results = [
                (doc, score) for doc, score in results
                if doc.metadata.get("memory_type") == memory_type
            ]
        
        return results
```

##### 2. ç”¨æˆ·æ¡£æ¡ˆè®°å¿†æ¨¡å¼
```python
from pydantic import BaseModel, Field
from typing import Dict, List, Optional
from enum import Enum

class ProfileMemoryType(str, Enum):
    PREFERENCE = "preference"      # ç”¨æˆ·åå¥½
    BEHAVIOR = "behavior"         # è¡Œä¸ºæ¨¡å¼
    BACKGROUND = "background"     # èƒŒæ™¯ä¿¡æ¯
    SKILL = "skill"              # æŠ€èƒ½å’Œä¸“é•¿
    GOAL = "goal"                # ç›®æ ‡å’Œæ„å›¾

class UserProfile(BaseModel):
    """ç”¨æˆ·æ¡£æ¡ˆæ•°æ®æ¨¡å‹"""
    user_id: str = Field(description="ç”¨æˆ·å”¯ä¸€æ ‡è¯†")
    name: Optional[str] = Field(default=None, description="ç”¨æˆ·å§“å")
    preferences: Dict[str, Any] = Field(default_factory=dict, description="ç”¨æˆ·åå¥½")
    skills: List[str] = Field(default_factory=list, description="ç”¨æˆ·æŠ€èƒ½")
    goals: List[str] = Field(default_factory=list, description="ç”¨æˆ·ç›®æ ‡")
    behavior_patterns: Dict[str, Any] = Field(default_factory=dict, description="è¡Œä¸ºæ¨¡å¼")
    background_info: Dict[str, Any] = Field(default_factory=dict, description="èƒŒæ™¯ä¿¡æ¯")
    last_updated: str = Field(default_factory=lambda: datetime.now().isoformat())

class ProfileMemoryManager:
    """ç”¨æˆ·æ¡£æ¡ˆè®°å¿†ç®¡ç†å™¨"""
    
    def __init__(self, memory_store: SemanticMemoryStore):
        self.memory_store = memory_store
        self.profiles: Dict[str, UserProfile] = {}
    
    def update_profile(self, user_id: str, update_data: Dict[str, Any], 
                      memory_type: ProfileMemoryType):
        """æ›´æ–°ç”¨æˆ·æ¡£æ¡ˆ"""
        if user_id not in self.profiles:
            self.profiles[user_id] = UserProfile(user_id=user_id)
        
        profile = self.profiles[user_id]
        
        # æ ¹æ®è®°å¿†ç±»å‹æ›´æ–°ä¸åŒå­—æ®µ
        if memory_type == ProfileMemoryType.PREFERENCE:
            profile.preferences.update(update_data)
        elif memory_type == ProfileMemoryType.SKILL:
            new_skills = update_data.get("skills", [])
            profile.skills.extend([s for s in new_skills if s not in profile.skills])
        elif memory_type == ProfileMemoryType.GOAL:
            new_goals = update_data.get("goals", [])
            profile.goals.extend([g for g in new_goals if g not in profile.goals])
        elif memory_type == ProfileMemoryType.BEHAVIOR:
            profile.behavior_patterns.update(update_data)
        elif memory_type == ProfileMemoryType.BACKGROUND:
            profile.background_info.update(update_data)
        
        profile.last_updated = datetime.now().isoformat()
        
        # ä¿å­˜åˆ°è®°å¿†å­˜å‚¨
        memory_content = f"ç”¨æˆ· {user_id} çš„ {memory_type.value} ä¿¡æ¯: {json.dumps(update_data, ensure_ascii=False)}"
        self.memory_store.save_memory(
            content=memory_content,
            memory_type=f"profile_{memory_type.value}",
            metadata={
                "user_id": user_id,
                "profile_field": memory_type.value
            }
        )
    
    def get_profile(self, user_id: str) -> Optional[UserProfile]:
        """è·å–ç”¨æˆ·æ¡£æ¡ˆ"""
        return self.profiles.get(user_id)
    
    def get_relevant_profile_info(self, user_id: str, context: str) -> Dict[str, Any]:
        """æ ¹æ®ä¸Šä¸‹æ–‡è·å–ç›¸å…³çš„æ¡£æ¡ˆä¿¡æ¯"""
        # æ£€ç´¢ç›¸å…³è®°å¿†
        memories = self.memory_store.retrieve_memories(
            query=context,
            k=5,
            memory_type=f"profile_"
        )
        
        # è¿‡æ»¤å½“å‰ç”¨æˆ·çš„è®°å¿†
        user_memories = [
            (doc, score) for doc, score in memories
            if doc.metadata.get("user_id") == user_id
        ]
        
        relevant_info = {
            "preferences": {},
            "skills": [],
            "goals": [],
            "behavior_patterns": {},
            "background_info": {}
        }
        
        for doc, score in user_memories:
            if score < 0.8:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                profile_field = doc.metadata.get("profile_field")
                if profile_field in relevant_info:
                    # è§£æè®°å¿†å†…å®¹å¹¶æ·»åŠ åˆ°ç›¸å…³ä¿¡æ¯ä¸­
                    try:
                        content_data = json.loads(doc.page_content.split(": ", 1)[1])
                        if isinstance(relevant_info[profile_field], dict):
                            relevant_info[profile_field].update(content_data)
                        elif isinstance(relevant_info[profile_field], list):
                            relevant_info[profile_field].extend(content_data.get(profile_field, []))
                    except:
                        pass
        
        return relevant_info
```

##### 3. é›†åˆè®°å¿†æ¨¡å¼
```python
class CollectionMemoryManager:
    """é›†åˆè®°å¿†ç®¡ç†å™¨ - ç®¡ç†ç›¸å…³è®°å¿†çš„é›†åˆ"""
    
    def __init__(self, memory_store: SemanticMemoryStore):
        self.memory_store = memory_store
        self.collections: Dict[str, Dict[str, Any]] = {}
    
    def create_collection(self, collection_id: str, description: str, 
                         tags: List[str] = None) -> str:
        """åˆ›å»ºè®°å¿†é›†åˆ"""
        self.collections[collection_id] = {
            "description": description,
            "tags": tags or [],
            "memory_ids": [],
            "created_at": datetime.now().isoformat(),
            "last_accessed": datetime.now().isoformat()
        }
        
        # å°†é›†åˆä¿¡æ¯ä¿å­˜åˆ°è®°å¿†å­˜å‚¨
        collection_content = f"è®°å¿†é›†åˆ: {description}"
        memory_id = self.memory_store.save_memory(
            content=collection_content,
            memory_type="collection_meta",
            metadata={
                "collection_id": collection_id,
                "tags": tags or []
            }
        )
        
        return collection_id
    
    def add_to_collection(self, collection_id: str, content: str, 
                         content_type: str = "general"):
        """å‘é›†åˆæ·»åŠ è®°å¿†"""
        if collection_id not in self.collections:
            raise ValueError(f"é›†åˆ {collection_id} ä¸å­˜åœ¨")
        
        # ä¿å­˜è®°å¿†å¹¶å…³è”åˆ°é›†åˆ
        memory_id = self.memory_store.save_memory(
            content=content,
            memory_type=f"collection_item_{content_type}",
            metadata={
                "collection_id": collection_id,
                "content_type": content_type
            }
        )
        
        # æ›´æ–°é›†åˆ
        self.collections[collection_id]["memory_ids"].append(memory_id)
        self.collections[collection_id]["last_accessed"] = datetime.now().isoformat()
        
        return memory_id
    
    def query_collection(self, collection_id: str, query: str, k: int = 5):
        """æŸ¥è¯¢é›†åˆä¸­çš„ç›¸å…³è®°å¿†"""
        if collection_id not in self.collections:
            return []
        
        # æ£€ç´¢é›†åˆä¸­çš„è®°å¿†
        all_memories = self.memory_store.retrieve_memories(query, k=k*2)
        
        # è¿‡æ»¤å±äºæŒ‡å®šé›†åˆçš„è®°å¿†
        collection_memories = [
            (doc, score) for doc, score in all_memories
            if doc.metadata.get("collection_id") == collection_id
        ]
        
        # æ›´æ–°è®¿é—®æ—¶é—´
        self.collections[collection_id]["last_accessed"] = datetime.now().isoformat()
        
        return collection_memories[:k]
    
    def get_collection_summary(self, collection_id: str) -> Dict[str, Any]:
        """è·å–é›†åˆæ‘˜è¦"""
        if collection_id not in self.collections:
            return {}
        
        collection = self.collections[collection_id]
        
        # è·å–é›†åˆä¸­çš„æ‰€æœ‰è®°å¿†
        memories = self.query_collection(collection_id, collection["description"], k=20)
        
        # ç”Ÿæˆæ‘˜è¦
        if memories:
            memory_contents = [doc.page_content for doc, _ in memories]
            summary_prompt = f"""
            è¯·æ€»ç»“ä»¥ä¸‹è®°å¿†é›†åˆçš„ä¸»è¦å†…å®¹:
            é›†åˆæè¿°: {collection['description']}
            è®°å¿†å†…å®¹: {' | '.join(memory_contents[:10])}  # é™åˆ¶é•¿åº¦
            """
            
            summary = llm.invoke([HumanMessage(content=summary_prompt)]).content
        else:
            summary = "é›†åˆä¸­æš‚æ— è®°å¿†å†…å®¹"
        
        return {
            "collection_id": collection_id,
            "description": collection["description"],
            "memory_count": len(collection["memory_ids"]),
            "tags": collection["tags"],
            "summary": summary,
            "created_at": collection["created_at"],
            "last_accessed": collection["last_accessed"]
        }
```

##### 4. æ™ºèƒ½è®°å¿†ä»£ç†
```python
from trustcall import create_tool

class MemoryAgent:
    """å…·æœ‰é•¿æœŸè®°å¿†èƒ½åŠ›çš„æ™ºèƒ½ä»£ç†"""
    
    def __init__(self):
        self.memory_store = SemanticMemoryStore()
        self.profile_manager = ProfileMemoryManager(self.memory_store)
        self.collection_manager = CollectionMemoryManager(self.memory_store)
        self.graph = self.build_memory_graph()
        
        # åˆå§‹åŒ–è®°å¿†å­˜å‚¨
        self.memory_store.initialize_store()
    
    def build_memory_graph(self):
        """æ„å»ºå¸¦è®°å¿†åŠŸèƒ½çš„å›¾"""
        
        # å®šä¹‰è®°å¿†å·¥å…·
        @create_tool
        def save_user_preference(preference_type: str, preference_value: str, user_id: str = "default") -> str:
            """ä¿å­˜ç”¨æˆ·åå¥½ä¿¡æ¯"""
            self.profile_manager.update_profile(
                user_id, 
                {preference_type: preference_value},
                ProfileMemoryType.PREFERENCE
            )
            return f"å·²ä¿å­˜ç”¨æˆ·åå¥½: {preference_type} = {preference_value}"
        
        @create_tool
        def recall_memories(query: str, user_id: str = "default") -> str:
            """å›å¿†ç›¸å…³è®°å¿†"""
            memories = self.memory_store.retrieve_memories(query, k=3)
            if memories:
                recalled = [f"è®°å¿† {i+1}: {doc.page_content}" for i, (doc, score) in enumerate(memories)]
                return "\\n".join(recalled)
            return "æœªæ‰¾åˆ°ç›¸å…³è®°å¿†"
        
        @create_tool
        def create_memory_collection(name: str, description: str) -> str:
            """åˆ›å»ºè®°å¿†é›†åˆ"""
            collection_id = self.collection_manager.create_collection(name, description)
            return f"å·²åˆ›å»ºè®°å¿†é›†åˆ: {collection_id}"
        
        tools = [save_user_preference, recall_memories, create_memory_collection]
        
        # æ„å»ºå›¾
        builder = StateGraph(MessagesState)
        
        builder.add_node("memory_assistant", self.memory_assistant_node)
        builder.add_node("tools", ToolNode(tools))
        builder.add_node("memory_decision", self.memory_decision_node)
        
        builder.add_edge("__start__", "memory_assistant")
        builder.add_conditional_edges(
            "memory_assistant",
            self.should_use_tools,
            {
                "tools": "tools",
                "memory": "memory_decision",
                "end": "__end__"
            }
        )
        builder.add_edge("tools", "memory_decision")
        builder.add_edge("memory_decision", "__end__")
        
        return builder.compile(checkpointer=MemorySaver())
    
    def memory_assistant_node(self, state: MessagesState):
        """è®°å¿†åŠ©æ‰‹èŠ‚ç‚¹"""
        messages = state["messages"]
        last_message = messages[-1]
        
        # æ£€ç´¢ç›¸å…³è®°å¿†æ¥å¢å¼ºå›ç­”
        relevant_memories = self.memory_store.retrieve_memories(
            last_message.content, k=3
        )
        
        # æ„å»ºåŒ…å«è®°å¿†ä¸Šä¸‹æ–‡çš„ç³»ç»Ÿæ¶ˆæ¯
        memory_context = ""
        if relevant_memories:
            memory_context = "\\n".join([
                f"ç›¸å…³è®°å¿†: {doc.page_content}" 
                for doc, score in relevant_memories[:2]
            ])
        
        system_message = SystemMessage(content=f"""
        ä½ æ˜¯ä¸€ä¸ªå…·æœ‰é•¿æœŸè®°å¿†èƒ½åŠ›çš„æ™ºèƒ½åŠ©æ‰‹ã€‚
        
        å½“å‰ç›¸å…³è®°å¿†:
        {memory_context}
        
        è¯·åŸºäºè¿™äº›è®°å¿†å’Œå½“å‰å¯¹è¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
        å¦‚æœéœ€è¦ä¿å­˜æ–°çš„è®°å¿†æˆ–åå¥½ï¼Œè¯·ä½¿ç”¨é€‚å½“çš„å·¥å…·ã€‚
        """)
        
        # è°ƒç”¨LLM
        response = llm_with_tools.invoke([system_message] + messages)
        return {"messages": [response]}
    
    def memory_decision_node(self, state: MessagesState):
        """è®°å¿†å†³ç­–èŠ‚ç‚¹ - å†³å®šæ˜¯å¦éœ€è¦ä¿å­˜è®°å¿†"""
        messages = state["messages"]
        last_response = messages[-1]
        
        # åˆ†æå¯¹è¯å†…å®¹ï¼Œå†³å®šæ˜¯å¦ä¿å­˜è®°å¿†
        should_save = self.should_save_memory(messages)
        
        if should_save:
            # æå–å…³é”®ä¿¡æ¯å¹¶ä¿å­˜
            key_info = self.extract_key_information(messages)
            for info in key_info:
                self.memory_store.save_memory(
                    content=info["content"],
                    memory_type=info["type"],
                    metadata={"user_id": "default", "conversation_turn": len(messages)}
                )
        
        return {"messages": messages}
    
    def should_save_memory(self, messages: List[BaseMessage]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¿å­˜è®°å¿†"""
        recent_messages = messages[-3:]  # åˆ†ææœ€è¿‘3æ¡æ¶ˆæ¯
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«é‡è¦ä¿¡æ¯çš„å…³é”®è¯
        important_keywords = [
            "æˆ‘å–œæ¬¢", "æˆ‘ä¸å–œæ¬¢", "æˆ‘çš„ç›®æ ‡", "æˆ‘çš„å·¥ä½œ", 
            "è®°ä½", "é‡è¦", "ä¸‹æ¬¡", "ä»¥å"
        ]
        
        for message in recent_messages:
            if any(keyword in message.content for keyword in important_keywords):
                return True
        
        return False
    
    def extract_key_information(self, messages: List[BaseMessage]) -> List[Dict[str, str]]:
        """ä»å¯¹è¯ä¸­æå–å…³é”®ä¿¡æ¯"""
        recent_conversation = "\\n".join([m.content for m in messages[-5:]])
        
        extraction_prompt = f"""
        ä»ä»¥ä¸‹å¯¹è¯ä¸­æå–éœ€è¦è®°ä½çš„å…³é”®ä¿¡æ¯:
        {recent_conversation}
        
        è¯·è¿”å›JSONæ ¼å¼çš„å…³é”®ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªä¿¡æ¯åŒ…å«contentå’Œtypeå­—æ®µã€‚
        typeå¯ä»¥æ˜¯: preference, fact, goal, skill, background
        """
        
        try:
            response = llm.invoke([HumanMessage(content=extraction_prompt)])
            # è¿™é‡Œåº”è¯¥ä½¿ç”¨æ›´robustçš„JSONè§£æ
            key_info = json.loads(response.content)
            return key_info if isinstance(key_info, list) else []
        except:
            return []
```

#### ğŸ“Š æ ¸å¿ƒåº”ç”¨ï¼štask_mAIstro

task_mAIstro æ˜¯ä¸€ä¸ªå…·æœ‰é•¿æœŸè®°å¿†èƒ½åŠ›çš„ä¸ªäººä»»åŠ¡ç®¡ç†åŠ©æ‰‹ï¼š

##### ä¸»è¦ç‰¹æ€§
1. **æ™ºèƒ½è®°å¿†å†³ç­–**: è‡ªåŠ¨åˆ¤æ–­ä»€ä¹ˆä¿¡æ¯å€¼å¾—é•¿æœŸä¿å­˜
2. **ç”¨æˆ·åå¥½å­¦ä¹ **: å­¦ä¹ ç”¨æˆ·çš„å·¥ä½œä¹ æƒ¯å’Œåå¥½
3. **ç¨‹åºæ€§è®°å¿†**: è®°ä½ç”¨æˆ·åˆ›å»ºå¾…åŠäº‹é¡¹çš„æ¨¡å¼
4. **ä¸Šä¸‹æ–‡æ„ŸçŸ¥**: åŸºäºå†å²äº¤äº’æä¾›ä¸ªæ€§åŒ–å»ºè®®

##### ä½¿ç”¨ç¤ºä¾‹
```python
# åˆ›å»ºè®°å¿†ä»£ç†å®ä¾‹
memory_agent = MemoryAgent()

# ç”¨æˆ·äº¤äº’
config = {"configurable": {"thread_id": "user_session_1"}}

# ç¬¬ä¸€æ¬¡äº¤äº’ - å­¦ä¹ ç”¨æˆ·åå¥½
result1 = memory_agent.invoke({
    "messages": [HumanMessage(content="æˆ‘å–œæ¬¢åœ¨æ—©ä¸Šå®Œæˆé‡è¦ä»»åŠ¡ï¼Œä¸‹åˆå¤„ç†é‚®ä»¶")]
}, config)

# åç»­äº¤äº’ - åŸºäºè®°å¿†æä¾›å»ºè®®
result2 = memory_agent.invoke({
    "messages": [HumanMessage(content="å¸®æˆ‘å®‰æ’æ˜å¤©çš„ä»»åŠ¡")]
}, config)
```

#### ğŸ“‹ å­¦ä¹ æ£€æŸ¥æ¸…å•
- [ ] ç†è§£è¯­ä¹‰è®°å¿†å­˜å‚¨çš„åŸç†å’Œå®ç°
- [ ] æŒæ¡ç”¨æˆ·æ¡£æ¡ˆçš„ç»“æ„åŒ–å­˜å‚¨
- [ ] å®ç°è®°å¿†é›†åˆçš„ç®¡ç†å’ŒæŸ¥è¯¢
- [ ] æ„å»ºæ™ºèƒ½è®°å¿†å†³ç­–é€»è¾‘
- [ ] é›†æˆå‘é‡æ•°æ®åº“è¿›è¡Œè¯­ä¹‰æœç´¢
- [ ] æµ‹è¯•é•¿æœŸè®°å¿†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡
- [ ] å®ç°è®°å¿†çš„æ›´æ–°å’Œè¿‡æœŸæœºåˆ¶

---

### Module-6: ç”Ÿäº§éƒ¨ç½² (Production Deployment)

#### ğŸ¯ å­¦ä¹ ç›®æ ‡
- æŒæ¡åŠ©æ‰‹(Assistants)çš„åˆ›å»ºå’Œç®¡ç†
- å­¦ä¹ é…ç½®ç‰ˆæœ¬æ§åˆ¶
- å®ç°ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- å¤„ç†åŒé‡æ–‡æœ¬å’Œè¿æ¥ç®¡ç†

#### ğŸ“ æ–‡ä»¶ç»“æ„
```
module-6/
â”œâ”€â”€ assistant.ipynb            # åŠ©æ‰‹ç®¡ç†åŸºç¡€
â”œâ”€â”€ creating.ipynb            # åŠ©æ‰‹åˆ›å»ºæµç¨‹
â”œâ”€â”€ connecting.ipynb          # è¿æ¥ç®¡ç†
â”œâ”€â”€ double-texting.ipynb      # åŒé‡æ–‡æœ¬å¤„ç†
â””â”€â”€ deployment/               # ç”Ÿäº§éƒ¨ç½²é…ç½®
    â”œâ”€â”€ task_maistro.py       # ç”Ÿäº§çº§åº”ç”¨
    â”œâ”€â”€ configuration.py      # é…ç½®ç®¡ç†
    â”œâ”€â”€ docker-compose-example.yml # Dockeréƒ¨ç½²
    â”œâ”€â”€ langgraph.json        # éƒ¨ç½²é…ç½®
    â””â”€â”€ requirements.txt      # ç”Ÿäº§ä¾èµ–
```

#### ğŸ”§ æ ¸å¿ƒæŠ€æœ¯è¦ç‚¹

##### 1. åŠ©æ‰‹ç³»ç»Ÿæ¶æ„
```python
from typing import Dict, Any, List, Optional
from pydantic import BaseModel, Field
from enum import Enum
import uuid
from datetime import datetime

class AssistantConfig(BaseModel):
    """åŠ©æ‰‹é…ç½®æ¨¡å‹"""
    assistant_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    name: str = Field(description="åŠ©æ‰‹åç§°")
    description: str = Field(description="åŠ©æ‰‹æè¿°")
    graph_id: str = Field(description="å…³è”çš„å›¾ID")
    version: str = Field(default="v1.0.0", description="ç‰ˆæœ¬å·")
    config: Dict[str, Any] = Field(default_factory=dict, description="é…ç½®å‚æ•°")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="å…ƒæ•°æ®")
    created_at: str = Field(default_factory=lambda: datetime.now().isoformat())
    updated_at: str = Field(default_factory=lambda: datetime.now().isoformat())
    is_active: bool = Field(default=True, description="æ˜¯å¦æ¿€æ´»")

class AssistantManager:
    """åŠ©æ‰‹ç®¡ç†å™¨"""
    
    def __init__(self):
        self.assistants: Dict[str, AssistantConfig] = {}
        self.graph_registry: Dict[str, Any] = {}
    
    def register_graph(self, graph_id: str, graph_instance: Any, 
                      description: str = ""):
        """æ³¨å†Œå›¾å®ä¾‹"""
        self.graph_registry[graph_id] = {
            "instance": graph_instance,
            "description": description,
            "registered_at": datetime.now().isoformat()
        }
    
    def create_assistant(self, name: str, graph_id: str, 
                        config: Dict[str, Any] = None) -> AssistantConfig:
        """åˆ›å»ºæ–°åŠ©æ‰‹"""
        if graph_id not in self.graph_registry:
            raise ValueError(f"å›¾ {graph_id} æœªæ³¨å†Œ")
        
        assistant_config = AssistantConfig(
            name=name,
            description=f"åŸºäº {graph_id} çš„æ™ºèƒ½åŠ©æ‰‹",
            graph_id=graph_id,
            config=config or {}
        )
        
        self.assistants[assistant_config.assistant_id] = assistant_config
        return assistant_config
    
    def update_assistant(self, assistant_id: str, 
                        updates: Dict[str, Any]) -> AssistantConfig:
        """æ›´æ–°åŠ©æ‰‹é…ç½®"""
        if assistant_id not in self.assistants:
            raise ValueError(f"åŠ©æ‰‹ {assistant_id} ä¸å­˜åœ¨")
        
        assistant = self.assistants[assistant_id]
        
        # æ›´æ–°å­—æ®µ
        for key, value in updates.items():
            if hasattr(assistant, key):
                setattr(assistant, key, value)
        
        assistant.updated_at = datetime.now().isoformat()
        return assistant
    
    def get_assistant(self, assistant_id: str) -> Optional[AssistantConfig]:
        """è·å–åŠ©æ‰‹é…ç½®"""
        return self.assistants.get(assistant_id)
    
    def list_assistants(self, active_only: bool = True) -> List[AssistantConfig]:
        """åˆ—å‡ºæ‰€æœ‰åŠ©æ‰‹"""
        assistants = list(self.assistants.values())
        if active_only:
            assistants = [a for a in assistants if a.is_active]
        return assistants
    
    def invoke_assistant(self, assistant_id: str, input_data: Dict[str, Any], 
                        config: Dict[str, Any] = None) -> Any:
        """è°ƒç”¨åŠ©æ‰‹æ‰§è¡Œä»»åŠ¡"""
        assistant = self.get_assistant(assistant_id)
        if not assistant or not assistant.is_active:
            raise ValueError(f"åŠ©æ‰‹ {assistant_id} ä¸å­˜åœ¨æˆ–æœªæ¿€æ´»")
        
        # è·å–å›¾å®ä¾‹
        graph_info = self.graph_registry[assistant.graph_id]
        graph = graph_info["instance"]
        
        # åˆå¹¶é…ç½®
        execution_config = {**(config or {}), **assistant.config}
        
        # æ‰§è¡Œå›¾
        return graph.invoke(input_data, execution_config)
```

##### 2. é…ç½®ç‰ˆæœ¬æ§åˆ¶
```python
class ConfigurationManager:
    """é…ç½®ç‰ˆæœ¬æ§åˆ¶ç®¡ç†å™¨"""
    
    def __init__(self):
        self.config_history: Dict[str, List[Dict[str, Any]]] = {}
        self.current_configs: Dict[str, Dict[str, Any]] = {}
    
    def set_config(self, config_key: str, config_value: Dict[str, Any], 
                   version: str = None):
        """è®¾ç½®é…ç½®å¹¶ä¿å­˜å†å²ç‰ˆæœ¬"""
        if version is None:
            version = f"v{len(self.config_history.get(config_key, []))+1}.0.0"
        
        # ä¿å­˜å†å²ç‰ˆæœ¬
        if config_key not in self.config_history:
            self.config_history[config_key] = []
        
        self.config_history[config_key].append({
            "version": version,
            "config": config_value.copy(),
            "timestamp": datetime.now().isoformat()
        })
        
        # æ›´æ–°å½“å‰é…ç½®
        self.current_configs[config_key] = config_value
    
    def get_config(self, config_key: str, version: str = None) -> Dict[str, Any]:
        """è·å–é…ç½®ï¼ˆæŒ‡å®šç‰ˆæœ¬æˆ–å½“å‰ç‰ˆæœ¬ï¼‰"""
        if version is None:
            return self.current_configs.get(config_key, {})
        
        # æŸ¥æ‰¾æŒ‡å®šç‰ˆæœ¬
        history = self.config_history.get(config_key, [])
        for config_entry in history:
            if config_entry["version"] == version:
                return config_entry["config"]
        
        raise ValueError(f"æœªæ‰¾åˆ°é…ç½® {config_key} çš„ç‰ˆæœ¬ {version}")
    
    def rollback_config(self, config_key: str, target_version: str):
        """å›æ»šé…ç½®åˆ°æŒ‡å®šç‰ˆæœ¬"""
        target_config = self.get_config(config_key, target_version)
        self.current_configs[config_key] = target_config.copy()
        
        # è®°å½•å›æ»šæ“ä½œ
        rollback_entry = {
            "version": f"rollback_to_{target_version}",
            "config": target_config.copy(),
            "timestamp": datetime.now().isoformat(),
            "rollback_from": self.config_history[config_key][-1]["version"]
        }
        self.config_history[config_key].append(rollback_entry)
    
    def list_versions(self, config_key: str) -> List[Dict[str, str]]:
        """åˆ—å‡ºé…ç½®çš„æ‰€æœ‰ç‰ˆæœ¬"""
        history = self.config_history.get(config_key, [])
        return [
            {
                "version": entry["version"],
                "timestamp": entry["timestamp"],
                "is_rollback": "rollback_to_" in entry["version"]
            }
            for entry in history
        ]
```

##### 3. ç”Ÿäº§éƒ¨ç½²é…ç½®
```python
# docker-compose-example.yml é…ç½®
docker_compose_config = """
version: '3.8'

services:
  langgraph-api:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY}
      - LANGCHAIN_TRACING_V2=true
      - POSTGRES_URI=postgresql://user:password@postgres:5432/langgraph
    depends_on:
      - postgres
      - redis
    volumes:
      - ./data:/app/data
    restart: unless-stopped
    
  postgres:
    image: postgres:15
    environment:
      - POSTGRES_DB=langgraph
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    restart: unless-stopped
    
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - langgraph-api
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
"""

# ç”Ÿäº§ç¯å¢ƒé…ç½®ç±»
class ProductionConfig:
    """ç”Ÿäº§ç¯å¢ƒé…ç½®"""
    
    def __init__(self):
        self.load_config()
    
    def load_config(self):
        """åŠ è½½ç”Ÿäº§é…ç½®"""
        self.database_url = os.getenv("POSTGRES_URI", "sqlite:///production.db")
        self.redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
        self.api_host = os.getenv("API_HOST", "0.0.0.0")
        self.api_port = int(os.getenv("API_PORT", "8000"))
        self.worker_count = int(os.getenv("WORKER_COUNT", "4"))
        self.log_level = os.getenv("LOG_LEVEL", "INFO")
        self.enable_cors = os.getenv("ENABLE_CORS", "true").lower() == "true"
        self.cors_origins = os.getenv("CORS_ORIGINS", "*").split(",")
        
        # å®‰å…¨é…ç½®
        self.api_key_required = os.getenv("API_KEY_REQUIRED", "false").lower() == "true"
        self.allowed_api_keys = os.getenv("ALLOWED_API_KEYS", "").split(",")
        
        # æ€§èƒ½é…ç½®
        self.request_timeout = int(os.getenv("REQUEST_TIMEOUT", "300"))
        self.max_request_size = int(os.getenv("MAX_REQUEST_SIZE", "10485760"))  # 10MB
        
        # ç›‘æ§é…ç½®
        self.enable_metrics = os.getenv("ENABLE_METRICS", "true").lower() == "true"
        self.metrics_port = int(os.getenv("METRICS_PORT", "9090"))
```

##### 4. åŒé‡æ–‡æœ¬å¤„ç†å’Œè¿æ¥ç®¡ç†
```python
from asyncio import Queue, Event
from typing import AsyncGenerator
import asyncio

class ConnectionManager:
    """è¿æ¥ç®¡ç†å™¨ - å¤„ç†WebSocketè¿æ¥å’ŒåŒé‡æ–‡æœ¬"""
    
    def __init__(self):
        self.active_connections: Dict[str, Dict[str, Any]] = {}
        self.message_queues: Dict[str, Queue] = {}
        self.processing_locks: Dict[str, Event] = {}
    
    async def connect(self, connection_id: str, websocket = None):
        """å»ºç«‹è¿æ¥"""
        self.active_connections[connection_id] = {
            "websocket": websocket,
            "connected_at": datetime.now().isoformat(),
            "last_activity": datetime.now().isoformat(),
            "message_count": 0
        }
        self.message_queues[connection_id] = Queue()
        self.processing_locks[connection_id] = Event()
        self.processing_locks[connection_id].set()  # åˆå§‹çŠ¶æ€ä¸ºå¯å¤„ç†
    
    async def disconnect(self, connection_id: str):
        """æ–­å¼€è¿æ¥"""
        if connection_id in self.active_connections:
            del self.active_connections[connection_id]
        if connection_id in self.message_queues:
            del self.message_queues[connection_id]
        if connection_id in self.processing_locks:
            del self.processing_locks[connection_id]
    
    async def handle_message(self, connection_id: str, message: str) -> bool:
        """å¤„ç†æ¶ˆæ¯ - é˜²æ­¢åŒé‡æ–‡æœ¬"""
        if connection_id not in self.active_connections:
            return False
        
        # æ£€æŸ¥æ˜¯å¦æ­£åœ¨å¤„ç†æ¶ˆæ¯
        processing_lock = self.processing_locks[connection_id]
        if not processing_lock.is_set():
            # æ­£åœ¨å¤„ç†ä¸­ï¼Œè¿™æ˜¯åŒé‡æ–‡æœ¬
            await self.send_response(connection_id, {
                "type": "warning",
                "message": "æ­£åœ¨å¤„ç†æ‚¨çš„æ¶ˆæ¯ï¼Œè¯·ç¨å€™..."
            })
            return False
        
        # è®¾ç½®å¤„ç†é”
        processing_lock.clear()
        
        try:
            # æ·»åŠ åˆ°æ¶ˆæ¯é˜Ÿåˆ—
            await self.message_queues[connection_id].put({
                "message": message,
                "timestamp": datetime.now().isoformat(),
                "message_id": str(uuid.uuid4())
            })
            
            # æ›´æ–°è¿æ¥ä¿¡æ¯
            self.active_connections[connection_id]["last_activity"] = datetime.now().isoformat()
            self.active_connections[connection_id]["message_count"] += 1
            
            return True
            
        finally:
            # é‡Šæ”¾å¤„ç†é”
            processing_lock.set()
    
    async def process_message_queue(self, connection_id: str, assistant_manager: AssistantManager):
        """å¤„ç†æ¶ˆæ¯é˜Ÿåˆ—"""
        if connection_id not in self.message_queues:
            return
        
        queue = self.message_queues[connection_id]
        
        while not queue.empty():
            try:
                message_data = await queue.get()
                
                # å¤„ç†æ¶ˆæ¯
                await self.send_response(connection_id, {
                    "type": "processing",
                    "message": "æ­£åœ¨å¤„ç†æ‚¨çš„æ¶ˆæ¯..."
                })
                
                # è¿™é‡Œè°ƒç”¨åŠ©æ‰‹å¤„ç†æ¶ˆæ¯
                # response = await assistant_manager.invoke_assistant(...)
                
                await self.send_response(connection_id, {
                    "type": "response",
                    "message": "å¤„ç†å®Œæˆçš„å›ç­”...",
                    "message_id": message_data["message_id"]
                })
                
            except Exception as e:
                await self.send_response(connection_id, {
                    "type": "error",
                    "message": f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}"
                })
    
    async def send_response(self, connection_id: str, response: Dict[str, Any]):
        """å‘é€å“åº”"""
        if connection_id not in self.active_connections:
            return
        
        connection = self.active_connections[connection_id]
        websocket = connection.get("websocket")
        
        if websocket:
            try:
                await websocket.send_json(response)
            except Exception as e:
                print(f"å‘é€å“åº”å¤±è´¥: {e}")
                # è¿æ¥å¯èƒ½å·²æ–­å¼€ï¼Œæ¸…ç†è¿æ¥
                await self.disconnect(connection_id)

class DoubleTextingHandler:
    """åŒé‡æ–‡æœ¬å¤„ç†å™¨"""
    
    def __init__(self, debounce_time: float = 1.0):
        self.debounce_time = debounce_time
        self.pending_messages: Dict[str, Dict[str, Any]] = {}
        self.timers: Dict[str, asyncio.Task] = {}
    
    async def handle_input(self, user_id: str, message: str, 
                          callback: callable) -> bool:
        """å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œé˜²æ­¢åŒé‡å‘é€"""
        # å–æ¶ˆä¹‹å‰çš„å®šæ—¶å™¨
        if user_id in self.timers:
            self.timers[user_id].cancel()
        
        # æ›´æ–°å¾…å¤„ç†æ¶ˆæ¯
        self.pending_messages[user_id] = {
            "message": message,
            "timestamp": datetime.now().isoformat(),
            "callback": callback
        }
        
        # è®¾ç½®æ–°çš„å®šæ—¶å™¨
        self.timers[user_id] = asyncio.create_task(
            self._process_after_delay(user_id)
        )
        
        return True
    
    async def _process_after_delay(self, user_id: str):
        """å»¶è¿Ÿå¤„ç†æ¶ˆæ¯"""
        await asyncio.sleep(self.debounce_time)
        
        if user_id in self.pending_messages:
            message_data = self.pending_messages[user_id]
            callback = message_data["callback"]
            
            try:
                await callback(message_data["message"])
            except Exception as e:
                print(f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
            finally:
                # æ¸…ç†
                if user_id in self.pending_messages:
                    del self.pending_messages[user_id]
                if user_id in self.timers:
                    del self.timers[user_id]
```

#### ğŸ“Š ç”Ÿäº§çº§åº”ç”¨ï¼štask_mAIstro

##### å®Œæ•´çš„ç”Ÿäº§éƒ¨ç½²ç¤ºä¾‹
```python
# ç”Ÿäº§çº§ task_mAIstro åº”ç”¨
class ProductionTaskMaistro:
    """ç”Ÿäº§çº§ä»»åŠ¡ç®¡ç†åŠ©æ‰‹"""
    
    def __init__(self, config: ProductionConfig):
        self.config = config
        self.assistant_manager = AssistantManager()
        self.connection_manager = ConnectionManager()
        self.double_texting_handler = DoubleTextingHandler()
        self.config_manager = ConfigurationManager()
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.setup_assistants()
        self.setup_monitoring()
    
    def setup_assistants(self):
        """è®¾ç½®åŠ©æ‰‹"""
        # æ³¨å†Œä»»åŠ¡ç®¡ç†å›¾
        task_graph = self.build_task_management_graph()
        self.assistant_manager.register_graph(
            "task_manager", 
            task_graph, 
            "æ™ºèƒ½ä»»åŠ¡ç®¡ç†åŠ©æ‰‹"
        )
        
        # åˆ›å»ºé»˜è®¤åŠ©æ‰‹
        self.assistant_manager.create_assistant(
            name="task_mAIstro",
            graph_id="task_manager",
            config={
                "memory_enabled": True,
                "learning_enabled": True,
                "max_context_length": 4000
            }
        )
    
    def setup_monitoring(self):
        """è®¾ç½®ç›‘æ§"""
        if self.config.enable_metrics:
            # è®¾ç½® Prometheus æŒ‡æ ‡
            # è®¾ç½®æ—¥å¿—è®°å½•
            # è®¾ç½®å¥åº·æ£€æŸ¥
            pass
    
    async def start_server(self):
        """å¯åŠ¨ç”Ÿäº§æœåŠ¡å™¨"""
        from fastapi import FastAPI, WebSocket
        from fastapi.middleware.cors import CORSMiddleware
        
        app = FastAPI(title="task_mAIstro API", version="1.0.0")
        
        # æ·»åŠ  CORS ä¸­é—´ä»¶
        if self.config.enable_cors:
            app.add_middleware(
                CORSMiddleware,
                allow_origins=self.config.cors_origins,
                allow_credentials=True,
                allow_methods=["*"],
                allow_headers=["*"],
            )
        
        # API è·¯ç”±
        @app.post("/assistants/{assistant_id}/invoke")
        async def invoke_assistant(assistant_id: str, request: Dict[str, Any]):
            return self.assistant_manager.invoke_assistant(assistant_id, request)
        
        @app.websocket("/ws/{connection_id}")
        async def websocket_endpoint(websocket: WebSocket, connection_id: str):
            await websocket.accept()
            await self.connection_manager.connect(connection_id, websocket)
            
            try:
                while True:
                    message = await websocket.receive_text()
                    await self.connection_manager.handle_message(connection_id, message)
                    await self.connection_manager.process_message_queue(
                        connection_id, self.assistant_manager
                    )
            except Exception as e:
                print(f"WebSocket é”™è¯¯: {e}")
            finally:
                await self.connection_manager.disconnect(connection_id)
        
        # å¯åŠ¨æœåŠ¡å™¨
        import uvicorn
        await uvicorn.run(
            app,
            host=self.config.api_host,
            port=self.config.api_port,
            workers=self.config.worker_count
        )
```

#### ğŸ“‹ å­¦ä¹ æ£€æŸ¥æ¸…å•
- [ ] ç†è§£åŠ©æ‰‹ç³»ç»Ÿçš„æ¶æ„å’Œç®¡ç†
- [ ] æŒæ¡é…ç½®ç‰ˆæœ¬æ§åˆ¶çš„å®ç°
- [ ] å­¦ä¼š Docker å®¹å™¨åŒ–éƒ¨ç½²
- [ ] å®ç° WebSocket è¿æ¥ç®¡ç†
- [ ] å¤„ç†åŒé‡æ–‡æœ¬å’Œæ¶ˆæ¯é˜²é‡
- [ ] é…ç½®ç”Ÿäº§ç¯å¢ƒçš„ç›‘æ§å’Œæ—¥å¿—
- [ ] æµ‹è¯•é«˜å¹¶å‘åœºæ™¯ä¸‹çš„æ€§èƒ½
- [ ] å®ç°å¥åº·æ£€æŸ¥å’Œæ•…éšœæ¢å¤

---

## ğŸ¯ å­¦ä¹ æˆæœæ€»ç»“

é€šè¿‡å®Œæˆæ‰€æœ‰æ¨¡å—çš„å­¦ä¹ ï¼Œæ‚¨å°†è·å¾—ï¼š

### æ ¸å¿ƒæŠ€èƒ½
- âœ… ä»é›¶å¼€å§‹æ„å»º LangGraph åº”ç”¨
- âœ… å®ç°å¤æ‚çš„å¤šä»£ç†åä½œç³»ç»Ÿ
- âœ… æŒæ¡çŠ¶æ€ç®¡ç†å’Œé•¿æœŸè®°å¿†
- âœ… æ„å»ºç”Ÿäº§çº§æ™ºèƒ½åº”ç”¨

### å®é™…é¡¹ç›®ç»éªŒ
- âœ… å¯¹è¯å¼ AI åº”ç”¨å¼€å‘
- âœ… ç ”ç©¶åŠ©æ‰‹ç³»ç»Ÿæ„å»º  
- âœ… ä¸ªäººä»»åŠ¡ç®¡ç†åŠ©æ‰‹
- âœ… ä¼ä¸šçº§éƒ¨ç½²å’Œç»´æŠ¤

### æŠ€æœ¯æ ˆç²¾é€š
- âœ… LangChain/LangGraph ç”Ÿæ€ç³»ç»Ÿ
- âœ… å‘é‡æ•°æ®åº“å’Œè¯­ä¹‰æœç´¢
- âœ… å®¹å™¨åŒ–å’Œå¾®æœåŠ¡æ¶æ„
- âœ… AI åº”ç”¨ç›‘æ§å’Œè°ƒè¯•

å‡†å¤‡å¥½å¼€å§‹æ‚¨çš„ LangGraph å­¦ä¹ ä¹‹æ—…äº†å—ï¼Ÿä» Module-0 å¼€å§‹ï¼Œä¸€æ­¥æ­¥æŒæ¡æ„å»ºæ™ºèƒ½ä»£ç†åº”ç”¨çš„æ ¸å¿ƒæŠ€èƒ½ï¼