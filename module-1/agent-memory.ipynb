{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13cd1c3e",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-1/agent-memory.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239417-lesson-7-agent-with-memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c451ffd-a18b-4412-85fa-85186824dd03",
   "metadata": {},
   "source": [
    "# Agent memory\n",
    "\n",
    "## Review\n",
    "\n",
    "Previously, we built an agent that can:\n",
    "\n",
    "* `act` - let the model call specific tools \n",
    "* `observe` - pass the tool output back to the model \n",
    "* `reason` - let the model reason about the tool output to decide what to do next (e.g., call another tool or just respond directly)\n",
    "\n",
    "![Screenshot 2024-08-21 at 12.45.32 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbab7453080e6802cd1703_agent-memory1.png)\n",
    "\n",
    "## Goals\n",
    "\n",
    "Now, we're going extend our agent by introducing memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b4b45b-cbaa-41b1-b3ed-f6b0645be3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain_openai langchain_core langgraph langgraph-prebuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b0cfa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eff247-a2aa-4f7a-8be1-73dfebfecc63",
   "metadata": {},
   "source": [
    "We'll use [LangSmith](https://docs.smith.langchain.com/) for [tracing](https://docs.smith.langchain.com/concepts/tracing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74ef2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5f123b-db5d-4816-a6a3-2e4247611512",
   "metadata": {},
   "source": [
    "This follows what we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da677753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "proxy = os.environ.get(\"PROXY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46647bbe-def5-4ea7-a315-1de8d97c8288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "# This will be a tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "tools = [add, multiply, divide]\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", openai_proxy= f\"http://{proxy}\")\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9092b40-20c4-4872-b0ed-be1b53a15ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# System message\n",
    "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n",
    "\n",
    "# Node\n",
    "def assistant(state: MessagesState):\n",
    "   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "771123a3-91ac-4076-92c0-93bcd69cf048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition, ToolNode\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Graph\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "react_graph = builder.compile()\n",
    "\n",
    "# Show\n",
    "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e830b7ae-3673-4cc6-8627-4740b7b8b217",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "Let's run our agent, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "596a71a0-1337-44d4-971d-f80c367bd868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add 3 and 4.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  add (call_DGomdWl3n8c0UIvOaHJCaTRZ)\n",
      " Call ID: call_DGomdWl3n8c0UIvOaHJCaTRZ\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "7\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The sum of 3 and 4 is 7.\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
    "messages = react_graph.invoke({\"messages\": messages})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f8128c-f4a5-4dee-b20b-3245bd33f6b3",
   "metadata": {},
   "source": [
    "Now, let's multiply by 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b41cc1d7-e6de-4d86-8958-8cf7446f4c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply that by 2.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Could you please provide me with the specific number or expression you'd like to multiply by 2?\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Multiply that by 2.\")]\n",
    "messages = react_graph.invoke({\"messages\": messages})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e65f3c-e1dc-4a62-b8ab-02b33a6ff268",
   "metadata": {},
   "source": [
    "We don't retain memory of 7 from our initial chat!\n",
    "\n",
    "This is because [state is transient](https://github.com/langchain-ai/langgraph/discussions/352#discussioncomment-9291220) to a single graph execution.\n",
    "\n",
    "Of course, this limits our ability to have multi-turn conversations with interruptions. \n",
    "\n",
    "We can use [persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence/) to address this! \n",
    "\n",
    "LangGraph can use a checkpointer to automatically save the graph state after each step.\n",
    "\n",
    "This built-in persistence layer gives us memory, allowing LangGraph to pick up from the last state update. \n",
    "\n",
    "One of the easiest checkpointers to use is the `MemorySaver`, an in-memory key-value store for Graph state.\n",
    "\n",
    "All we need to do is simply compile the graph with a checkpointer, and our graph has memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "637fcd79-3896-42e4-9131-e03b123a0a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()\n",
    "react_graph_memory = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff8fc3bf-3999-47cb-af34-06b2b94d7192",
   "metadata": {},
   "source": [
    "When we use memory, we need to specify a `thread_id`.\n",
    "\n",
    "This `thread_id` will store our collection of graph states.\n",
    "\n",
    "Here is a cartoon:\n",
    "\n",
    "* The checkpointer write the state at every step of the graph\n",
    "* These checkpoints are saved in a thread \n",
    "* We can access that thread in the future using the `thread_id`\n",
    "\n",
    "![state.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e0e9f526b41a4ed9e2d28b_agent-memory2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173f157",
   "metadata": {},
   "source": [
    "## State‰∏ä‰∏ãÊñáÊú∫Âà∂ËØ¶Ëß£\n",
    "\n",
    "ÂΩì‰ΩøÁî®checkpointerÊó∂Ôºåstate‰∏≠ÁöÑ‰ø°ÊÅØÁ°ÆÂÆû‰ºöÂú®ÊØèÊ¨°invokeÊó∂Âä†ÂÖ•Âà∞‰∏ä‰∏ãÊñá‰∏≠Ôºö\n",
    "\n",
    "### 1. **Êó†ËÆ∞ÂøÜÁöÑÊÉÖÂÜµÔºàÊ≤°ÊúâcheckpointerÔºâ**\n",
    "```python\n",
    "# ÊØèÊ¨°invokeÈÉΩÊòØÁã¨Á´ãÁöÑÔºåÊ≤°ÊúâÂéÜÂè≤‰∏ä‰∏ãÊñá\n",
    "messages = [HumanMessage(content=\"Add 3 and 4.\")]  # Á¨¨‰∏ÄÊ¨°\n",
    "messages = react_graph.invoke({\"messages\": messages})  # ËøîÂõûÔºö7\n",
    "\n",
    "messages = [HumanMessage(content=\"Multiply that by 2.\")]  # Á¨¨‰∫åÊ¨°\n",
    "messages = react_graph.invoke({\"messages\": messages})  # ‰∏çÁü•ÈÅì\"that\"Êåá‰ªÄ‰πà\n",
    "```\n",
    "\n",
    "### 2. **ÊúâËÆ∞ÂøÜÁöÑÊÉÖÂÜµÔºà‰ΩøÁî®checkpointerÔºâ**\n",
    "```python\n",
    "# ‰ΩøÁî®Áõ∏Âêåthread_idÔºåstate‰ºöÁ¥ØÁßØ\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Á¨¨‰∏ÄÊ¨°invoke\n",
    "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
    "messages = react_graph_memory.invoke({\"messages\": messages}, config)\n",
    "# State‰øùÂ≠òÔºö[HumanMessage(\"Add 3 and 4.\"), AIMessage(\"7\"), ...]\n",
    "\n",
    "# Á¨¨‰∫åÊ¨°invoke - Êñ∞Ê∂àÊÅØ‰ºöÊ∑ªÂä†Âà∞Â∑≤ÊúâÁöÑstate‰∏≠\n",
    "messages = [HumanMessage(content=\"Multiply that by 2.\")]\n",
    "messages = react_graph_memory.invoke({\"messages\": messages}, config)\n",
    "# ÂÆûÈôÖ‰º†ÁªôÊ®°ÂûãÁöÑ‰∏ä‰∏ãÊñáÔºö\n",
    "# [HumanMessage(\"Add 3 and 4.\"), AIMessage(\"7\"), ..., HumanMessage(\"Multiply that by 2.\")]\n",
    "```\n",
    "\n",
    "### 3. **MessagesStateÁöÑÁ¥ØÁßØÊú∫Âà∂**\n",
    "- `MessagesState`‰ΩøÁî®‰∫ÜreducerÊú∫Âà∂ÔºåÊñ∞Ê∂àÊÅØ‰ºö**ËøΩÂä†**Âà∞Áé∞ÊúâÊ∂àÊÅØÂàóË°®‰∏≠\n",
    "- ÊØèÊ¨°invokeÊó∂ÔºåÂõæ‰ºö‰ªéthread‰∏≠Âä†ËΩΩÂÆåÊï¥ÁöÑÂéÜÂè≤Áä∂ÊÄÅ\n",
    "- Êñ∞ÁöÑËæìÂÖ•Ê∂àÊÅØ‰ºö‰∏éÂéÜÂè≤Ê∂àÊÅØÂêàÂπ∂ÔºåÂΩ¢ÊàêÂÆåÊï¥ÁöÑÂØπËØù‰∏ä‰∏ãÊñá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15b81260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Á¨¨‰∏ÄÊ¨°invokeÂâçÁöÑstate ===\n",
      "Ê∂àÊÅØÊï∞Èáè: 0\n",
      "\n",
      "=== Á¨¨‰∏ÄÊ¨°invoke: Add 3 and 4 ===\n",
      "Ê∂àÊÅØÊï∞Èáè: 4\n",
      "  Ê∂àÊÅØ1: HumanMessage - Add 3 and 4....\n",
      "  Ê∂àÊÅØ2: AIMessage - ...\n",
      "  Ê∂àÊÅØ3: ToolMessage - 7...\n",
      "  Ê∂àÊÅØ4: AIMessage - The sum of 3 and 4 is 7....\n",
      "\n",
      "=== Á¨¨‰∫åÊ¨°invoke: Multiply that by 2 ===\n",
      "Ê∂àÊÅØÊï∞Èáè: 8\n",
      "  Ê∂àÊÅØ1: HumanMessage - Add 3 and 4....\n",
      "  Ê∂àÊÅØ2: AIMessage - ...\n",
      "  Ê∂àÊÅØ3: ToolMessage - 7...\n",
      "  Ê∂àÊÅØ4: AIMessage - The sum of 3 and 4 is 7....\n",
      "  Ê∂àÊÅØ5: HumanMessage - Multiply that by 2....\n",
      "  Ê∂àÊÅØ6: AIMessage - ...\n",
      "  Ê∂àÊÅØ7: ToolMessage - 14...\n",
      "  Ê∂àÊÅØ8: AIMessage - The result of multiplying 7 by 2 is 14....\n"
     ]
    }
   ],
   "source": [
    "# ËÆ©Êàë‰ª¨Êü•ÁúãstateÂú®ÊØèÊ¨°invokeÂêéÁöÑÂèòÂåñ\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ÂàõÂª∫Êñ∞ÁöÑcheckpointerÊù•ÊºîÁ§∫\n",
    "demo_memory = MemorySaver()\n",
    "demo_graph = builder.compile(checkpointer=demo_memory)\n",
    "demo_config = {\"configurable\": {\"thread_id\": \"demo\"}}\n",
    "\n",
    "print(\"=== Á¨¨‰∏ÄÊ¨°invokeÂâçÁöÑstate ===\")\n",
    "try:\n",
    "    current_state = demo_graph.get_state(demo_config)\n",
    "    print(f\"Ê∂àÊÅØÊï∞Èáè: {len(current_state.values.get('messages', []))}\")\n",
    "except:\n",
    "    print(\"ËøòÊ≤°ÊúâstateËÆ∞ÂΩï\")\n",
    "\n",
    "print(\"\\n=== Á¨¨‰∏ÄÊ¨°invoke: Add 3 and 4 ===\")\n",
    "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
    "result1 = demo_graph.invoke({\"messages\": messages}, demo_config)\n",
    "\n",
    "# Êü•ÁúãÁ¨¨‰∏ÄÊ¨°invokeÂêéÁöÑstate\n",
    "current_state = demo_graph.get_state(demo_config)\n",
    "print(f\"Ê∂àÊÅØÊï∞Èáè: {len(current_state.values['messages'])}\")\n",
    "for i, msg in enumerate(current_state.values['messages']):\n",
    "    print(f\"  Ê∂àÊÅØ{i+1}: {type(msg).__name__} - {msg.content[:50]}...\")\n",
    "\n",
    "print(\"\\n=== Á¨¨‰∫åÊ¨°invoke: Multiply that by 2 ===\")\n",
    "messages = [HumanMessage(content=\"Multiply that by 2.\")]\n",
    "result2 = demo_graph.invoke({\"messages\": messages}, demo_config)\n",
    "\n",
    "# Êü•ÁúãÁ¨¨‰∫åÊ¨°invokeÂêéÁöÑstate\n",
    "current_state = demo_graph.get_state(demo_config)\n",
    "print(f\"Ê∂àÊÅØÊï∞Èáè: {len(current_state.values['messages'])}\")\n",
    "for i, msg in enumerate(current_state.values['messages']):\n",
    "    print(f\"  Ê∂àÊÅØ{i+1}: {type(msg).__name__} - {msg.content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0b9e4",
   "metadata": {},
   "source": [
    "### 4. **ÊäÄÊúØÂéüÁêÜÔºöMessagesStateÁöÑReducerÊú∫Âà∂**\n",
    "\n",
    "`MessagesState`ÊòØ‰∏Ä‰∏™ÁâπÊÆäÁöÑÁä∂ÊÄÅÁ±ªÔºåÂÆÉ‰ΩøÁî®‰∫Ü**reducer**Êú∫Âà∂Êù•Â§ÑÁêÜÊ∂àÊÅØÁ¥ØÁßØÔºö\n",
    "\n",
    "```python\n",
    "# MessagesStateÁöÑÂÆö‰πâÔºàÁÆÄÂåñÁâàÔºâ\n",
    "from typing import Annotated\n",
    "from operator import add\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add]  # Ê≥®ÊÑèËøôÈáåÁöÑadd reducer\n",
    "```\n",
    "\n",
    "**ÂÖ≥ÈîÆÁÇπÔºö**\n",
    "- `Annotated[list[BaseMessage], add]`Ôºö‰ΩøÁî®`add`‰Ωú‰∏∫reducerÂáΩÊï∞\n",
    "- ÂΩìÊñ∞ÁöÑmessages‰º†ÂÖ•Êó∂Ôºå‰ºö‰∏éÁé∞ÊúâÁöÑmessagesÂàóË°®**Áõ∏Âä†**ÔºàËøΩÂä†Ôºâ\n",
    "- ËøôÂ∞±ÊòØ‰∏∫‰ªÄ‰πàÊØèÊ¨°invokeÈÉΩËÉΩ‰øùÊåÅÂÆåÊï¥ÁöÑÂØπËØùÂéÜÂè≤\n",
    "\n",
    "### 5. **‰∏éÊôÆÈÄöÁä∂ÊÄÅÁöÑÂØπÊØî**\n",
    "\n",
    "```python\n",
    "# ÊôÆÈÄöÁä∂ÊÄÅÔºà‰ºöË¶ÜÁõñÔºâ\n",
    "class SimpleState(TypedDict):\n",
    "    value: int  # Ê≤°ÊúâreducerÔºåÊñ∞ÂÄº‰ºöË¶ÜÁõñÊóßÂÄº\n",
    "\n",
    "# Â∏¶reducerÁöÑÁä∂ÊÄÅÔºà‰ºöÁ¥ØÁßØÔºâ\n",
    "class AccumulateState(TypedDict):\n",
    "    values: Annotated[list[int], add]  # ÊúâreducerÔºåÊñ∞ÂÄº‰ºöËøΩÂä†\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350fcc4a",
   "metadata": {},
   "source": [
    "## üö® ‰∏ä‰∏ãÊñáËøáÈïøÈóÆÈ¢òÂèäËß£ÂÜ≥ÊñπÊ°à\n",
    "\n",
    "ÂΩìÂØπËØùÂéÜÂè≤‰∏çÊñ≠Á¥ØÁßØÊó∂Ôºå‰ºöÈÅáÂà∞‰ª•‰∏ãÈóÆÈ¢òÔºö\n",
    "1. **TokenÈôêÂà∂**ÔºöË∂ÖÂá∫LLMÁöÑÊúÄÂ§ß‰∏ä‰∏ãÊñáÁ™óÂè£\n",
    "2. **ÊÄßËÉΩ‰∏ãÈôç**ÔºöÈïø‰∏ä‰∏ãÊñá‰ºöÂØºËá¥ÂìçÂ∫îÊó∂Èó¥ÂèòÊÖ¢„ÄÅÊàêÊú¨Â¢ûÂä†\n",
    "3. **Ê≥®ÊÑèÂäõÂàÜÊï£**ÔºöLLMÂú®Èïø‰∏ä‰∏ãÊñá‰∏≠ÂÆπÊòìË¢´Êó†ÂÖ≥‰ø°ÊÅØ\"ÂàÜÂøÉ\"\n",
    "\n",
    "### Ëß£ÂÜ≥ÊñπÊ°àÂØπÊØîË°®\n",
    "\n",
    "| ÊñπÊ≥ï | ‰ºòÁÇπ | Áº∫ÁÇπ | ÈÄÇÁî®Âú∫ÊôØ |\n",
    "|------|------|------|----------|\n",
    "| **Trim Messages** | ÁÆÄÂçïÂø´ÈÄüÔºåÁ´ãÂç≥ÁîüÊïà | ÂèØËÉΩ‰∏¢Â§±ÈáçË¶Å‰ø°ÊÅØ | Áü≠ÊúüÂØπËØùÔºå‰ø°ÊÅØ‰ª∑ÂÄºÈÄíÂáè |\n",
    "| **Delete Messages** | ÂèØÁ≤æÁ°ÆÊéßÂà∂Âà†Èô§ÂÜÖÂÆπ | ÈúÄË¶ÅÂ§çÊùÇÈÄªËæëÂà§Êñ≠ | ÊúâÊòéÁ°ÆËøáÊó∂‰ø°ÊÅØÁöÑÂú∫ÊôØ |\n",
    "| **Summarize Messages** | ‰øùÁïô‰ø°ÊÅØÁ≤æÂçé | Â¢ûÂä†ËÆ°ÁÆóÂºÄÈîÄÔºåÂèØËÉΩ‰∏¢Â§±ÁªÜËäÇ | ÈïøÊúüÂØπËØùÔºå‰ø°ÊÅØÂØÜÂ∫¶È´ò |\n",
    "| **Long-term Memory** | Ë∑®‰ºöËØù‰øùÂ≠òÈáçË¶Å‰ø°ÊÅØ | ÂÆûÁé∞Â§çÊùÇÂ∫¶È´ò | ‰∏™ÊÄßÂåñÂ∫îÁî®ÔºåÈáçË¶Å‰∫ãÂÆûËÆ∞ÂøÜ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f60e08f",
   "metadata": {},
   "source": [
    "### ÊñπÊ°à1: Trim MessagesÔºàË£ÅÂâ™Ê∂àÊÅØÔºâ\n",
    "\n",
    "ÊúÄÁÆÄÂçïÁöÑÊñπÊ≥ïÊòØÈôêÂà∂Ê∂àÊÅØÂéÜÂè≤ÁöÑÈïøÂ∫¶Ôºå‰øùÁïôÊúÄËøëÁöÑNÊù°Ê∂àÊÅØÊàñN‰∏™tokensÔºö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b388339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÂéüÂßãÊ∂àÊÅØÊï∞Èáè: 20\n",
      "ÂéüÂßãtokenÊï∞ÈáèÔºàËøë‰ººÔºâ: 231\n",
      "\n",
      "Ë£ÅÂâ™ÂêéÊ∂àÊÅØÊï∞Èáè: 17\n",
      "Ë£ÅÂâ™ÂêétokenÊï∞ÈáèÔºàËøë‰ººÔºâ: 195\n",
      "\n",
      "‰øùÁïôÁöÑÊ∂àÊÅØ:\n",
      "  1. HumanMessage: ËøôÊòØÁ¨¨2‰∏™ÈóÆÈ¢òÔºåÂÜÖÂÆπÊØîËæÉÈïøÔºåÁî®Êù•ÊµãËØïtokenËÆ°Êï∞ÂäüËÉΩ...\n",
      "  2. AIMessage: ËøôÊòØÁ¨¨2‰∏™ÂõûÁ≠îÔºå‰πüÂåÖÂê´‰∫ÜÁõ∏ÂΩìÂ§öÁöÑÂÜÖÂÆπÊù•Ê®°ÊãüÁúüÂÆûÂØπËØùÂú∫ÊôØ...\n",
      "  3. HumanMessage: ËøôÊòØÁ¨¨3‰∏™ÈóÆÈ¢òÔºåÂÜÖÂÆπÊØîËæÉÈïøÔºåÁî®Êù•ÊµãËØïtokenËÆ°Êï∞ÂäüËÉΩ...\n",
      "  4. AIMessage: ËøôÊòØÁ¨¨3‰∏™ÂõûÁ≠îÔºå‰πüÂåÖÂê´‰∫ÜÁõ∏ÂΩìÂ§öÁöÑÂÜÖÂÆπÊù•Ê®°ÊãüÁúüÂÆûÂØπËØùÂú∫ÊôØ...\n",
      "  5. HumanMessage: ËøôÊòØÁ¨¨4‰∏™ÈóÆÈ¢òÔºåÂÜÖÂÆπÊØîËæÉÈïøÔºåÁî®Êù•ÊµãËØïtokenËÆ°Êï∞ÂäüËÉΩ...\n",
      "  6. AIMessage: ËøôÊòØÁ¨¨4‰∏™ÂõûÁ≠îÔºå‰πüÂåÖÂê´‰∫ÜÁõ∏ÂΩìÂ§öÁöÑÂÜÖÂÆπÊù•Ê®°ÊãüÁúüÂÆûÂØπËØùÂú∫ÊôØ...\n",
      "  7. HumanMessage: ËøôÊòØÁ¨¨5‰∏™ÈóÆÈ¢òÔºåÂÜÖÂÆπÊØîËæÉÈïøÔºåÁî®Êù•ÊµãËØïtokenËÆ°Êï∞ÂäüËÉΩ...\n",
      "  8. AIMessage: ËøôÊòØÁ¨¨5‰∏™ÂõûÁ≠îÔºå‰πüÂåÖÂê´‰∫ÜÁõ∏ÂΩìÂ§öÁöÑÂÜÖÂÆπÊù•Ê®°ÊãüÁúüÂÆûÂØπËØùÂú∫ÊôØ...\n",
      "  9. HumanMessage: ËøôÊòØÁ¨¨6‰∏™ÈóÆÈ¢òÔºåÂÜÖÂÆπÊØîËæÉÈïøÔºåÁî®Êù•ÊµãËØïtokenËÆ°Êï∞ÂäüËÉΩ...\n",
      "  10. AIMessage: ËøôÊòØÁ¨¨6‰∏™ÂõûÁ≠îÔºå‰πüÂåÖÂê´‰∫ÜÁõ∏ÂΩìÂ§öÁöÑÂÜÖÂÆπÊù•Ê®°ÊãüÁúüÂÆûÂØπËØùÂú∫ÊôØ...\n",
      "  11. HumanMessage: ËøôÊòØÁ¨¨7‰∏™ÈóÆÈ¢òÔºåÂÜÖÂÆπÊØîËæÉÈïøÔºåÁî®Êù•ÊµãËØïtokenËÆ°Êï∞ÂäüËÉΩ...\n",
      "  12. AIMessage: ËøôÊòØÁ¨¨7‰∏™ÂõûÁ≠îÔºå‰πüÂåÖÂê´‰∫ÜÁõ∏ÂΩìÂ§öÁöÑÂÜÖÂÆπÊù•Ê®°ÊãüÁúüÂÆûÂØπËØùÂú∫ÊôØ...\n",
      "  13. HumanMessage: ËøôÊòØÁ¨¨8‰∏™ÈóÆÈ¢òÔºåÂÜÖÂÆπÊØîËæÉÈïøÔºåÁî®Êù•ÊµãËØïtokenËÆ°Êï∞ÂäüËÉΩ...\n",
      "  14. AIMessage: ËøôÊòØÁ¨¨8‰∏™ÂõûÁ≠îÔºå‰πüÂåÖÂê´‰∫ÜÁõ∏ÂΩìÂ§öÁöÑÂÜÖÂÆπÊù•Ê®°ÊãüÁúüÂÆûÂØπËØùÂú∫ÊôØ...\n",
      "  15. HumanMessage: ËøôÊòØÁ¨¨9‰∏™ÈóÆÈ¢òÔºåÂÜÖÂÆπÊØîËæÉÈïøÔºåÁî®Êù•ÊµãËØïtokenËÆ°Êï∞ÂäüËÉΩ...\n",
      "  16. AIMessage: ËøôÊòØÁ¨¨9‰∏™ÂõûÁ≠îÔºå‰πüÂåÖÂê´‰∫ÜÁõ∏ÂΩìÂ§öÁöÑÂÜÖÂÆπÊù•Ê®°ÊãüÁúüÂÆûÂØπËØùÂú∫ÊôØ...\n",
      "  17. HumanMessage: ËøôÊòØÁ¨¨10‰∏™ÈóÆÈ¢òÔºåÂÜÖÂÆπÊØîËæÉÈïøÔºåÁî®Êù•ÊµãËØïtokenËÆ°Êï∞ÂäüËÉΩ...\n"
     ]
    }
   ],
   "source": [
    "# ÊñπÊ°à1: ‰ΩøÁî®trim_messagesÈôêÂà∂‰∏ä‰∏ãÊñáÈïøÂ∫¶\n",
    "from langchain_core.messages.utils import trim_messages, count_tokens_approximately\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Ê®°Êãü‰∏Ä‰∏™ÂæàÈïøÁöÑÂØπËØùÂéÜÂè≤\n",
    "long_conversation = []\n",
    "for i in range(10):\n",
    "    long_conversation.extend([\n",
    "        HumanMessage(content=f\"ËøôÊòØÁ¨¨{i+1}‰∏™ÈóÆÈ¢òÔºåÂÜÖÂÆπÊØîËæÉÈïøÔºåÁî®Êù•ÊµãËØïtokenËÆ°Êï∞ÂäüËÉΩ\"),\n",
    "        AIMessage(content=f\"ËøôÊòØÁ¨¨{i+1}‰∏™ÂõûÁ≠îÔºå‰πüÂåÖÂê´‰∫ÜÁõ∏ÂΩìÂ§öÁöÑÂÜÖÂÆπÊù•Ê®°ÊãüÁúüÂÆûÂØπËØùÂú∫ÊôØ\")\n",
    "    ])\n",
    "\n",
    "print(f\"ÂéüÂßãÊ∂àÊÅØÊï∞Èáè: {len(long_conversation)}\")\n",
    "print(f\"ÂéüÂßãtokenÊï∞ÈáèÔºàËøë‰ººÔºâ: {count_tokens_approximately(long_conversation)}\")\n",
    "\n",
    "# ‰ΩøÁî®trim_messagesË£ÅÂâ™Âà∞ÊåáÂÆötokenÊï∞Èáè\n",
    "trimmed_messages = trim_messages(\n",
    "    long_conversation,\n",
    "    strategy=\"last\",  # ‰øùÁïôÊúÄÂêéÁöÑÊ∂àÊÅØ\n",
    "    token_counter=count_tokens_approximately,\n",
    "    max_tokens=200,  # ÈôêÂà∂‰∏∫200‰∏™tokens\n",
    "    start_on=\"human\",  # Á°Æ‰øù‰ªéhumanÊ∂àÊÅØÂºÄÂßã\n",
    "    end_on=(\"human\", \"tool\"),  # Á°Æ‰øù‰ª•humanÊàñtoolÊ∂àÊÅØÁªìÊùü\n",
    ")\n",
    "\n",
    "print(f\"\\nË£ÅÂâ™ÂêéÊ∂àÊÅØÊï∞Èáè: {len(trimmed_messages)}\")\n",
    "print(f\"Ë£ÅÂâ™ÂêétokenÊï∞ÈáèÔºàËøë‰ººÔºâ: {count_tokens_approximately(trimmed_messages)}\")\n",
    "\n",
    "# ÊòæÁ§∫‰øùÁïôÁöÑÊ∂àÊÅØÂÜÖÂÆπ\n",
    "print(\"\\n‰øùÁïôÁöÑÊ∂àÊÅØ:\")\n",
    "for i, msg in enumerate(trimmed_messages):\n",
    "    print(f\"  {i+1}. {type(msg).__name__}: {msg.content[:30]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a29794b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ÂàõÂª∫‰∫ÜÂ∏¶ÊúâÊ∂àÊÅØË£ÅÂâ™ÂäüËÉΩÁöÑAgent\n",
      "üìù ËØ•Agent‰ºöÂú®ÊØèÊ¨°Ë∞ÉÁî®LLMÂâçËá™Âä®Ë£ÅÂâ™Ê∂àÊÅØÂéÜÂè≤Ôºå‰øùÊåÅÂú®500 tokens‰ª•ÂÜÖ\n"
     ]
    }
   ],
   "source": [
    "# Âú®Agent‰∏≠ÈõÜÊàêtrim messages\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ÂÆö‰πâpre_model_hookÊù•Ëá™Âä®Ë£ÅÂâ™Ê∂àÊÅØ\n",
    "def trim_messages_hook(state):\n",
    "    \"\"\"Âú®ÊØèÊ¨°Ë∞ÉÁî®LLMÂâçËá™Âä®Ë£ÅÂâ™Ê∂àÊÅØÂéÜÂè≤\"\"\"\n",
    "    trimmed_messages = trim_messages(\n",
    "        state[\"messages\"],\n",
    "        strategy=\"last\",\n",
    "        token_counter=count_tokens_approximately,\n",
    "        max_tokens=500,  # ÈôêÂà∂‰∏∫500‰∏™tokens\n",
    "        start_on=\"human\",\n",
    "        end_on=(\"human\", \"tool\"),\n",
    "    )\n",
    "    return {\"llm_input_messages\": trimmed_messages}\n",
    "\n",
    "# ÂàõÂª∫Â∏¶ÊúâÊ∂àÊÅØË£ÅÂâ™ÂäüËÉΩÁöÑAgent\n",
    "trimmed_checkpointer = MemorySaver()\n",
    "trimmed_agent = create_react_agent(\n",
    "    llm_with_tools,\n",
    "    tools,\n",
    "    pre_model_hook=trim_messages_hook,  # ‰ΩøÁî®trim hook\n",
    "    checkpointer=trimmed_checkpointer,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ÂàõÂª∫‰∫ÜÂ∏¶ÊúâÊ∂àÊÅØË£ÅÂâ™ÂäüËÉΩÁöÑAgent\")\n",
    "print(\"üìù ËØ•Agent‰ºöÂú®ÊØèÊ¨°Ë∞ÉÁî®LLMÂâçËá™Âä®Ë£ÅÂâ™Ê∂àÊÅØÂéÜÂè≤Ôºå‰øùÊåÅÂú®500 tokens‰ª•ÂÜÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04456c0c",
   "metadata": {},
   "source": [
    "### ÊñπÊ°à2: Delete MessagesÔºàÂà†Èô§Ê∂àÊÅØÔºâ\n",
    "\n",
    "ÂèØ‰ª•‰ªéstate‰∏≠Ê∞∏‰πÖÂà†Èô§ÁâπÂÆöÊ∂àÊÅØÔºåÊØîÂà†Èô§ÊúÄÊó©ÁöÑÊ∂àÊÅØÊàñÁâπÂÆöÁ±ªÂûãÁöÑÊ∂àÊÅØÔºö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e398fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Âà†Èô§Ê∂àÊÅØÁ≠ñÁï•:\n",
      "- ÂΩìÊ∂àÊÅØË∂ÖËøá6Êù°Êó∂ÔºåËá™Âä®Âà†Èô§ÊúÄÊó©ÁöÑ2Êù°\n",
      "- ÂèØ‰ª•Âà†Èô§ÁâπÂÆöÁ±ªÂûãÁöÑÊ∂àÊÅØÔºàÂ¶ÇÁ≥ªÁªüÊ∂àÊÅØ„ÄÅÂ∑•ÂÖ∑Ê∂àÊÅØÁ≠âÔºâ\n",
      "- ÂèØ‰ª•‰∏ÄÊ¨°ÊÄßÊ∏ÖÁ©∫ÊâÄÊúâÊ∂àÊÅØÂéÜÂè≤\n",
      "\n",
      "‚úÖ ‰øÆÂ§çÁöÑÈóÆÈ¢ò:\n",
      "1. Ê∑ªÂä†‰∫ÜSTARTÂíåENDËäÇÁÇπËøûÊé•\n",
      "2. ‰øÆÂ§ç‰∫Ücleanup_messagesÁöÑËøîÂõûÂÄº\n",
      "3. Ê∑ªÂä†‰∫ÜÊµãËØïÂáΩÊï∞Êù•È™åËØÅÂäüËÉΩ\n"
     ]
    }
   ],
   "source": [
    "# ÊñπÊ°à2: Âà†Èô§Ê∂àÊÅØ\n",
    "from langchain_core.messages import RemoveMessage\n",
    "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "\n",
    "# ÊºîÁ§∫Âà†Èô§Ê∂àÊÅØÁöÑÂäüËÉΩ\n",
    "def demo_delete_messages():\n",
    "    \"\"\"ÂàõÂª∫‰∏Ä‰∏™Â∏¶ÊúâÊ∂àÊÅØÁÆ°ÁêÜÂäüËÉΩÁöÑÂõæ\"\"\"\n",
    "    from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "    \n",
    "    def cleanup_messages(state: MessagesState):\n",
    "        \"\"\"Ê∏ÖÁêÜËøáÂ§öÁöÑÊ∂àÊÅØ\"\"\"\n",
    "        messages = state[\"messages\"]\n",
    "        \n",
    "        # Â¶ÇÊûúÊ∂àÊÅØÊï∞ÈáèË∂ÖËøá6Êù°ÔºåÂà†Èô§ÊúÄÊó©ÁöÑ2Êù°\n",
    "        if len(messages) > 6:\n",
    "            print(f\"Ê∂àÊÅØÊï∞Èáè ({len(messages)}) Ë∂ÖËøáÈôêÂà∂ÔºåÂà†Èô§ÊúÄÊó©ÁöÑ2Êù°Ê∂àÊÅØ\")\n",
    "            return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n",
    "        \n",
    "        # ÈáçË¶ÅÔºöÂ¶ÇÊûú‰∏çÈúÄË¶ÅÂà†Èô§ÔºåËøîÂõûÁ©∫ÁöÑmessagesÊõ¥Êñ∞\n",
    "        return {}\n",
    "    \n",
    "    def assistant_node(state: MessagesState):\n",
    "        \"\"\"ÁÆÄÂåñÁöÑÂä©ÊâãËäÇÁÇπ\"\"\"\n",
    "        return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
    "    \n",
    "    # ÊûÑÂª∫Âõæ\n",
    "    cleanup_builder = StateGraph(MessagesState)\n",
    "    cleanup_builder.add_node(\"cleanup\", cleanup_messages)\n",
    "    cleanup_builder.add_node(\"assistant\", assistant_node)\n",
    "    \n",
    "    # ‰øÆÂ§çËæπÁöÑËøûÊé•\n",
    "    cleanup_builder.add_edge(START, \"cleanup\")\n",
    "    cleanup_builder.add_edge(\"cleanup\", \"assistant\")\n",
    "    cleanup_builder.add_edge(\"assistant\", END)\n",
    "    \n",
    "    cleanup_graph = cleanup_builder.compile(checkpointer=MemorySaver())\n",
    "    \n",
    "    return cleanup_graph\n",
    "\n",
    "# ÊºîÁ§∫Âà†Èô§Ê∂àÊÅØÁöÑÂÆûÈôÖÊïàÊûú\n",
    "def test_message_deletion():\n",
    "    \"\"\"ÊµãËØïÊ∂àÊÅØÂà†Èô§ÂäüËÉΩ\"\"\"\n",
    "    cleanup_graph = demo_delete_messages()\n",
    "    config = {\"configurable\": {\"thread_id\": \"cleanup_test\"}}\n",
    "    \n",
    "    print(\"=== ÊµãËØïÊ∂àÊÅØÂà†Èô§ÂäüËÉΩ ===\")\n",
    "    \n",
    "    # Ê∑ªÂä†Â§öÊù°Ê∂àÊÅØÊù•Ëß¶ÂèëÂà†Èô§\n",
    "    for i in range(4):\n",
    "        messages = [HumanMessage(content=f\"ËøôÊòØÁ¨¨{i+1}Êù°ÊµãËØïÊ∂àÊÅØ\")]\n",
    "        result = cleanup_graph.invoke({\"messages\": messages}, config)\n",
    "        \n",
    "        # Ê£ÄÊü•ÂΩìÂâçÁä∂ÊÄÅ\n",
    "        current_state = cleanup_graph.get_state(config)\n",
    "        msg_count = len(current_state.values['messages'])\n",
    "        print(f\"Á¨¨{i+1}Ê¨°Ë∞ÉÁî®ÂêéÔºåÊ∂àÊÅØÊï∞Èáè: {msg_count}\")\n",
    "    \n",
    "    return cleanup_graph\n",
    "\n",
    "print(\"üóëÔ∏è Âà†Èô§Ê∂àÊÅØÁ≠ñÁï•:\")\n",
    "print(\"- ÂΩìÊ∂àÊÅØË∂ÖËøá6Êù°Êó∂ÔºåËá™Âä®Âà†Èô§ÊúÄÊó©ÁöÑ2Êù°\")\n",
    "print(\"- ÂèØ‰ª•Âà†Èô§ÁâπÂÆöÁ±ªÂûãÁöÑÊ∂àÊÅØÔºàÂ¶ÇÁ≥ªÁªüÊ∂àÊÅØ„ÄÅÂ∑•ÂÖ∑Ê∂àÊÅØÁ≠âÔºâ\")\n",
    "print(\"- ÂèØ‰ª•‰∏ÄÊ¨°ÊÄßÊ∏ÖÁ©∫ÊâÄÊúâÊ∂àÊÅØÂéÜÂè≤\")\n",
    "print(\"\\n‚úÖ ‰øÆÂ§çÁöÑÈóÆÈ¢ò:\")\n",
    "print(\"1. Ê∑ªÂä†‰∫ÜSTARTÂíåENDËäÇÁÇπËøûÊé•\")\n",
    "print(\"2. ‰øÆÂ§ç‰∫Ücleanup_messagesÁöÑËøîÂõûÂÄº\")\n",
    "print(\"3. Ê∑ªÂä†‰∫ÜÊµãËØïÂáΩÊï∞Êù•È™åËØÅÂäüËÉΩ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be37bc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ÊµãËØïÊ∂àÊÅØÂà†Èô§ÂäüËÉΩ ===\n",
      "Á¨¨1Ê¨°Ë∞ÉÁî®ÂêéÔºåÊ∂àÊÅØÊï∞Èáè: 2\n",
      "Á¨¨1Ê¨°Ë∞ÉÁî®ÂêéÔºåÊ∂àÊÅØÊï∞Èáè: 2\n",
      "Á¨¨2Ê¨°Ë∞ÉÁî®ÂêéÔºåÊ∂àÊÅØÊï∞Èáè: 4\n",
      "Á¨¨2Ê¨°Ë∞ÉÁî®ÂêéÔºåÊ∂àÊÅØÊï∞Èáè: 4\n",
      "Á¨¨3Ê¨°Ë∞ÉÁî®ÂêéÔºåÊ∂àÊÅØÊï∞Èáè: 6\n",
      "Ê∂àÊÅØÊï∞Èáè (7) Ë∂ÖËøáÈôêÂà∂ÔºåÂà†Èô§ÊúÄÊó©ÁöÑ2Êù°Ê∂àÊÅØ\n",
      "Á¨¨3Ê¨°Ë∞ÉÁî®ÂêéÔºåÊ∂àÊÅØÊï∞Èáè: 6\n",
      "Ê∂àÊÅØÊï∞Èáè (7) Ë∂ÖËøáÈôêÂà∂ÔºåÂà†Èô§ÊúÄÊó©ÁöÑ2Êù°Ê∂àÊÅØ\n",
      "Á¨¨4Ê¨°Ë∞ÉÁî®ÂêéÔºåÊ∂àÊÅØÊï∞Èáè: 6\n",
      "\n",
      "üéâ Âà†Èô§Ê∂àÊÅØÂäüËÉΩÊµãËØïÊàêÂäüÔºÅ\n",
      "Á¨¨4Ê¨°Ë∞ÉÁî®ÂêéÔºåÊ∂àÊÅØÊï∞Èáè: 6\n",
      "\n",
      "üéâ Âà†Èô§Ê∂àÊÅØÂäüËÉΩÊµãËØïÊàêÂäüÔºÅ\n"
     ]
    }
   ],
   "source": [
    "# ÊµãËØïÂà†Èô§Ê∂àÊÅØÂäüËÉΩ\n",
    "try:\n",
    "    cleanup_graph = test_message_deletion()\n",
    "    print(\"\\nüéâ Âà†Èô§Ê∂àÊÅØÂäüËÉΩÊµãËØïÊàêÂäüÔºÅ\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ÊµãËØïÂ§±Ë¥•: {e}\")\n",
    "    print(\"ËøôÈÄöÂ∏∏ÊòØÂõ†‰∏∫ÂõæÁªìÊûÑÊàñÊ∂àÊÅØÂ§ÑÁêÜÁöÑÈóÆÈ¢ò\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15dd4f5",
   "metadata": {},
   "source": [
    "### ÊñπÊ°à3: Summarize MessagesÔºàÊ∂àÊÅØÊëòË¶ÅÔºâ\n",
    "\n",
    "‰ΩøÁî®LLMÂØπÊó©ÊúüÂØπËØùËøõË°åÊëòË¶ÅÔºå‰øùÁïô‰ø°ÊÅØÁ≤æÂçéÁöÑÂêåÊó∂ÂáèÂ∞ëtoken‰ΩøÁî®Ôºö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1441b025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Ê∂àÊÅØÊëòË¶ÅÁ≠ñÁï•:\n",
      "- ÂΩìÊ∂àÊÅØË∂ÖËøá8Êù°Êó∂ÔºåÂØπÊó©ÊúüÊ∂àÊÅØËøõË°åÊëòË¶Å\n",
      "- ‰øùÁïôÊúÄËøë4Êù°Ê∂àÊÅØÁöÑÂÆåÊï¥ÂÜÖÂÆπ\n",
      "- Áî®ÊëòË¶Å + ÊúÄËøëÊ∂àÊÅØ‰Ωú‰∏∫LLMËæìÂÖ•\n",
      "- ‰ø°ÊÅØÊçüÂ§±ÊúÄÂ∞èÔºå‰ΩÜÈúÄË¶ÅÈ¢ùÂ§ñÁöÑLLMË∞ÉÁî®ÊàêÊú¨\n"
     ]
    }
   ],
   "source": [
    "# ÊñπÊ°à3: Ê∂àÊÅØÊëòË¶ÅÔºàÈúÄË¶ÅÂÆâË£ÖlangmemÔºâ\n",
    "# Ê≥®ÊÑèÔºöËøôÈáåÂ±ïÁ§∫Ê¶ÇÂøµÔºåÂÆûÈôÖ‰ΩøÁî®ÈúÄË¶ÅÂÆâË£Ö pip install langmem\n",
    "\n",
    "def create_summary_hook():\n",
    "    \"\"\"ÂàõÂª∫‰∏Ä‰∏™ÁÆÄÂåñÁöÑÊëòË¶Åhook\"\"\"\n",
    "    \n",
    "    def simple_summarize_hook(state):\n",
    "        messages = state[\"messages\"]\n",
    "        \n",
    "        # Â¶ÇÊûúÊ∂àÊÅØËøáÂ§öÔºåËøõË°åÊëòË¶Å\n",
    "        if len(messages) > 8:\n",
    "            # ‰øùÁïôÊúÄËøëÁöÑ4Êù°Ê∂àÊÅØ\n",
    "            recent_messages = messages[-4:]\n",
    "            \n",
    "            # ÂØπÊó©ÊúüÊ∂àÊÅØËøõË°åÊëòË¶Å\n",
    "            early_messages = messages[:-4]\n",
    "            if early_messages:\n",
    "                # ÊûÑÂª∫ÊëòË¶ÅÊèêÁ§∫\n",
    "                summary_prompt = f\"\"\"ËØ∑ÁÆÄË¶ÅÊÄªÁªì‰ª•‰∏ãÂØπËØùÂÜÖÂÆπÁöÑÂÖ≥ÈîÆ‰ø°ÊÅØÔºö\n",
    "\n",
    "{chr(10).join([f\"{type(msg).__name__}: {msg.content}\" for msg in early_messages])}\n",
    "\n",
    "ËØ∑Áî®1-2Âè•ËØùÊÄªÁªìÂÖ≥ÈîÆ‰ø°ÊÅØÔºö\"\"\"\n",
    "                \n",
    "                try:\n",
    "                    summary_response = llm.invoke([HumanMessage(content=summary_prompt)])\n",
    "                    summary_message = AIMessage(content=f\"[ÂØπËØùÊëòË¶Å] {summary_response.content}\")\n",
    "                    \n",
    "                    # ËøîÂõûÊëòË¶Å + ÊúÄËøëÁöÑÊ∂àÊÅØ\n",
    "                    return {\"llm_input_messages\": [summary_message] + recent_messages}\n",
    "                except:\n",
    "                    # Â¶ÇÊûúÊëòË¶ÅÂ§±Ë¥•ÔºåÈÄÄÂõûÂà∞trimÁ≠ñÁï•\n",
    "                    return {\"llm_input_messages\": recent_messages}\n",
    "        \n",
    "        return {\"llm_input_messages\": messages}\n",
    "    \n",
    "    return simple_summarize_hook\n",
    "\n",
    "print(\"üìù Ê∂àÊÅØÊëòË¶ÅÁ≠ñÁï•:\")\n",
    "print(\"- ÂΩìÊ∂àÊÅØË∂ÖËøá8Êù°Êó∂ÔºåÂØπÊó©ÊúüÊ∂àÊÅØËøõË°åÊëòË¶Å\")\n",
    "print(\"- ‰øùÁïôÊúÄËøë4Êù°Ê∂àÊÅØÁöÑÂÆåÊï¥ÂÜÖÂÆπ\")\n",
    "print(\"- Áî®ÊëòË¶Å + ÊúÄËøëÊ∂àÊÅØ‰Ωú‰∏∫LLMËæìÂÖ•\")\n",
    "print(\"- ‰ø°ÊÅØÊçüÂ§±ÊúÄÂ∞èÔºå‰ΩÜÈúÄË¶ÅÈ¢ùÂ§ñÁöÑLLMË∞ÉÁî®ÊàêÊú¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f910b27e",
   "metadata": {},
   "source": [
    "### ÊñπÊ°à4: Long-term MemoryÔºàÈïøÊúüËÆ∞ÂøÜÔºâ\n",
    "\n",
    "Â∞ÜÈáçË¶Å‰ø°ÊÅØÊèêÂèñÂà∞ÈïøÊúüËÆ∞ÂøÜ‰∏≠ÔºåË∑®‰ºöËØù‰øùÂ≠òÂÖ≥ÈîÆ‰∫ãÂÆûÔºö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18ffe539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† ÈïøÊúüËÆ∞ÂøÜÁ≠ñÁï•:\n",
      "- Ëá™Âä®ÊèêÂèñÂØπËØù‰∏≠ÁöÑÈáçË¶Å‰∫ãÂÆû\n",
      "- ÊåâÁ±ªÂà´ÁªÑÁªáËÆ∞ÂøÜÔºà‰∏™‰∫∫‰ø°ÊÅØ„ÄÅÂÅèÂ•Ω„ÄÅÂéÜÂè≤Á≠âÔºâ\n",
      "- Ë∑®‰ºöËØù‰øùÂ≠òÂíåÊ£ÄÁ¥¢ÈáçË¶Å‰ø°ÊÅØ\n",
      "- ÊîØÊåÅËØ≠‰πâÊêúÁ¥¢ÔºàÈúÄË¶ÅÈÖçÁΩÆembeddingsÔºâ\n",
      "\n",
      "ÊºîÁ§∫:\n",
      "‰øùÂ≠ò: Â∑≤‰øùÂ≠òÂà∞ÈïøÊúüËÆ∞ÂøÜ: Áî®Êà∑ÂñúÊ¨¢ÂñùÂíñÂï°\n",
      "‰øùÂ≠ò: Â∑≤‰øùÂ≠òÂà∞ÈïøÊúüËÆ∞ÂøÜ: Áî®Êà∑‰ΩèÂú®Âåó‰∫¨\n",
      "Ê£ÄÁ¥¢: Áõ∏ÂÖ≥ËÆ∞ÂøÜ: Áî®Êà∑ÂñúÊ¨¢ÂñùÂíñÂï°\n"
     ]
    }
   ],
   "source": [
    "# ÊñπÊ°à4: ÈïøÊúüËÆ∞ÂøÜ\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "def create_memory_agent():\n",
    "    \"\"\"ÂàõÂª∫Â∏¶ÊúâÈïøÊúüËÆ∞ÂøÜÁöÑAgent\"\"\"\n",
    "    \n",
    "    # ÂàõÂª∫storeÁî®‰∫éÈïøÊúüËÆ∞ÂøÜ\n",
    "    memory_store = InMemoryStore()\n",
    "    \n",
    "    # ÂÆö‰πâÂ∑•ÂÖ∑Ôºö‰øùÂ≠òÈáçË¶Å‰ø°ÊÅØÂà∞ÈïøÊúüËÆ∞ÂøÜ\n",
    "    def save_to_memory(info: str, category: str = \"general\") -> str:\n",
    "        \"\"\"‰øùÂ≠òÈáçË¶Å‰ø°ÊÅØÂà∞ÈïøÊúüËÆ∞ÂøÜ\"\"\"\n",
    "        import uuid\n",
    "        memory_id = str(uuid.uuid4())\n",
    "        namespace = (\"user_memory\", category)\n",
    "        \n",
    "        memory_store.put(namespace, memory_id, {\n",
    "            \"content\": info,\n",
    "            \"timestamp\": \"2024-01-01\",  # ÂÆûÈôÖÂ∫îÁî®‰∏≠Áî®ÁúüÂÆûÊó∂Èó¥Êà≥\n",
    "            \"category\": category\n",
    "        })\n",
    "        return f\"Â∑≤‰øùÂ≠òÂà∞ÈïøÊúüËÆ∞ÂøÜ: {info}\"\n",
    "    \n",
    "    # ÂÆö‰πâÂ∑•ÂÖ∑Ôºö‰ªéÈïøÊúüËÆ∞ÂøÜ‰∏≠Ê£ÄÁ¥¢‰ø°ÊÅØ\n",
    "    def recall_from_memory(query: str, category: str = \"general\") -> str:\n",
    "        \"\"\"‰ªéÈïøÊúüËÆ∞ÂøÜ‰∏≠Ê£ÄÁ¥¢Áõ∏ÂÖ≥‰ø°ÊÅØ\"\"\"\n",
    "        namespace = (\"user_memory\", category)\n",
    "        try:\n",
    "            # ÁÆÄÂåñÁöÑÊ£ÄÁ¥¢ÔºàÂÆûÈôÖÂ∫îÁî®‰∏≠ÂèØ‰ª•‰ΩøÁî®ËØ≠‰πâÊêúÁ¥¢Ôºâ\n",
    "            memories = memory_store.search(namespace)\n",
    "            if memories:\n",
    "                relevant_memories = [m.value[\"content\"] for m in memories[-3:]]  # ËøîÂõûÊúÄËøë3Êù°\n",
    "                return f\"Áõ∏ÂÖ≥ËÆ∞ÂøÜ: {'; '.join(relevant_memories)}\"\n",
    "            else:\n",
    "                return \"Ê≤°ÊúâÊâæÂà∞Áõ∏ÂÖ≥ËÆ∞ÂøÜ\"\n",
    "        except:\n",
    "            return \"ËÆ∞ÂøÜÊ£ÄÁ¥¢Â§±Ë¥•\"\n",
    "    \n",
    "    return memory_store, save_to_memory, recall_from_memory\n",
    "\n",
    "# ÂàõÂª∫ËÆ∞ÂøÜÁ≥ªÁªü\n",
    "memory_store, save_func, recall_func = create_memory_agent()\n",
    "\n",
    "print(\"üß† ÈïøÊúüËÆ∞ÂøÜÁ≠ñÁï•:\")\n",
    "print(\"- Ëá™Âä®ÊèêÂèñÂØπËØù‰∏≠ÁöÑÈáçË¶Å‰∫ãÂÆû\")\n",
    "print(\"- ÊåâÁ±ªÂà´ÁªÑÁªáËÆ∞ÂøÜÔºà‰∏™‰∫∫‰ø°ÊÅØ„ÄÅÂÅèÂ•Ω„ÄÅÂéÜÂè≤Á≠âÔºâ\")\n",
    "print(\"- Ë∑®‰ºöËØù‰øùÂ≠òÂíåÊ£ÄÁ¥¢ÈáçË¶Å‰ø°ÊÅØ\")\n",
    "print(\"- ÊîØÊåÅËØ≠‰πâÊêúÁ¥¢ÔºàÈúÄË¶ÅÈÖçÁΩÆembeddingsÔºâ\")\n",
    "\n",
    "# ÊºîÁ§∫‰øùÂ≠òÂíåÊ£ÄÁ¥¢\n",
    "print(\"\\nÊºîÁ§∫:\")\n",
    "print(\"‰øùÂ≠ò:\", save_func(\"Áî®Êà∑ÂñúÊ¨¢ÂñùÂíñÂï°\", \"preferences\"))\n",
    "print(\"‰øùÂ≠ò:\", save_func(\"Áî®Êà∑‰ΩèÂú®Âåó‰∫¨\", \"personal\"))\n",
    "print(\"Ê£ÄÁ¥¢:\", recall_func(\"Áî®Êà∑ÂÅèÂ•Ω\", \"preferences\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ca2e9d",
   "metadata": {},
   "source": [
    "### üéØ ÊúÄ‰Ω≥ÂÆûË∑µÔºöÁªºÂêàËß£ÂÜ≥ÊñπÊ°à\n",
    "\n",
    "Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÔºåÈÄöÂ∏∏‰ºöÁªÑÂêàÂ§öÁßçÁ≠ñÁï•Ôºö\n",
    "\n",
    "```python\n",
    "# Êé®ËçêÁöÑÂàÜÂ±ÇËÆ∞ÂøÜÁÆ°ÁêÜÁ≠ñÁï•\n",
    "def create_production_agent():\n",
    "    \"\"\"\n",
    "    Áîü‰∫ßÁ∫ßËÆ∞ÂøÜÁÆ°ÁêÜAgent\n",
    "    - Áü≠ÊúüËÆ∞ÂøÜÔºötrim + summarize\n",
    "    - ‰∏≠ÊúüËÆ∞ÂøÜÔºöÈáçË¶Å‰ø°ÊÅØÊèêÂèñ\n",
    "    - ÈïøÊúüËÆ∞ÂøÜÔºöË∑®‰ºöËØù‰∫ãÂÆûÂ≠òÂÇ®\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. TrimÁ≠ñÁï•ÔºöÁ´ãÂç≥ÈôêÂà∂tokenÊï∞Èáè\n",
    "    def trim_hook(state):\n",
    "        return trim_messages(\n",
    "            state[\"messages\"],\n",
    "            max_tokens=1000,  # Âü∫Á°ÄÈôêÂà∂\n",
    "            strategy=\"last\"\n",
    "        )\n",
    "    \n",
    "    # 2. SummaryÁ≠ñÁï•Ôºö‰øùÁïô‰ø°ÊÅØÁ≤æÂçé\n",
    "    def smart_hook(state):\n",
    "        messages = state[\"messages\"]\n",
    "        if len(messages) > 10:\n",
    "            # ÊëòË¶Å + trim\n",
    "            summary = summarize_early_messages(messages[:-6])\n",
    "            recent = messages[-6:]\n",
    "            return [summary] + recent\n",
    "        else:\n",
    "            # ‰ªÖtrim\n",
    "            return trim_messages(messages, max_tokens=800)\n",
    "    \n",
    "    # 3. ÈïøÊúüËÆ∞ÂøÜÔºöËá™Âä®ÊèêÂèñÈáçË¶Å‰ø°ÊÅØ\n",
    "    def extract_facts(state):\n",
    "        # ÂàÜÊûêÊúÄËøëÂØπËØùÔºåÊèêÂèñÈáçË¶Å‰∫ãÂÆûÂà∞store\n",
    "        pass\n",
    "    \n",
    "    return \"ÁªºÂêàÁ≠ñÁï•Agent\"\n",
    "```\n",
    "\n",
    "### üìä ÊÄßËÉΩÂØπÊØî\n",
    "\n",
    "| Á≠ñÁï• | Âª∂Ëøü | ÊàêÊú¨ | ‰ø°ÊÅØ‰øùÁïô | ÂÆûÁé∞ÈöæÂ∫¶ |\n",
    "|------|------|------|----------|----------|\n",
    "| Trim | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê |\n",
    "| Delete | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |\n",
    "| Summary | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n",
    "| Long-term | ‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f722a1d6-e73c-4023-86ed-8b07d392278d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add 3 and 4.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  add (call_VoBRTeymaaryV18Wo8RhfJJW)\n",
      " Call ID: call_VoBRTeymaaryV18Wo8RhfJJW\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "7\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The sum of 3 and 4 is 7.\n"
     ]
    }
   ],
   "source": [
    "# Specify a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Specify an input\n",
    "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
    "\n",
    "# Run\n",
    "messages = react_graph_memory.invoke({\"messages\": messages},config)\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91a8a16-6bf1-48e2-a889-ae04a37c7a2b",
   "metadata": {},
   "source": [
    "If we pass the same `thread_id`, then we can proceed from from the previously logged state checkpoint! \n",
    "\n",
    "In this case, the above conversation is captured in the thread.\n",
    "\n",
    "The `HumanMessage` we pass (`\"Multiply that by 2.\"`) is appended to the above conversation.\n",
    "\n",
    "So, the model now know that `that` refers to the `The sum of 3 and 4 is 7.`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee38c6ef-8bfb-4c66-9214-6f474c9b8451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add 3 and 4.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  add (call_VoBRTeymaaryV18Wo8RhfJJW)\n",
      " Call ID: call_VoBRTeymaaryV18Wo8RhfJJW\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "7\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The sum of 3 and 4 is 7.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply that by 2.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_iJKrSjRKw5iNdEloQmqk8vvf)\n",
      " Call ID: call_iJKrSjRKw5iNdEloQmqk8vvf\n",
      "  Args:\n",
      "    a: 7\n",
      "    b: 2\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "14\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The result of multiplying 7 by 2 is 14.\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Multiply that by 2.\")]\n",
    "messages = react_graph_memory.invoke({\"messages\": messages}, config)\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b7774e-566f-4c92-9429-ed953bcacaa5",
   "metadata": {},
   "source": [
    "## LangGraph Studio\n",
    "\n",
    "\n",
    "**‚ö†Ô∏è DISCLAIMER**\n",
    "\n",
    "Since the filming of these videos, we've updated Studio so that it can be run locally and opened in your browser. This is now the preferred way to run Studio (rather than using the Desktop App as shown in the video). See documentation [here](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server) on the local development server and [here](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server). To start the local development server, run the following command in your terminal in the `module-1/studio/` directory in this module:\n",
    "\n",
    "```\n",
    "langgraph dev\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0928ece4",
   "metadata": {},
   "source": [
    "### üé® ‰ªÄ‰πàÊòØ LangGraph StudioÔºü\n",
    "\n",
    "**LangGraph Studio** ÊòØ‰∏Ä‰∏™Âº∫Â§ßÁöÑÂèØËßÜÂåñÂºÄÂèëÂ∑•ÂÖ∑Ôºå‰∏ìÈó®Áî®‰∫éÊûÑÂª∫„ÄÅË∞ÉËØïÂíåÁõëÊéßLangGraphÂ∫îÁî®„ÄÇ\n",
    "\n",
    "### üöÄ ‰∏ªË¶ÅÂäüËÉΩ\n",
    "\n",
    "1. **ÂèØËßÜÂåñÂõæÁªìÊûÑ**\n",
    "   - Áõ¥ËßÇÊòæÁ§∫ÂõæÁöÑËäÇÁÇπÂíåËæπ\n",
    "   - ÂÆûÊó∂Êü•ÁúãÊï∞ÊçÆÊµÅ\n",
    "   - Ë∞ÉËØïÂõæÁöÑÊâßË°åË∑ØÂæÑ\n",
    "\n",
    "2. **‰∫§‰∫íÂºèË∞ÉËØï**\n",
    "   - ËÆæÁΩÆÊñ≠ÁÇπÊöÇÂÅúÊâßË°å\n",
    "   - Ê£ÄÊü•ÊØè‰∏™Ê≠•È™§ÁöÑÁä∂ÊÄÅ\n",
    "   - ‰øÆÊîπÁä∂ÊÄÅÂÄºËøõË°åÊµãËØï\n",
    "\n",
    "3. **ÂÆûÊó∂ÁõëÊéß**\n",
    "   - Êü•ÁúãÊâßË°åÊó•Âøó\n",
    "   - ÁõëÊéßÊÄßËÉΩÊåáÊ†á\n",
    "   - ËøΩË∏™ÈîôËØØÂíåÂºÇÂ∏∏\n",
    "\n",
    "4. **Áä∂ÊÄÅÁÆ°ÁêÜ**\n",
    "   - Êü•ÁúãcheckpointsÂéÜÂè≤\n",
    "   - Êó∂Èó¥ÊóÖË°åË∞ÉËØï\n",
    "   - Áä∂ÊÄÅÂõûÊªöÂíåÂàÜÂèâ\n",
    "\n",
    "### üì¶ ‰∏§ÁßçËøêË°åÊñπÂºè\n",
    "\n",
    "#### 1. **Êú¨Âú∞ÂºÄÂèëÊúçÂä°Âô®ÔºàÊé®ËçêÔºâ**\n",
    "```bash\n",
    "# Âú®È°πÁõÆÁõÆÂΩï‰∏≠ËøêË°å\n",
    "langgraph dev\n",
    "```\n",
    "- üåê Âú®ÊµèËßàÂô®‰∏≠ÊâìÂºÄ `http://localhost:3000`\n",
    "- üîÑ ÁÉ≠ÈáçËΩΩÔºå‰ª£Á†Å‰øÆÊîπÁ´ãÂç≥ÁîüÊïà\n",
    "- üõ†Ô∏è ÂÆåÊï¥ÁöÑÂºÄÂèë‰ΩìÈ™å\n",
    "\n",
    "#### 2. **Ê°åÈù¢Â∫îÁî®ÔºàÂ∑≤ÂºÉÁî®Ôºâ**\n",
    "- ‚ö†Ô∏è ÊóßÁâàÊú¨‰ΩøÁî®Ê°åÈù¢Â∫îÁî®\n",
    "- üì± Áé∞Âú®Êé®Ëçê‰ΩøÁî®WebÁâàÊú¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c66214",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Â¶Ç‰Ωï‰ΩøÁî® LangGraph Studio\n",
    "\n",
    "#### Ê≠•È™§1: ÂÆâË£Ö‰æùËµñ\n",
    "```bash\n",
    "pip install langgraph-cli\n",
    "```\n",
    "\n",
    "#### Ê≠•È™§2: ÂáÜÂ§áÈ°πÁõÆÈÖçÁΩÆ\n",
    "ÂàõÂª∫ `langgraph.json` ÈÖçÁΩÆÊñá‰ª∂Ôºö\n",
    "```json\n",
    "{\n",
    "  \"dependencies\": [\".\"],\n",
    "  \"graphs\": {\n",
    "    \"agent\": \"./your_graph.py:graph\"\n",
    "  },\n",
    "  \"env\": \".env\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### Ê≠•È™§3: ÂêØÂä®ÂºÄÂèëÊúçÂä°Âô®\n",
    "```bash\n",
    "# Âú®ÂåÖÂê´langgraph.jsonÁöÑÁõÆÂΩï‰∏≠ËøêË°å\n",
    "langgraph dev\n",
    "```\n",
    "\n",
    "#### Ê≠•È™§4: Âú®ÊµèËßàÂô®‰∏≠ËÆøÈóÆ\n",
    "- üåê ÊâìÂºÄ `http://localhost:3000`\n",
    "- üéØ ÈÄâÊã©‰Ω†ÁöÑÂõæËøõË°åË∞ÉËØï\n",
    "- üîç ÂèØËßÜÂåñÂõæÁªìÊûÑÂíåÊâßË°åÊµÅÁ®ã\n",
    "\n",
    "### üéØ Studio‰∏≠ËÉΩÁúãÂà∞‰ªÄ‰πàÔºü\n",
    "\n",
    "1. **ÂõæÂèØËßÜÂåñÁïåÈù¢**\n",
    "   - ËäÇÁÇπÂíåËæπÁöÑÂõæÂΩ¢Ë°®Á§∫\n",
    "   - Êï∞ÊçÆÊµÅÂä®ÁöÑÂÆûÊó∂ÂèØËßÜÂåñ\n",
    "   - ÊâßË°åË∑ØÂæÑÁöÑÈ´ò‰∫ÆÊòæÁ§∫\n",
    "\n",
    "2. **Áä∂ÊÄÅÊ£ÄÊü•Âô®**\n",
    "   - ÊØè‰∏™checkpointÁöÑËØ¶ÁªÜÁä∂ÊÄÅ\n",
    "   - Ê∂àÊÅØÂéÜÂè≤ÁöÑÂÆåÊï¥ËÆ∞ÂΩï\n",
    "   - ‰∏≠Èó¥ÁªìÊûúÁöÑÊü•Áúã\n",
    "\n",
    "3. **Ë∞ÉËØïÊéßÂà∂Âè∞**\n",
    "   - ËÆæÁΩÆÊñ≠ÁÇπÊöÇÂÅúÊâßË°å\n",
    "   - ÂçïÊ≠•Ë∞ÉËØïÊØè‰∏™ËäÇÁÇπ\n",
    "   - ‰øÆÊîπÁä∂ÊÄÅÂÄºËøõË°åÊµãËØï\n",
    "\n",
    "4. **ÊÄßËÉΩÁõëÊéß**\n",
    "   - ÊâßË°åÊó∂Èó¥ÁªüËÆ°\n",
    "   - ÂÜÖÂ≠ò‰ΩøÁî®ÊÉÖÂÜµ\n",
    "   - ÈîôËØØÊó•ÂøóËøΩË∏™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5cbbf0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Â∑≤ÂàõÂª∫ LangGraph Studio ÈÖçÁΩÆÊñá‰ª∂: /home/echo/workspace/langchain-academy/module-1/langgraph.json\n",
      "\n",
      "üìã ÈÖçÁΩÆÂÜÖÂÆπ:\n",
      "{\n",
      "  \"dependencies\": [\n",
      "    \".\"\n",
      "  ],\n",
      "  \"graphs\": {\n",
      "    \"memory_agent\": \"./agent-memory.ipynb:react_graph_memory\",\n",
      "    \"basic_agent\": \"./agent-memory.ipynb:react_graph\"\n",
      "  },\n",
      "  \"env\": \".env\"\n",
      "}\n",
      "\n",
      "üöÄ ‰∏ã‰∏ÄÊ≠•Êìç‰Ωú:\n",
      "1. ÊâìÂºÄÁªàÁ´ØÔºåÂàáÊç¢Âà∞ module-1 ÁõÆÂΩï\n",
      "2. ËøêË°åÂëΩ‰ª§: langgraph dev\n",
      "3. Âú®ÊµèËßàÂô®‰∏≠ÊâìÂºÄ: http://localhost:3000\n",
      "4. ÈÄâÊã© memory_agent Êàñ basic_agent ËøõË°åÂèØËßÜÂåñË∞ÉËØï\n",
      "\n",
      "üí° Studio ÁöÑ‰ºòÂäø:\n",
      "- ÂÆûÊó∂Êü•ÁúãagentÁöÑÂÜ≥Á≠ñËøáÁ®ã\n",
      "- ÂèØËßÜÂåñÊ∂àÊÅØÊµÅÂíåÁä∂ÊÄÅÂèòÂåñ\n",
      "- ËÆæÁΩÆÊñ≠ÁÇπËøõË°åË∞ÉËØï\n",
      "- Êü•ÁúãÊØè‰∏™checkpointÁöÑËØ¶ÁªÜ‰ø°ÊÅØ\n"
     ]
    }
   ],
   "source": [
    "# ‰∏∫ÂΩìÂâçÈ°πÁõÆÂàõÂª∫ LangGraph Studio ÈÖçÁΩÆ\n",
    "import json\n",
    "import os\n",
    "\n",
    "def create_langgraph_config():\n",
    "    \"\"\"‰∏∫ÂΩìÂâçÁöÑagent-memoryÈ°πÁõÆÂàõÂª∫StudioÈÖçÁΩÆ\"\"\"\n",
    "    \n",
    "    # ÈÖçÁΩÆÊñá‰ª∂ÂÜÖÂÆπ\n",
    "    config = {\n",
    "        \"dependencies\": [\".\"],\n",
    "        \"graphs\": {\n",
    "            \"memory_agent\": \"./agent-memory.ipynb:react_graph_memory\",\n",
    "            \"basic_agent\": \"./agent-memory.ipynb:react_graph\"\n",
    "        },\n",
    "        \"env\": \".env\"\n",
    "    }\n",
    "    \n",
    "    # ÂΩìÂâçmodule-1ÁõÆÂΩïÁöÑË∑ØÂæÑ\n",
    "    module1_path = \"/home/echo/workspace/langchain-academy/module-1\"\n",
    "    config_path = os.path.join(module1_path, \"langgraph.json\")\n",
    "    \n",
    "    # ÂÜôÂÖ•ÈÖçÁΩÆÊñá‰ª∂\n",
    "    try:\n",
    "        with open(config_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"‚úÖ Â∑≤ÂàõÂª∫ LangGraph Studio ÈÖçÁΩÆÊñá‰ª∂: {config_path}\")\n",
    "        print(\"\\nüìã ÈÖçÁΩÆÂÜÖÂÆπ:\")\n",
    "        print(json.dumps(config, indent=2, ensure_ascii=False))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ÂàõÂª∫ÈÖçÁΩÆÊñá‰ª∂Â§±Ë¥•: {e}\")\n",
    "    \n",
    "    return config_path\n",
    "\n",
    "# ÂàõÂª∫ÈÖçÁΩÆÊñá‰ª∂\n",
    "config_path = create_langgraph_config()\n",
    "\n",
    "print(\"\\nüöÄ ‰∏ã‰∏ÄÊ≠•Êìç‰Ωú:\")\n",
    "print(\"1. ÊâìÂºÄÁªàÁ´ØÔºåÂàáÊç¢Âà∞ module-1 ÁõÆÂΩï\")\n",
    "print(\"2. ËøêË°åÂëΩ‰ª§: langgraph dev\")\n",
    "print(\"3. Âú®ÊµèËßàÂô®‰∏≠ÊâìÂºÄ: http://localhost:3000\")\n",
    "print(\"4. ÈÄâÊã© memory_agent Êàñ basic_agent ËøõË°åÂèØËßÜÂåñË∞ÉËØï\")\n",
    "\n",
    "print(\"\\nüí° Studio ÁöÑ‰ºòÂäø:\")\n",
    "print(\"- ÂÆûÊó∂Êü•ÁúãagentÁöÑÂÜ≥Á≠ñËøáÁ®ã\")\n",
    "print(\"- ÂèØËßÜÂåñÊ∂àÊÅØÊµÅÂíåÁä∂ÊÄÅÂèòÂåñ\") \n",
    "print(\"- ËÆæÁΩÆÊñ≠ÁÇπËøõË°åË∞ÉËØï\")\n",
    "print(\"- Êü•ÁúãÊØè‰∏™checkpointÁöÑËØ¶ÁªÜ‰ø°ÊÅØ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26669505",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Ëß£ÂÜ≥ \"langgraphÔºöÊú™ÊâæÂà∞ÂëΩ‰ª§\" ÈóÆÈ¢ò\n",
    "\n",
    "Â¶ÇÊûúÈÅáÂà∞ `langgraph` ÂëΩ‰ª§Êú™ÊâæÂà∞ÁöÑÈîôËØØÔºåÈúÄË¶ÅÂÖàÂÆâË£Ö LangGraph CLIÔºö\n",
    "\n",
    "### ÊñπÊ≥ï1: ÂÆâË£Ö LangGraph CLIÔºàÊé®ËçêÔºâ\n",
    "```bash\n",
    "# ÂÆâË£Ö LangGraph CLI Â∑•ÂÖ∑\n",
    "pip install langgraph-cli\n",
    "\n",
    "# È™åËØÅÂÆâË£ÖÊàêÂäü\n",
    "langgraph --version\n",
    "```\n",
    "\n",
    "### ÊñπÊ≥ï2: Â¶ÇÊûúÊñπÊ≥ï1‰∏çË°åÔºåÂ∞ùËØï‰ª•‰∏ãÊ≠•È™§\n",
    "```bash\n",
    "# 1. Êõ¥Êñ∞ pip\n",
    "pip install --upgrade pip\n",
    "\n",
    "# 2. ÂÆâË£Ö LangGraph CLI\n",
    "pip install langgraph-cli\n",
    "\n",
    "# 3. Â¶ÇÊûúËøòÊòØ‰∏çË°åÔºåÂ∞ùËØï‰ΩøÁî® pipxÔºàÊé®ËçêÁî®‰∫éCLIÂ∑•ÂÖ∑Ôºâ\n",
    "pip install pipx\n",
    "pipx install langgraph-cli\n",
    "```\n",
    "\n",
    "### ÊñπÊ≥ï3: Âú®ÂΩìÂâçÁéØÂ¢É‰∏≠Áõ¥Êé•ÂÆâË£Ö\n",
    "Â¶ÇÊûú‰Ω†Âú®ËôöÊãüÁéØÂ¢É‰∏≠ÔºåÁ°Æ‰øùÁéØÂ¢ÉÂ∑≤ÊøÄÊ¥ªÔºö"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e5ba35",
   "metadata": {},
   "source": [
    "## ‚úÖ ÈóÆÈ¢òÂ∑≤Ëß£ÂÜ≥ÔºÅLangGraph Studio Áé∞Âú®ÂèØ‰ª•Ê≠£Â∏∏ËøêË°å\n",
    "\n",
    "ÂàöÊâçÈÅáÂà∞ÁöÑ `Could not find python file for graph` ÈîôËØØÊòØÂõ†‰∏∫ Studio Êó†Ê≥ïÁõ¥Êé•‰ªé `.ipynb` Êñá‰ª∂ÂØºÂÖ•ÂõæÂØπË±°„ÄÇ\n",
    "\n",
    "### üîß Ëß£ÂÜ≥ÊñπÊ°à\n",
    "\n",
    "1. **ÂàõÂª∫‰∫Ü `agent_graphs.py` Êñá‰ª∂**\n",
    "   - Â∞Ü notebook ‰∏≠ÁöÑÂõæÂÆö‰πâÂØºÂá∫Âà∞ Python Êñá‰ª∂\n",
    "   - ÂåÖÂê´‰∫Ü `basic_agent` Âíå `memory_agent` ‰∏§‰∏™Âõæ\n",
    "\n",
    "2. **Êõ¥Êñ∞‰∫Ü `langgraph.json` ÈÖçÁΩÆ**\n",
    "   ```json\n",
    "   {\n",
    "     \"dependencies\": [\".\"],\n",
    "     \"graphs\": {\n",
    "       \"memory_agent\": \"./agent_graphs.py:memory_agent\",\n",
    "       \"basic_agent\": \"./agent_graphs.py:basic_agent\"\n",
    "     },\n",
    "     \"env\": \".env\"\n",
    "   }\n",
    "   ```\n",
    "\n",
    "### üöÄ Áé∞Âú®‰Ω†ÂèØ‰ª•Ôºö\n",
    "\n",
    "1. **ÈáçÊñ∞ÂêØÂä® Studio**Ôºö\n",
    "   ```bash\n",
    "   cd /home/echo/workspace/langchain-academy/module-1\n",
    "   langgraph dev\n",
    "   ```\n",
    "\n",
    "2. **Âú®ÊµèËßàÂô®‰∏≠ËÆøÈóÆ**Ôºö\n",
    "   - üåê Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "   - üìö API Docs: http://127.0.0.1:2024/docs\n",
    "\n",
    "3. **ÈÄâÊã©ÂõæËøõË°åË∞ÉËØï**Ôºö\n",
    "   - `memory_agent` - Â∏¶ËÆ∞ÂøÜÂäüËÉΩÁöÑ‰ª£ÁêÜ\n",
    "   - `basic_agent` - Âü∫Á°Ä‰ª£ÁêÜÔºàÊó†ËÆ∞ÂøÜÔºâ\n",
    "\n",
    "### üí° ÈáçË¶ÅÊèêÁ§∫\n",
    "\n",
    "- ‚úÖ **ÈóÆÈ¢òÂ∑≤Ëß£ÂÜ≥**ÔºöStudio Áé∞Âú®Â∫îËØ•ËÉΩÊ≠£Â∏∏Âä†ËΩΩÂõæ\n",
    "- üîÑ **ÂäüËÉΩÂÆåÊï¥**ÔºöÊâÄÊúâÂéüÊúâÂäüËÉΩÈÉΩ‰øùÊåÅ‰∏çÂèò\n",
    "- üéØ **ÂèØËßÜÂåñË∞ÉËØï**ÔºöÁé∞Âú®ÂèØ‰ª•ÁúãÂà∞ÂÆåÊï¥ÁöÑÂõæÊâßË°åÊµÅÁ®ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "48ca7dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Ê£ÄÊü• LangGraph CLI ÊòØÂê¶Â∑≤ÂÆâË£Ö...\n",
      "‚ùå langgraph ÂëΩ‰ª§Êú™ÊâæÂà∞\n",
      "\n",
      "üì• ÂºÄÂßãÂÆâË£Ö LangGraph CLI...\n",
      "üîÑ Ê≠£Âú®ÂÆâË£Ö LangGraph CLI...\n",
      "‚úÖ LangGraph CLI ÂÆâË£ÖÊàêÂäü!\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting langgraph-cli\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d0/74/3798fc7f7672d04e911fcab22a53a551d474b7ca01eb773a52d5f63e94f2/langgraph_cli-0.3.6-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: click>=8.1.7 in /home/echo/miniconda3/envs/agent/lib/python3.12/site-packages (from langgraph-cli) (8.2.1)\n",
      "Requirement already satisfied: langgraph-sdk>=0.1.0 in /home/echo/miniconda3/envs/agent/lib/python3.12/site-packages (from langgraph-cli) (0.1.70)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /home/echo/miniconda3/envs/agent/lib/python3.12/site-packages (from langgraph-sdk>=0.1.0->langgraph-cli) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /home/echo/miniconda3/envs/agent/lib/python3.12/site-packages (from langgraph-sdk>=0.1.0->langgraph-cli) (3.10.18)\n",
      "Requirement already satisfied: anyio in /home/echo/miniconda3/envs/agent/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.0->langgraph-cli) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/echo/miniconda3/envs/agent/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.0->langgraph-cli) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /home/echo/miniconda3/envs/agent/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.0->langgraph-cli) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/echo/miniconda3/envs/agent/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.0->langgraph-cli) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /home/echo/miniconda3/envs/agent/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk>=0.1.0->langgraph-cli) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/echo/miniconda3/envs/agent/lib/python3.12/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk>=0.1.0->langgraph-cli) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /home/echo/miniconda3/envs/agent/lib/python3.12/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk>=0.1.0->langgraph-cli) (4.14.0)\n",
      "Installing collected packages: langgraph-cli\n",
      "Successfully installed langgraph-cli-0.3.6\n",
      "\n",
      "üì¶ ÁâàÊú¨‰ø°ÊÅØ: LangGraph CLI, version 0.3.6\n"
     ]
    }
   ],
   "source": [
    "# Áé∞Âú®Â∞±ÂÆâË£Ö LangGraph CLI\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def install_langgraph_cli():\n",
    "    \"\"\"ÂÆâË£Ö LangGraph CLI\"\"\"\n",
    "    try:\n",
    "        print(\"üîÑ Ê≠£Âú®ÂÆâË£Ö LangGraph CLI...\")\n",
    "        \n",
    "        # Â∞ùËØïÂÆâË£Ö langgraph-cli\n",
    "        result = subprocess.run([\n",
    "            sys.executable, \"-m\", \"pip\", \"install\", \"langgraph-cli\"\n",
    "        ], capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ LangGraph CLI ÂÆâË£ÖÊàêÂäü!\")\n",
    "            print(result.stdout)\n",
    "            \n",
    "            # È™åËØÅÂÆâË£Ö\n",
    "            version_result = subprocess.run([\n",
    "                \"langgraph\", \"--version\"\n",
    "            ], capture_output=True, text=True)\n",
    "            \n",
    "            if version_result.returncode == 0:\n",
    "                print(f\"üì¶ ÁâàÊú¨‰ø°ÊÅØ: {version_result.stdout.strip()}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è ÂÆâË£ÖÊàêÂäü‰ΩÜÂèØËÉΩÈúÄË¶ÅÈáçÂêØÁªàÁ´Ø\")\n",
    "                return False\n",
    "                \n",
    "        else:\n",
    "            print(f\"‚ùå ÂÆâË£ÖÂ§±Ë¥•: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ÂÆâË£ÖËøáÁ®ã‰∏≠Âá∫Èîô: {e}\")\n",
    "        return False\n",
    "\n",
    "def check_langgraph_command():\n",
    "    \"\"\"Ê£ÄÊü• langgraph ÂëΩ‰ª§ÊòØÂê¶ÂèØÁî®\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run([\"langgraph\", \"--version\"], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ LangGraph CLI Â∑≤ÂèØÁî®: {result.stdout.strip()}\")\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå langgraph ÂëΩ‰ª§Êú™ÊâæÂà∞\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Ê£ÄÊü•ÂëΩ‰ª§Êó∂Âá∫Èîô: {e}\")\n",
    "        return False\n",
    "\n",
    "# È¶ñÂÖàÊ£ÄÊü•ÊòØÂê¶Â∑≤ÁªèÂÆâË£Ö\n",
    "print(\"üîç Ê£ÄÊü• LangGraph CLI ÊòØÂê¶Â∑≤ÂÆâË£Ö...\")\n",
    "if not check_langgraph_command():\n",
    "    print(\"\\nüì• ÂºÄÂßãÂÆâË£Ö LangGraph CLI...\")\n",
    "    success = install_langgraph_cli()\n",
    "    \n",
    "    if not success:\n",
    "        print(\"\\nüõ†Ô∏è Â¶ÇÊûúËá™Âä®ÂÆâË£ÖÂ§±Ë¥•ÔºåËØ∑ÊâãÂä®ËøêË°å‰ª•‰∏ãÂëΩ‰ª§:\")\n",
    "        print(\"pip install langgraph-cli\")\n",
    "        print(\"\\nÊàñËÄÖÂú®ÁªàÁ´Ø‰∏≠ËøêË°å:\")\n",
    "        print(\"python -m pip install langgraph-cli\")\n",
    "else:\n",
    "    print(\"üéâ LangGraph CLI Â∑≤ÁªèÂèØÁî®ÔºÅ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3c11a6",
   "metadata": {},
   "source": [
    "### üîß Êõø‰ª£ÊñπÊ°àÔºöÂ¶ÇÊûúCLIÂÆâË£ÖÊúâÈóÆÈ¢ò\n",
    "\n",
    "Â¶ÇÊûú `langgraph-cli` ÂÆâË£ÖÊúâÈóÆÈ¢òÔºå‰Ω†ËøòÊúâ‰ª•‰∏ãÂá†ÁßçÈÄâÊã©Ôºö\n",
    "\n",
    "#### ÊñπÊ°à1: ‰ΩøÁî®ÁªàÁ´ØÊâãÂä®ÂÆâË£Ö\n",
    "```bash\n",
    "# Âú®ÁªàÁ´Ø‰∏≠ËøêË°åÔºàÊé®ËçêÔºâ\n",
    "pip install langgraph-cli\n",
    "\n",
    "# ÊàñËÄÖÊåáÂÆöPythonÁâàÊú¨\n",
    "python3 -m pip install langgraph-cli\n",
    "\n",
    "# È™åËØÅÂÆâË£Ö\n",
    "langgraph --version\n",
    "```\n",
    "\n",
    "#### ÊñπÊ°à2: ‰∏¥Êó∂Ëß£ÂÜ≥ÊñπÊ°à - Áõ¥Êé•ËøêË°åPython‰ª£Á†Å\n",
    "```python\n",
    "# Â¶ÇÊûúCLIÂ∑•ÂÖ∑ÊúâÈóÆÈ¢òÔºåÂèØ‰ª•Áõ¥Êé•Âú®notebook‰∏≠ËøêË°åÂõæ\n",
    "# Ëøô‰∏ç‰ºöÊúâStudioÁöÑÂèØËßÜÂåñÁïåÈù¢Ôºå‰ΩÜÂèØ‰ª•Ê≠£Â∏∏‰ΩøÁî®agent\n",
    "\n",
    "# Á§∫‰æãÔºöÁõ¥Êé•ÊµãËØïÊàë‰ª¨ÁöÑËÆ∞ÂøÜagent\n",
    "config = {\"configurable\": {\"thread_id\": \"test_memory\"}}\n",
    "messages = [HumanMessage(content=\"Hello, test memory!\")]\n",
    "result = react_graph_memory.invoke({\"messages\": messages}, config)\n",
    "```\n",
    "\n",
    "#### ÊñπÊ°à3: ‰ΩøÁî® Jupyter Êâ©Â±ïÔºàÂ¶ÇÊûúÊúâÁöÑËØùÔºâ\n",
    "Êüê‰∫õÁéØÂ¢ÉÂèØËÉΩÊèê‰æõ Jupyter Êâ©Â±ïÊù•ÂèØËßÜÂåñ LangGraph\n",
    "\n",
    "#### ÊñπÊ°à4: Ê£ÄÊü•ÁéØÂ¢ÉË∑ØÂæÑ\n",
    "```bash\n",
    "# Ê£ÄÊü•PythonÂåÖÂÆâË£ÖË∑ØÂæÑ\n",
    "python -c \"import sys; print(sys.path)\"\n",
    "\n",
    "# Ê£ÄÊü•pipÂÆâË£Ö‰ΩçÁΩÆ\n",
    "which pip\n",
    "which python\n",
    "\n",
    "# Â¶ÇÊûú‰ΩøÁî®condaÁéØÂ¢É\n",
    "conda list langgraph-cli\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6194198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ ÊµãËØïAgentËÆ∞ÂøÜÂäüËÉΩÔºàÊó†ÈúÄStudioÔºâ\n",
      "==================================================\n",
      "\n",
      "üí¨ Á¨¨‰∏ÄËΩÆÂØπËØù:\n",
      "Áî®Êà∑: ÊàëÂè´Âº†‰∏âÔºåËØ∑ËÆ∞‰ΩèÊàëÁöÑÂêçÂ≠ó\n",
      "Âä©Êâã: Êàë‰ºöËÆ∞‰Ωè‰Ω†ÁöÑÂêçÂ≠óÔºåÂº†‰∏â„ÄÇÂ¶Ç‰ΩïÂ∏ÆÂä©ÊÇ®Ôºü\n",
      "\n",
      "üí¨ Á¨¨‰∫åËΩÆÂØπËØù:\n",
      "Áî®Êà∑: ‰Ω†ËøòËÆ∞ÂæóÊàëÁöÑÂêçÂ≠óÂêóÔºü\n",
      "Âä©Êâã: ÂΩìÁÑ∂ÔºåÊÇ®Âè´Âº†‰∏â„ÄÇÊúâ‰ªÄ‰πàÊàëÂèØ‰ª•Â∏ÆÂøôÁöÑÂêóÔºü\n",
      "\n",
      "üìä ÂΩìÂâçthread‰∏≠ÁöÑÊ∂àÊÅØÊï∞Èáè: 4\n",
      "\n",
      "üí¨ Á¨¨‰∏âËΩÆÂØπËØù:\n",
      "Áî®Êà∑: ËÆ°ÁÆó 15 + 25\n",
      "Âä©Êâã: 15 Âä† 25 Á≠â‰∫é 40„ÄÇ\n",
      "\n",
      "üí¨ Á¨¨ÂõõËΩÆÂØπËØù:\n",
      "Áî®Êà∑: ÂàöÊâçÁöÑËÆ°ÁÆóÁªìÊûú‰πò‰ª•2ÊòØÂ§öÂ∞ëÔºü\n",
      "Âä©Êâã: ÂàöÊâçÁöÑËÆ°ÁÆóÁªìÊûú 40 ‰πò‰ª• 2 Á≠â‰∫é 80„ÄÇ\n",
      "\n",
      "üìà ÊúÄÁªàÊ∂àÊÅØÊï∞Èáè: 12\n",
      "\n",
      "‚úÖ ÊµãËØïÂÆåÊàêÔºÅÂç≥‰ΩøÊ≤°ÊúâStudioÔºåAgentÁöÑËÆ∞ÂøÜÂäüËÉΩ‰πüÊ≠£Â∏∏Â∑•‰Ωú\n",
      "üí° Studio‰∏ªË¶ÅÊòØÊèê‰æõÂèØËßÜÂåñÂíåË∞ÉËØïÂäüËÉΩÔºåÊ†∏ÂøÉagentÂäüËÉΩ‰∏ç‰æùËµñÂÆÉ\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Âç≥‰ΩøÊ≤°ÊúâStudioÔºåÊàë‰ª¨‰πüÂèØ‰ª•ÊµãËØïAgentÁöÑËÆ∞ÂøÜÂäüËÉΩÔºÅ\n",
    "\n",
    "def test_memory_without_studio():\n",
    "    \"\"\"Âú®Ê≤°ÊúâStudioÁöÑÊÉÖÂÜµ‰∏ãÊµãËØïËÆ∞ÂøÜÂäüËÉΩ\"\"\"\n",
    "    \n",
    "    print(\"üß™ ÊµãËØïAgentËÆ∞ÂøÜÂäüËÉΩÔºàÊó†ÈúÄStudioÔºâ\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # ‰ΩøÁî®Êàë‰ª¨‰πãÂâçÂàõÂª∫ÁöÑÂ∏¶ËÆ∞ÂøÜÁöÑÂõæ\n",
    "    config = {\"configurable\": {\"thread_id\": \"test_without_studio\"}}\n",
    "    \n",
    "    # Á¨¨‰∏ÄËΩÆÂØπËØù\n",
    "    print(\"\\nüí¨ Á¨¨‰∏ÄËΩÆÂØπËØù:\")\n",
    "    messages = [HumanMessage(content=\"ÊàëÂè´Âº†‰∏âÔºåËØ∑ËÆ∞‰ΩèÊàëÁöÑÂêçÂ≠ó\")]\n",
    "    result1 = react_graph_memory.invoke({\"messages\": messages}, config)\n",
    "    print(f\"Áî®Êà∑: {messages[0].content}\")\n",
    "    print(f\"Âä©Êâã: {result1['messages'][-1].content}\")\n",
    "    \n",
    "    # Á¨¨‰∫åËΩÆÂØπËØù - ÊµãËØïËÆ∞ÂøÜ\n",
    "    print(\"\\nüí¨ Á¨¨‰∫åËΩÆÂØπËØù:\")\n",
    "    messages = [HumanMessage(content=\"‰Ω†ËøòËÆ∞ÂæóÊàëÁöÑÂêçÂ≠óÂêóÔºü\")]\n",
    "    result2 = react_graph_memory.invoke({\"messages\": messages}, config)\n",
    "    print(f\"Áî®Êà∑: {messages[0].content}\")\n",
    "    print(f\"Âä©Êâã: {result2['messages'][-1].content}\")\n",
    "    \n",
    "    # Ê£ÄÊü•Áä∂ÊÄÅ\n",
    "    current_state = react_graph_memory.get_state(config)\n",
    "    print(f\"\\nüìä ÂΩìÂâçthread‰∏≠ÁöÑÊ∂àÊÅØÊï∞Èáè: {len(current_state.values['messages'])}\")\n",
    "    \n",
    "    # Á¨¨‰∏âËΩÆÂØπËØù - Êï∞Â≠¶ËÆ°ÁÆó\n",
    "    print(\"\\nüí¨ Á¨¨‰∏âËΩÆÂØπËØù:\")\n",
    "    messages = [HumanMessage(content=\"ËÆ°ÁÆó 15 + 25\")]\n",
    "    result3 = react_graph_memory.invoke({\"messages\": messages}, config)\n",
    "    print(f\"Áî®Êà∑: {messages[0].content}\")\n",
    "    print(f\"Âä©Êâã: {result3['messages'][-1].content}\")\n",
    "    \n",
    "    # Á¨¨ÂõõËΩÆÂØπËØù - ÊµãËØïËÆ°ÁÆóËÆ∞ÂøÜ\n",
    "    print(\"\\nüí¨ Á¨¨ÂõõËΩÆÂØπËØù:\")\n",
    "    messages = [HumanMessage(content=\"ÂàöÊâçÁöÑËÆ°ÁÆóÁªìÊûú‰πò‰ª•2ÊòØÂ§öÂ∞ëÔºü\")]\n",
    "    result4 = react_graph_memory.invoke({\"messages\": messages}, config)\n",
    "    print(f\"Áî®Êà∑: {messages[0].content}\")\n",
    "    print(f\"Âä©Êâã: {result4['messages'][-1].content}\")\n",
    "    \n",
    "    # ÊúÄÁªàÁä∂ÊÄÅÊ£ÄÊü•\n",
    "    final_state = react_graph_memory.get_state(config)\n",
    "    print(f\"\\nüìà ÊúÄÁªàÊ∂àÊÅØÊï∞Èáè: {len(final_state.values['messages'])}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ ÊµãËØïÂÆåÊàêÔºÅÂç≥‰ΩøÊ≤°ÊúâStudioÔºåAgentÁöÑËÆ∞ÂøÜÂäüËÉΩ‰πüÊ≠£Â∏∏Â∑•‰Ωú\")\n",
    "    print(\"üí° Studio‰∏ªË¶ÅÊòØÊèê‰æõÂèØËßÜÂåñÂíåË∞ÉËØïÂäüËÉΩÔºåÊ†∏ÂøÉagentÂäüËÉΩ‰∏ç‰æùËµñÂÆÉ\")\n",
    "\n",
    "# ËøêË°åÊµãËØï\n",
    "test_memory_without_studio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d87a92d",
   "metadata": {},
   "source": [
    "## üéØ Ëß£ÂÜ≥ \"langgraphÔºöÊú™ÊâæÂà∞ÂëΩ‰ª§\" ÈóÆÈ¢òÊÄªÁªì\n",
    "\n",
    "### ‚úÖ ÈóÆÈ¢òÂ∑≤Ëß£ÂÜ≥ÔºÅÊ†∏ÂøÉÂäüËÉΩÊ≠£Â∏∏\n",
    "\n",
    "Â¶Ç‰∏äÈù¢ÁöÑÊµãËØïÊâÄÁ§∫ÔºåÂç≥‰ΩøÈÅáÂà∞ `langgraph` ÂëΩ‰ª§ÈóÆÈ¢òÔºå**AgentÁöÑÊ†∏ÂøÉËÆ∞ÂøÜÂäüËÉΩ‰ªçÁÑ∂ÂÆåÂÖ®Ê≠£Â∏∏**Ôºö\n",
    "\n",
    "- ‚úÖ **ËÆ∞ÂøÜÂäüËÉΩÊ≠£Â∏∏**ÔºöAgentËÉΩËÆ∞‰ΩèÁî®Êà∑ÂêçÂ≠óÂº†‰∏â\n",
    "- ‚úÖ **ËÆ°ÁÆóÂäüËÉΩÊ≠£Â∏∏**ÔºöËÉΩÊ≠£Á°ÆÊâßË°åÊï∞Â≠¶ËøêÁÆó\n",
    "- ‚úÖ **‰∏ä‰∏ãÊñáÁ¥ØÁßØÊ≠£Â∏∏**ÔºöËÉΩÂºïÁî®‰πãÂâçÁöÑËÆ°ÁÆóÁªìÊûú\n",
    "- ‚úÖ **Áä∂ÊÄÅÊåÅ‰πÖÂåñÊ≠£Â∏∏**ÔºöÊ∂àÊÅØÂéÜÂè≤Ê≠£Á°Æ‰øùÂ≠ò\n",
    "\n",
    "### üîß ÂÖ≥‰∫é LangGraph Studio\n",
    "\n",
    "**ÈáçË¶ÅÊèêÈÜí**Ôºö\n",
    "- üéØ **StudioÊòØË∞ÉËØïÂ∑•ÂÖ∑**Ôºå‰∏çÊòØËøêË°åAgentÁöÑÂøÖÈúÄÂìÅ\n",
    "- üöÄ **AgentÊú¨Ë∫´ÂÆåÂÖ®Áã¨Á´ã**ÔºåÂèØ‰ª•Ê≠£Â∏∏Â∑•‰Ωú\n",
    "- üîç **StudioÊèê‰æõÂèØËßÜÂåñ**ÔºåÂ∏ÆÂä©ÁêÜËß£ÂíåË∞ÉËØï\n",
    "\n",
    "### üìã Â¶ÇÊûú‰ªçÊÉ≥‰ΩøÁî®StudioÔºåÂ∞ùËØï‰ª•‰∏ãÊ≠•È™§Ôºö\n",
    "\n",
    "1. **Âú®ÁªàÁ´Ø‰∏≠ÊâãÂä®ÂÆâË£Ö**Ôºö\n",
    "   ```bash\n",
    "   pip install langgraph-cli\n",
    "   ```\n",
    "\n",
    "2. **Ê£ÄÊü•ÁéØÂ¢É**Ôºö\n",
    "   ```bash\n",
    "   which python\n",
    "   which pip\n",
    "   echo $PATH\n",
    "   ```\n",
    "\n",
    "3. **ÈáçÊñ∞Âä†ËΩΩÁéØÂ¢É**Ôºö\n",
    "   ```bash\n",
    "   source ~/.bashrc  # Êàñ ~/.zshrc\n",
    "   ```\n",
    "\n",
    "4. **‰ΩøÁî®ÁªùÂØπË∑ØÂæÑ**Ôºö\n",
    "   ```bash\n",
    "   python -m pip install langgraph-cli\n",
    "   ```\n",
    "\n",
    "### üéâ ÁªìËÆ∫\n",
    "\n",
    "‰Ω†ÁöÑAgentÂ≠¶‰π†ÊàêÊûúÂ∑≤ÁªèÂÆåÂÖ®ÊàêÂäüÔºÅËÆ∞ÂøÜÂäüËÉΩ„ÄÅÁä∂ÊÄÅÁÆ°ÁêÜ„ÄÅÊ∂àÊÅØÁ¥ØÁßØÈÉΩÊ≠£Â∏∏Â∑•‰Ωú„ÄÇStudioÂè™ÊòØÈî¶‰∏äÊ∑ªËä±ÁöÑË∞ÉËØïÂ∑•ÂÖ∑Ôºå‰∏çÂΩ±ÂìçÊ†∏ÂøÉÂäüËÉΩÁöÑ‰ΩøÁî®„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d093bc2",
   "metadata": {},
   "source": [
    "### üéØ Studio ÁöÑÂÆûÈôÖÂ∫îÁî®Âú∫ÊôØ\n",
    "\n",
    "#### 1. **Ë∞ÉËØïÂ§çÊùÇÁöÑAgentË°å‰∏∫**\n",
    "```python\n",
    "# ÂΩì‰Ω†ÁöÑAgentË°®Áé∞ÂºÇÂ∏∏Êó∂ÔºåÂèØ‰ª•ÈÄöËøáStudioÔºö\n",
    "- Êü•ÁúãÊØè‰∏™ËäÇÁÇπÁöÑËæìÂÖ•ËæìÂá∫\n",
    "- Ê£ÄÊü•Ê∂àÊÅØÂéÜÂè≤ÁöÑÁ¥ØÁßØËøáÁ®ã\n",
    "- ÂèëÁé∞Â∑•ÂÖ∑Ë∞ÉÁî®ÁöÑÈóÆÈ¢ò\n",
    "- ËßÇÂØüÁä∂ÊÄÅÂú®ÂêÑ‰∏™Ê≠•È™§ÁöÑÂèòÂåñ\n",
    "```\n",
    "\n",
    "#### 2. **‰ºòÂåñËÆ∞ÂøÜÁÆ°ÁêÜÁ≠ñÁï•**\n",
    "```python\n",
    "# ÂØπ‰∫éÊàë‰ª¨ÂàöÂ≠¶‰π†ÁöÑËÆ∞ÂøÜÁÆ°ÁêÜÔºö\n",
    "- ÂèØËßÜÂåñÊ∂àÊÅØÊï∞ÈáèÁöÑÂ¢ûÈïø\n",
    "- ËßÇÂØütrim/deleteÊìç‰ΩúÁöÑÊïàÊûú\n",
    "- ÁõëÊéßtoken‰ΩøÁî®ÊÉÖÂÜµ\n",
    "- ÊµãËØï‰∏çÂêåÁöÑËÆ∞ÂøÜÁ≠ñÁï•\n",
    "```\n",
    "\n",
    "#### 3. **ÁêÜËß£ÂõæÁöÑÊâßË°åÊµÅÁ®ã**\n",
    "```python\n",
    "# StudioËÉΩÂ∏Æ‰Ω†ÁúãÊ∏ÖÔºö\n",
    "- Êù°‰ª∂ËæπÁöÑÈÄâÊã©ÈÄªËæë\n",
    "- Âæ™ÁéØÊâßË°åÁöÑÊ¨°Êï∞\n",
    "- Âπ∂Ë°åËäÇÁÇπÁöÑÊâßË°åÈ°∫Â∫è\n",
    "- ÈîôËØØÂ§ÑÁêÜÁöÑË∑ØÂæÑ\n",
    "```\n",
    "\n",
    "### üîç Studio vs ‰º†ÁªüË∞ÉËØï\n",
    "\n",
    "| ‰º†ÁªüË∞ÉËØïÊñπÂºè | LangGraph Studio |\n",
    "|-------------|------------------|\n",
    "| ÊâìÂç∞Êó•Âøó | ÂèØËßÜÂåñÂõæÁªìÊûÑ |\n",
    "| ÁåúÊµãÊâßË°åË∑ØÂæÑ | ÂÆûÊó∂Ë∑ØÂæÑÈ´ò‰∫Æ |\n",
    "| ÊâãÂä®Ê£ÄÊü•Áä∂ÊÄÅ | Ëá™Âä®Áä∂ÊÄÅÊ£ÄÊü•Âô® |\n",
    "| ÈáçÂ§çËøêË°åÊµãËØï | Êó∂Èó¥ÊóÖË°åË∞ÉËØï |\n",
    "| ÈùôÊÄÅ‰ª£Á†ÅÂàÜÊûê | Âä®ÊÄÅÊâßË°åÁõëÊéß |\n",
    "\n",
    "### üí° ÊúÄ‰Ω≥ÂÆûË∑µÂª∫ËÆÆ\n",
    "\n",
    "1. **ÂºÄÂèëÈò∂ÊÆµ**Ôºö‰ΩøÁî®StudioËøõË°åÂø´ÈÄüÂéüÂûãÈ™åËØÅ\n",
    "2. **Ë∞ÉËØïÈò∂ÊÆµ**ÔºöÂà©Áî®Êñ≠ÁÇπÂíåÁä∂ÊÄÅÊ£ÄÊü•ÊâæÈóÆÈ¢ò\n",
    "3. **‰ºòÂåñÈò∂ÊÆµ**ÔºöÁõëÊéßÊÄßËÉΩÊåáÊ†á‰ºòÂåñÊâßË°åÊïàÁéá\n",
    "4. **ÈÉ®ÁΩ≤Ââç**ÔºöÂÖ®Èù¢ÊµãËØïÂêÑÁßçËæπÁïåÊÉÖÂÜµ\n",
    "\n",
    "**ÊÄªÁªì**ÔºöLangGraph StudioÊòØAgentÂºÄÂèë‰∏çÂèØÊàñÁº∫ÁöÑÂèØËßÜÂåñË∞ÉËØïÂ∑•ÂÖ∑ÔºåËÆ©Â§çÊùÇÁöÑÂõæÊâßË°åËøáÁ®ãÂèòÂæóÊ∏ÖÊô∞ÂèØËßÅÔºÅ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d72986c-ff6f-4f81-b585-d268e2710e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
