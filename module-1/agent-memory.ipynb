{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13cd1c3e",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-1/agent-memory.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239417-lesson-7-agent-with-memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c451ffd-a18b-4412-85fa-85186824dd03",
   "metadata": {},
   "source": [
    "# Agent memory\n",
    "\n",
    "## Review\n",
    "\n",
    "Previously, we built an agent that can:\n",
    "\n",
    "* `act` - let the model call specific tools \n",
    "* `observe` - pass the tool output back to the model \n",
    "* `reason` - let the model reason about the tool output to decide what to do next (e.g., call another tool or just respond directly)\n",
    "\n",
    "![Screenshot 2024-08-21 at 12.45.32 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbab7453080e6802cd1703_agent-memory1.png)\n",
    "\n",
    "## Goals\n",
    "\n",
    "Now, we're going extend our agent by introducing memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b4b45b-cbaa-41b1-b3ed-f6b0645be3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain_openai langchain_core langgraph langgraph-prebuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b0cfa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eff247-a2aa-4f7a-8be1-73dfebfecc63",
   "metadata": {},
   "source": [
    "We'll use [LangSmith](https://docs.smith.langchain.com/) for [tracing](https://docs.smith.langchain.com/concepts/tracing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74ef2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5f123b-db5d-4816-a6a3-2e4247611512",
   "metadata": {},
   "source": [
    "This follows what we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da677753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "proxy = os.environ.get(\"PROXY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46647bbe-def5-4ea7-a315-1de8d97c8288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "# This will be a tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "tools = [add, multiply, divide]\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", openai_proxy= f\"http://{proxy}\")\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9092b40-20c4-4872-b0ed-be1b53a15ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# System message\n",
    "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n",
    "\n",
    "# Node\n",
    "def assistant(state: MessagesState):\n",
    "   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "771123a3-91ac-4076-92c0-93bcd69cf048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition, ToolNode\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Graph\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "react_graph = builder.compile()\n",
    "\n",
    "# Show\n",
    "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e830b7ae-3673-4cc6-8627-4740b7b8b217",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "Let's run our agent, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "596a71a0-1337-44d4-971d-f80c367bd868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add 3 and 4.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  add (call_DGomdWl3n8c0UIvOaHJCaTRZ)\n",
      " Call ID: call_DGomdWl3n8c0UIvOaHJCaTRZ\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "7\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The sum of 3 and 4 is 7.\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
    "messages = react_graph.invoke({\"messages\": messages})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f8128c-f4a5-4dee-b20b-3245bd33f6b3",
   "metadata": {},
   "source": [
    "Now, let's multiply by 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b41cc1d7-e6de-4d86-8958-8cf7446f4c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply that by 2.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Could you please provide me with the specific number or expression you'd like to multiply by 2?\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Multiply that by 2.\")]\n",
    "messages = react_graph.invoke({\"messages\": messages})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e65f3c-e1dc-4a62-b8ab-02b33a6ff268",
   "metadata": {},
   "source": [
    "We don't retain memory of 7 from our initial chat!\n",
    "\n",
    "This is because [state is transient](https://github.com/langchain-ai/langgraph/discussions/352#discussioncomment-9291220) to a single graph execution.\n",
    "\n",
    "Of course, this limits our ability to have multi-turn conversations with interruptions. \n",
    "\n",
    "We can use [persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence/) to address this! \n",
    "\n",
    "LangGraph can use a checkpointer to automatically save the graph state after each step.\n",
    "\n",
    "This built-in persistence layer gives us memory, allowing LangGraph to pick up from the last state update. \n",
    "\n",
    "One of the easiest checkpointers to use is the `MemorySaver`, an in-memory key-value store for Graph state.\n",
    "\n",
    "All we need to do is simply compile the graph with a checkpointer, and our graph has memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "637fcd79-3896-42e4-9131-e03b123a0a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()\n",
    "react_graph_memory = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff8fc3bf-3999-47cb-af34-06b2b94d7192",
   "metadata": {},
   "source": [
    "When we use memory, we need to specify a `thread_id`.\n",
    "\n",
    "This `thread_id` will store our collection of graph states.\n",
    "\n",
    "Here is a cartoon:\n",
    "\n",
    "* The checkpointer write the state at every step of the graph\n",
    "* These checkpoints are saved in a thread \n",
    "* We can access that thread in the future using the `thread_id`\n",
    "\n",
    "![state.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e0e9f526b41a4ed9e2d28b_agent-memory2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173f157",
   "metadata": {},
   "source": [
    "## State上下文机制详解\n",
    "\n",
    "当使用checkpointer时，state中的信息确实会在每次invoke时加入到上下文中：\n",
    "\n",
    "### 1. **无记忆的情况（没有checkpointer）**\n",
    "```python\n",
    "# 每次invoke都是独立的，没有历史上下文\n",
    "messages = [HumanMessage(content=\"Add 3 and 4.\")]  # 第一次\n",
    "messages = react_graph.invoke({\"messages\": messages})  # 返回：7\n",
    "\n",
    "messages = [HumanMessage(content=\"Multiply that by 2.\")]  # 第二次\n",
    "messages = react_graph.invoke({\"messages\": messages})  # 不知道\"that\"指什么\n",
    "```\n",
    "\n",
    "### 2. **有记忆的情况（使用checkpointer）**\n",
    "```python\n",
    "# 使用相同thread_id，state会累积\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# 第一次invoke\n",
    "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
    "messages = react_graph_memory.invoke({\"messages\": messages}, config)\n",
    "# State保存：[HumanMessage(\"Add 3 and 4.\"), AIMessage(\"7\"), ...]\n",
    "\n",
    "# 第二次invoke - 新消息会添加到已有的state中\n",
    "messages = [HumanMessage(content=\"Multiply that by 2.\")]\n",
    "messages = react_graph_memory.invoke({\"messages\": messages}, config)\n",
    "# 实际传给模型的上下文：\n",
    "# [HumanMessage(\"Add 3 and 4.\"), AIMessage(\"7\"), ..., HumanMessage(\"Multiply that by 2.\")]\n",
    "```\n",
    "\n",
    "### 3. **MessagesState的累积机制**\n",
    "- `MessagesState`使用了reducer机制，新消息会**追加**到现有消息列表中\n",
    "- 每次invoke时，图会从thread中加载完整的历史状态\n",
    "- 新的输入消息会与历史消息合并，形成完整的对话上下文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15b81260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 第一次invoke前的state ===\n",
      "消息数量: 0\n",
      "\n",
      "=== 第一次invoke: Add 3 and 4 ===\n",
      "消息数量: 4\n",
      "  消息1: HumanMessage - Add 3 and 4....\n",
      "  消息2: AIMessage - ...\n",
      "  消息3: ToolMessage - 7...\n",
      "  消息4: AIMessage - The sum of 3 and 4 is 7....\n",
      "\n",
      "=== 第二次invoke: Multiply that by 2 ===\n",
      "消息数量: 8\n",
      "  消息1: HumanMessage - Add 3 and 4....\n",
      "  消息2: AIMessage - ...\n",
      "  消息3: ToolMessage - 7...\n",
      "  消息4: AIMessage - The sum of 3 and 4 is 7....\n",
      "  消息5: HumanMessage - Multiply that by 2....\n",
      "  消息6: AIMessage - ...\n",
      "  消息7: ToolMessage - 14...\n",
      "  消息8: AIMessage - The result of multiplying 7 by 2 is 14....\n"
     ]
    }
   ],
   "source": [
    "# 让我们查看state在每次invoke后的变化\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# 创建新的checkpointer来演示\n",
    "demo_memory = MemorySaver()\n",
    "demo_graph = builder.compile(checkpointer=demo_memory)\n",
    "demo_config = {\"configurable\": {\"thread_id\": \"demo\"}}\n",
    "\n",
    "print(\"=== 第一次invoke前的state ===\")\n",
    "try:\n",
    "    current_state = demo_graph.get_state(demo_config)\n",
    "    print(f\"消息数量: {len(current_state.values.get('messages', []))}\")\n",
    "except:\n",
    "    print(\"还没有state记录\")\n",
    "\n",
    "print(\"\\n=== 第一次invoke: Add 3 and 4 ===\")\n",
    "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
    "result1 = demo_graph.invoke({\"messages\": messages}, demo_config)\n",
    "\n",
    "# 查看第一次invoke后的state\n",
    "current_state = demo_graph.get_state(demo_config)\n",
    "print(f\"消息数量: {len(current_state.values['messages'])}\")\n",
    "for i, msg in enumerate(current_state.values['messages']):\n",
    "    print(f\"  消息{i+1}: {type(msg).__name__} - {msg.content[:50]}...\")\n",
    "\n",
    "print(\"\\n=== 第二次invoke: Multiply that by 2 ===\")\n",
    "messages = [HumanMessage(content=\"Multiply that by 2.\")]\n",
    "result2 = demo_graph.invoke({\"messages\": messages}, demo_config)\n",
    "\n",
    "# 查看第二次invoke后的state\n",
    "current_state = demo_graph.get_state(demo_config)\n",
    "print(f\"消息数量: {len(current_state.values['messages'])}\")\n",
    "for i, msg in enumerate(current_state.values['messages']):\n",
    "    print(f\"  消息{i+1}: {type(msg).__name__} - {msg.content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0b9e4",
   "metadata": {},
   "source": [
    "### 4. **技术原理：MessagesState的Reducer机制**\n",
    "\n",
    "`MessagesState`是一个特殊的状态类，它使用了**reducer**机制来处理消息累积：\n",
    "\n",
    "```python\n",
    "# MessagesState的定义（简化版）\n",
    "from typing import Annotated\n",
    "from operator import add\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add]  # 注意这里的add reducer\n",
    "```\n",
    "\n",
    "**关键点：**\n",
    "- `Annotated[list[BaseMessage], add]`：使用`add`作为reducer函数\n",
    "- 当新的messages传入时，会与现有的messages列表**相加**（追加）\n",
    "- 这就是为什么每次invoke都能保持完整的对话历史\n",
    "\n",
    "### 5. **与普通状态的对比**\n",
    "\n",
    "```python\n",
    "# 普通状态（会覆盖）\n",
    "class SimpleState(TypedDict):\n",
    "    value: int  # 没有reducer，新值会覆盖旧值\n",
    "\n",
    "# 带reducer的状态（会累积）\n",
    "class AccumulateState(TypedDict):\n",
    "    values: Annotated[list[int], add]  # 有reducer，新值会追加\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350fcc4a",
   "metadata": {},
   "source": [
    "## 🚨 上下文过长问题及解决方案\n",
    "\n",
    "当对话历史不断累积时，会遇到以下问题：\n",
    "1. **Token限制**：超出LLM的最大上下文窗口\n",
    "2. **性能下降**：长上下文会导致响应时间变慢、成本增加\n",
    "3. **注意力分散**：LLM在长上下文中容易被无关信息\"分心\"\n",
    "\n",
    "### 解决方案对比表\n",
    "\n",
    "| 方法 | 优点 | 缺点 | 适用场景 |\n",
    "|------|------|------|----------|\n",
    "| **Trim Messages** | 简单快速，立即生效 | 可能丢失重要信息 | 短期对话，信息价值递减 |\n",
    "| **Delete Messages** | 可精确控制删除内容 | 需要复杂逻辑判断 | 有明确过时信息的场景 |\n",
    "| **Summarize Messages** | 保留信息精华 | 增加计算开销，可能丢失细节 | 长期对话，信息密度高 |\n",
    "| **Long-term Memory** | 跨会话保存重要信息 | 实现复杂度高 | 个性化应用，重要事实记忆 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f60e08f",
   "metadata": {},
   "source": [
    "### 方案1: Trim Messages（裁剪消息）\n",
    "\n",
    "最简单的方法是限制消息历史的长度，保留最近的N条消息或N个tokens："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b388339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始消息数量: 20\n",
      "原始token数量（近似）: 231\n",
      "\n",
      "裁剪后消息数量: 17\n",
      "裁剪后token数量（近似）: 195\n",
      "\n",
      "保留的消息:\n",
      "  1. HumanMessage: 这是第2个问题，内容比较长，用来测试token计数功能...\n",
      "  2. AIMessage: 这是第2个回答，也包含了相当多的内容来模拟真实对话场景...\n",
      "  3. HumanMessage: 这是第3个问题，内容比较长，用来测试token计数功能...\n",
      "  4. AIMessage: 这是第3个回答，也包含了相当多的内容来模拟真实对话场景...\n",
      "  5. HumanMessage: 这是第4个问题，内容比较长，用来测试token计数功能...\n",
      "  6. AIMessage: 这是第4个回答，也包含了相当多的内容来模拟真实对话场景...\n",
      "  7. HumanMessage: 这是第5个问题，内容比较长，用来测试token计数功能...\n",
      "  8. AIMessage: 这是第5个回答，也包含了相当多的内容来模拟真实对话场景...\n",
      "  9. HumanMessage: 这是第6个问题，内容比较长，用来测试token计数功能...\n",
      "  10. AIMessage: 这是第6个回答，也包含了相当多的内容来模拟真实对话场景...\n",
      "  11. HumanMessage: 这是第7个问题，内容比较长，用来测试token计数功能...\n",
      "  12. AIMessage: 这是第7个回答，也包含了相当多的内容来模拟真实对话场景...\n",
      "  13. HumanMessage: 这是第8个问题，内容比较长，用来测试token计数功能...\n",
      "  14. AIMessage: 这是第8个回答，也包含了相当多的内容来模拟真实对话场景...\n",
      "  15. HumanMessage: 这是第9个问题，内容比较长，用来测试token计数功能...\n",
      "  16. AIMessage: 这是第9个回答，也包含了相当多的内容来模拟真实对话场景...\n",
      "  17. HumanMessage: 这是第10个问题，内容比较长，用来测试token计数功能...\n"
     ]
    }
   ],
   "source": [
    "# 方案1: 使用trim_messages限制上下文长度\n",
    "from langchain_core.messages.utils import trim_messages, count_tokens_approximately\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# 模拟一个很长的对话历史\n",
    "long_conversation = []\n",
    "for i in range(10):\n",
    "    long_conversation.extend([\n",
    "        HumanMessage(content=f\"这是第{i+1}个问题，内容比较长，用来测试token计数功能\"),\n",
    "        AIMessage(content=f\"这是第{i+1}个回答，也包含了相当多的内容来模拟真实对话场景\")\n",
    "    ])\n",
    "\n",
    "print(f\"原始消息数量: {len(long_conversation)}\")\n",
    "print(f\"原始token数量（近似）: {count_tokens_approximately(long_conversation)}\")\n",
    "\n",
    "# 使用trim_messages裁剪到指定token数量\n",
    "trimmed_messages = trim_messages(\n",
    "    long_conversation,\n",
    "    strategy=\"last\",  # 保留最后的消息\n",
    "    token_counter=count_tokens_approximately,\n",
    "    max_tokens=200,  # 限制为200个tokens\n",
    "    start_on=\"human\",  # 确保从human消息开始\n",
    "    end_on=(\"human\", \"tool\"),  # 确保以human或tool消息结束\n",
    ")\n",
    "\n",
    "print(f\"\\n裁剪后消息数量: {len(trimmed_messages)}\")\n",
    "print(f\"裁剪后token数量（近似）: {count_tokens_approximately(trimmed_messages)}\")\n",
    "\n",
    "# 显示保留的消息内容\n",
    "print(\"\\n保留的消息:\")\n",
    "for i, msg in enumerate(trimmed_messages):\n",
    "    print(f\"  {i+1}. {type(msg).__name__}: {msg.content[:30]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a29794b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 创建了带有消息裁剪功能的Agent\n",
      "📝 该Agent会在每次调用LLM前自动裁剪消息历史，保持在500 tokens以内\n"
     ]
    }
   ],
   "source": [
    "# 在Agent中集成trim messages\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# 定义pre_model_hook来自动裁剪消息\n",
    "def trim_messages_hook(state):\n",
    "    \"\"\"在每次调用LLM前自动裁剪消息历史\"\"\"\n",
    "    trimmed_messages = trim_messages(\n",
    "        state[\"messages\"],\n",
    "        strategy=\"last\",\n",
    "        token_counter=count_tokens_approximately,\n",
    "        max_tokens=500,  # 限制为500个tokens\n",
    "        start_on=\"human\",\n",
    "        end_on=(\"human\", \"tool\"),\n",
    "    )\n",
    "    return {\"llm_input_messages\": trimmed_messages}\n",
    "\n",
    "# 创建带有消息裁剪功能的Agent\n",
    "trimmed_checkpointer = MemorySaver()\n",
    "trimmed_agent = create_react_agent(\n",
    "    llm_with_tools,\n",
    "    tools,\n",
    "    pre_model_hook=trim_messages_hook,  # 使用trim hook\n",
    "    checkpointer=trimmed_checkpointer,\n",
    ")\n",
    "\n",
    "print(\"✅ 创建了带有消息裁剪功能的Agent\")\n",
    "print(\"📝 该Agent会在每次调用LLM前自动裁剪消息历史，保持在500 tokens以内\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04456c0c",
   "metadata": {},
   "source": [
    "### 方案2: Delete Messages（删除消息）\n",
    "\n",
    "可以从state中永久删除特定消息，比删除最早的消息或特定类型的消息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e398fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️ 删除消息策略:\n",
      "- 当消息超过6条时，自动删除最早的2条\n",
      "- 可以删除特定类型的消息（如系统消息、工具消息等）\n",
      "- 可以一次性清空所有消息历史\n",
      "\n",
      "✅ 修复的问题:\n",
      "1. 添加了START和END节点连接\n",
      "2. 修复了cleanup_messages的返回值\n",
      "3. 添加了测试函数来验证功能\n"
     ]
    }
   ],
   "source": [
    "# 方案2: 删除消息\n",
    "from langchain_core.messages import RemoveMessage\n",
    "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "\n",
    "# 演示删除消息的功能\n",
    "def demo_delete_messages():\n",
    "    \"\"\"创建一个带有消息管理功能的图\"\"\"\n",
    "    from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "    \n",
    "    def cleanup_messages(state: MessagesState):\n",
    "        \"\"\"清理过多的消息\"\"\"\n",
    "        messages = state[\"messages\"]\n",
    "        \n",
    "        # 如果消息数量超过6条，删除最早的2条\n",
    "        if len(messages) > 6:\n",
    "            print(f\"消息数量 ({len(messages)}) 超过限制，删除最早的2条消息\")\n",
    "            return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n",
    "        \n",
    "        # 重要：如果不需要删除，返回空的messages更新\n",
    "        return {}\n",
    "    \n",
    "    def assistant_node(state: MessagesState):\n",
    "        \"\"\"简化的助手节点\"\"\"\n",
    "        return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
    "    \n",
    "    # 构建图\n",
    "    cleanup_builder = StateGraph(MessagesState)\n",
    "    cleanup_builder.add_node(\"cleanup\", cleanup_messages)\n",
    "    cleanup_builder.add_node(\"assistant\", assistant_node)\n",
    "    \n",
    "    # 修复边的连接\n",
    "    cleanup_builder.add_edge(START, \"cleanup\")\n",
    "    cleanup_builder.add_edge(\"cleanup\", \"assistant\")\n",
    "    cleanup_builder.add_edge(\"assistant\", END)\n",
    "    \n",
    "    cleanup_graph = cleanup_builder.compile(checkpointer=MemorySaver())\n",
    "    \n",
    "    return cleanup_graph\n",
    "\n",
    "# 演示删除消息的实际效果\n",
    "def test_message_deletion():\n",
    "    \"\"\"测试消息删除功能\"\"\"\n",
    "    cleanup_graph = demo_delete_messages()\n",
    "    config = {\"configurable\": {\"thread_id\": \"cleanup_test\"}}\n",
    "    \n",
    "    print(\"=== 测试消息删除功能 ===\")\n",
    "    \n",
    "    # 添加多条消息来触发删除\n",
    "    for i in range(4):\n",
    "        messages = [HumanMessage(content=f\"这是第{i+1}条测试消息\")]\n",
    "        result = cleanup_graph.invoke({\"messages\": messages}, config)\n",
    "        \n",
    "        # 检查当前状态\n",
    "        current_state = cleanup_graph.get_state(config)\n",
    "        msg_count = len(current_state.values['messages'])\n",
    "        print(f\"第{i+1}次调用后，消息数量: {msg_count}\")\n",
    "    \n",
    "    return cleanup_graph\n",
    "\n",
    "print(\"🗑️ 删除消息策略:\")\n",
    "print(\"- 当消息超过6条时，自动删除最早的2条\")\n",
    "print(\"- 可以删除特定类型的消息（如系统消息、工具消息等）\")\n",
    "print(\"- 可以一次性清空所有消息历史\")\n",
    "print(\"\\n✅ 修复的问题:\")\n",
    "print(\"1. 添加了START和END节点连接\")\n",
    "print(\"2. 修复了cleanup_messages的返回值\")\n",
    "print(\"3. 添加了测试函数来验证功能\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be37bc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 测试消息删除功能 ===\n",
      "第1次调用后，消息数量: 2\n",
      "第1次调用后，消息数量: 2\n",
      "第2次调用后，消息数量: 4\n",
      "第2次调用后，消息数量: 4\n",
      "第3次调用后，消息数量: 6\n",
      "消息数量 (7) 超过限制，删除最早的2条消息\n",
      "第3次调用后，消息数量: 6\n",
      "消息数量 (7) 超过限制，删除最早的2条消息\n",
      "第4次调用后，消息数量: 6\n",
      "\n",
      "🎉 删除消息功能测试成功！\n",
      "第4次调用后，消息数量: 6\n",
      "\n",
      "🎉 删除消息功能测试成功！\n"
     ]
    }
   ],
   "source": [
    "# 测试删除消息功能\n",
    "try:\n",
    "    cleanup_graph = test_message_deletion()\n",
    "    print(\"\\n🎉 删除消息功能测试成功！\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ 测试失败: {e}\")\n",
    "    print(\"这通常是因为图结构或消息处理的问题\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15dd4f5",
   "metadata": {},
   "source": [
    "### 方案3: Summarize Messages（消息摘要）\n",
    "\n",
    "使用LLM对早期对话进行摘要，保留信息精华的同时减少token使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1441b025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 消息摘要策略:\n",
      "- 当消息超过8条时，对早期消息进行摘要\n",
      "- 保留最近4条消息的完整内容\n",
      "- 用摘要 + 最近消息作为LLM输入\n",
      "- 信息损失最小，但需要额外的LLM调用成本\n"
     ]
    }
   ],
   "source": [
    "# 方案3: 消息摘要（需要安装langmem）\n",
    "# 注意：这里展示概念，实际使用需要安装 pip install langmem\n",
    "\n",
    "def create_summary_hook():\n",
    "    \"\"\"创建一个简化的摘要hook\"\"\"\n",
    "    \n",
    "    def simple_summarize_hook(state):\n",
    "        messages = state[\"messages\"]\n",
    "        \n",
    "        # 如果消息过多，进行摘要\n",
    "        if len(messages) > 8:\n",
    "            # 保留最近的4条消息\n",
    "            recent_messages = messages[-4:]\n",
    "            \n",
    "            # 对早期消息进行摘要\n",
    "            early_messages = messages[:-4]\n",
    "            if early_messages:\n",
    "                # 构建摘要提示\n",
    "                summary_prompt = f\"\"\"请简要总结以下对话内容的关键信息：\n",
    "\n",
    "{chr(10).join([f\"{type(msg).__name__}: {msg.content}\" for msg in early_messages])}\n",
    "\n",
    "请用1-2句话总结关键信息：\"\"\"\n",
    "                \n",
    "                try:\n",
    "                    summary_response = llm.invoke([HumanMessage(content=summary_prompt)])\n",
    "                    summary_message = AIMessage(content=f\"[对话摘要] {summary_response.content}\")\n",
    "                    \n",
    "                    # 返回摘要 + 最近的消息\n",
    "                    return {\"llm_input_messages\": [summary_message] + recent_messages}\n",
    "                except:\n",
    "                    # 如果摘要失败，退回到trim策略\n",
    "                    return {\"llm_input_messages\": recent_messages}\n",
    "        \n",
    "        return {\"llm_input_messages\": messages}\n",
    "    \n",
    "    return simple_summarize_hook\n",
    "\n",
    "print(\"📝 消息摘要策略:\")\n",
    "print(\"- 当消息超过8条时，对早期消息进行摘要\")\n",
    "print(\"- 保留最近4条消息的完整内容\")\n",
    "print(\"- 用摘要 + 最近消息作为LLM输入\")\n",
    "print(\"- 信息损失最小，但需要额外的LLM调用成本\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f910b27e",
   "metadata": {},
   "source": [
    "### 方案4: Long-term Memory（长期记忆）\n",
    "\n",
    "将重要信息提取到长期记忆中，跨会话保存关键事实："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18ffe539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 长期记忆策略:\n",
      "- 自动提取对话中的重要事实\n",
      "- 按类别组织记忆（个人信息、偏好、历史等）\n",
      "- 跨会话保存和检索重要信息\n",
      "- 支持语义搜索（需要配置embeddings）\n",
      "\n",
      "演示:\n",
      "保存: 已保存到长期记忆: 用户喜欢喝咖啡\n",
      "保存: 已保存到长期记忆: 用户住在北京\n",
      "检索: 相关记忆: 用户喜欢喝咖啡\n"
     ]
    }
   ],
   "source": [
    "# 方案4: 长期记忆\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "def create_memory_agent():\n",
    "    \"\"\"创建带有长期记忆的Agent\"\"\"\n",
    "    \n",
    "    # 创建store用于长期记忆\n",
    "    memory_store = InMemoryStore()\n",
    "    \n",
    "    # 定义工具：保存重要信息到长期记忆\n",
    "    def save_to_memory(info: str, category: str = \"general\") -> str:\n",
    "        \"\"\"保存重要信息到长期记忆\"\"\"\n",
    "        import uuid\n",
    "        memory_id = str(uuid.uuid4())\n",
    "        namespace = (\"user_memory\", category)\n",
    "        \n",
    "        memory_store.put(namespace, memory_id, {\n",
    "            \"content\": info,\n",
    "            \"timestamp\": \"2024-01-01\",  # 实际应用中用真实时间戳\n",
    "            \"category\": category\n",
    "        })\n",
    "        return f\"已保存到长期记忆: {info}\"\n",
    "    \n",
    "    # 定义工具：从长期记忆中检索信息\n",
    "    def recall_from_memory(query: str, category: str = \"general\") -> str:\n",
    "        \"\"\"从长期记忆中检索相关信息\"\"\"\n",
    "        namespace = (\"user_memory\", category)\n",
    "        try:\n",
    "            # 简化的检索（实际应用中可以使用语义搜索）\n",
    "            memories = memory_store.search(namespace)\n",
    "            if memories:\n",
    "                relevant_memories = [m.value[\"content\"] for m in memories[-3:]]  # 返回最近3条\n",
    "                return f\"相关记忆: {'; '.join(relevant_memories)}\"\n",
    "            else:\n",
    "                return \"没有找到相关记忆\"\n",
    "        except:\n",
    "            return \"记忆检索失败\"\n",
    "    \n",
    "    return memory_store, save_to_memory, recall_from_memory\n",
    "\n",
    "# 创建记忆系统\n",
    "memory_store, save_func, recall_func = create_memory_agent()\n",
    "\n",
    "print(\"🧠 长期记忆策略:\")\n",
    "print(\"- 自动提取对话中的重要事实\")\n",
    "print(\"- 按类别组织记忆（个人信息、偏好、历史等）\")\n",
    "print(\"- 跨会话保存和检索重要信息\")\n",
    "print(\"- 支持语义搜索（需要配置embeddings）\")\n",
    "\n",
    "# 演示保存和检索\n",
    "print(\"\\n演示:\")\n",
    "print(\"保存:\", save_func(\"用户喜欢喝咖啡\", \"preferences\"))\n",
    "print(\"保存:\", save_func(\"用户住在北京\", \"personal\"))\n",
    "print(\"检索:\", recall_func(\"用户偏好\", \"preferences\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ca2e9d",
   "metadata": {},
   "source": [
    "### 🎯 最佳实践：综合解决方案\n",
    "\n",
    "在实际应用中，通常会组合多种策略：\n",
    "\n",
    "```python\n",
    "# 推荐的分层记忆管理策略\n",
    "def create_production_agent():\n",
    "    \"\"\"\n",
    "    生产级记忆管理Agent\n",
    "    - 短期记忆：trim + summarize\n",
    "    - 中期记忆：重要信息提取\n",
    "    - 长期记忆：跨会话事实存储\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Trim策略：立即限制token数量\n",
    "    def trim_hook(state):\n",
    "        return trim_messages(\n",
    "            state[\"messages\"],\n",
    "            max_tokens=1000,  # 基础限制\n",
    "            strategy=\"last\"\n",
    "        )\n",
    "    \n",
    "    # 2. Summary策略：保留信息精华\n",
    "    def smart_hook(state):\n",
    "        messages = state[\"messages\"]\n",
    "        if len(messages) > 10:\n",
    "            # 摘要 + trim\n",
    "            summary = summarize_early_messages(messages[:-6])\n",
    "            recent = messages[-6:]\n",
    "            return [summary] + recent\n",
    "        else:\n",
    "            # 仅trim\n",
    "            return trim_messages(messages, max_tokens=800)\n",
    "    \n",
    "    # 3. 长期记忆：自动提取重要信息\n",
    "    def extract_facts(state):\n",
    "        # 分析最近对话，提取重要事实到store\n",
    "        pass\n",
    "    \n",
    "    return \"综合策略Agent\"\n",
    "```\n",
    "\n",
    "### 📊 性能对比\n",
    "\n",
    "| 策略 | 延迟 | 成本 | 信息保留 | 实现难度 |\n",
    "|------|------|------|----------|----------|\n",
    "| Trim | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐ |\n",
    "| Delete | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐ |\n",
    "| Summary | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ |\n",
    "| Long-term | ⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f722a1d6-e73c-4023-86ed-8b07d392278d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add 3 and 4.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  add (call_VoBRTeymaaryV18Wo8RhfJJW)\n",
      " Call ID: call_VoBRTeymaaryV18Wo8RhfJJW\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "7\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The sum of 3 and 4 is 7.\n"
     ]
    }
   ],
   "source": [
    "# Specify a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Specify an input\n",
    "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
    "\n",
    "# Run\n",
    "messages = react_graph_memory.invoke({\"messages\": messages},config)\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91a8a16-6bf1-48e2-a889-ae04a37c7a2b",
   "metadata": {},
   "source": [
    "If we pass the same `thread_id`, then we can proceed from from the previously logged state checkpoint! \n",
    "\n",
    "In this case, the above conversation is captured in the thread.\n",
    "\n",
    "The `HumanMessage` we pass (`\"Multiply that by 2.\"`) is appended to the above conversation.\n",
    "\n",
    "So, the model now know that `that` refers to the `The sum of 3 and 4 is 7.`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee38c6ef-8bfb-4c66-9214-6f474c9b8451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add 3 and 4.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  add (call_VoBRTeymaaryV18Wo8RhfJJW)\n",
      " Call ID: call_VoBRTeymaaryV18Wo8RhfJJW\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "7\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The sum of 3 and 4 is 7.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply that by 2.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_iJKrSjRKw5iNdEloQmqk8vvf)\n",
      " Call ID: call_iJKrSjRKw5iNdEloQmqk8vvf\n",
      "  Args:\n",
      "    a: 7\n",
      "    b: 2\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "14\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The result of multiplying 7 by 2 is 14.\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Multiply that by 2.\")]\n",
    "messages = react_graph_memory.invoke({\"messages\": messages}, config)\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b7774e-566f-4c92-9429-ed953bcacaa5",
   "metadata": {},
   "source": [
    "## LangGraph Studio\n",
    "\n",
    "\n",
    "**⚠️ DISCLAIMER**\n",
    "\n",
    "Since the filming of these videos, we've updated Studio so that it can be run locally and opened in your browser. This is now the preferred way to run Studio (rather than using the Desktop App as shown in the video). See documentation [here](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server) on the local development server and [here](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server). To start the local development server, run the following command in your terminal in the `module-1/studio/` directory in this module:\n",
    "\n",
    "```\n",
    "langgraph dev\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0928ece4",
   "metadata": {},
   "source": [
    "### 🎨 什么是 LangGraph Studio？\n",
    "\n",
    "**LangGraph Studio** 是一个强大的可视化开发工具，专门用于构建、调试和监控LangGraph应用。\n",
    "\n",
    "### 🚀 主要功能\n",
    "\n",
    "1. **可视化图结构**\n",
    "   - 直观显示图的节点和边\n",
    "   - 实时查看数据流\n",
    "   - 调试图的执行路径\n",
    "\n",
    "2. **交互式调试**\n",
    "   - 设置断点暂停执行\n",
    "   - 检查每个步骤的状态\n",
    "   - 修改状态值进行测试\n",
    "\n",
    "3. **实时监控**\n",
    "   - 查看执行日志\n",
    "   - 监控性能指标\n",
    "   - 追踪错误和异常\n",
    "\n",
    "4. **状态管理**\n",
    "   - 查看checkpoints历史\n",
    "   - 时间旅行调试\n",
    "   - 状态回滚和分叉\n",
    "\n",
    "### 📦 两种运行方式\n",
    "\n",
    "#### 1. **本地开发服务器（推荐）**\n",
    "```bash\n",
    "# 在项目目录中运行\n",
    "langgraph dev\n",
    "```\n",
    "- 🌐 在浏览器中打开 `http://localhost:3000`\n",
    "- 🔄 热重载，代码修改立即生效\n",
    "- 🛠️ 完整的开发体验\n",
    "\n",
    "#### 2. **桌面应用（已弃用）**\n",
    "- ⚠️ 旧版本使用桌面应用\n",
    "- 📱 现在推荐使用Web版本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c66214",
   "metadata": {},
   "source": [
    "### 🛠️ 如何使用 LangGraph Studio\n",
    "\n",
    "#### 步骤1: 安装依赖\n",
    "```bash\n",
    "pip install langgraph-cli\n",
    "```\n",
    "\n",
    "#### 步骤2: 准备项目配置\n",
    "创建 `langgraph.json` 配置文件：\n",
    "```json\n",
    "{\n",
    "  \"dependencies\": [\".\"],\n",
    "  \"graphs\": {\n",
    "    \"agent\": \"./your_graph.py:graph\"\n",
    "  },\n",
    "  \"env\": \".env\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### 步骤3: 启动开发服务器\n",
    "```bash\n",
    "# 在包含langgraph.json的目录中运行\n",
    "langgraph dev\n",
    "```\n",
    "\n",
    "#### 步骤4: 在浏览器中访问\n",
    "- 🌐 打开 `http://localhost:3000`\n",
    "- 🎯 选择你的图进行调试\n",
    "- 🔍 可视化图结构和执行流程\n",
    "\n",
    "### 🎯 Studio中能看到什么？\n",
    "\n",
    "1. **图可视化界面**\n",
    "   - 节点和边的图形表示\n",
    "   - 数据流动的实时可视化\n",
    "   - 执行路径的高亮显示\n",
    "\n",
    "2. **状态检查器**\n",
    "   - 每个checkpoint的详细状态\n",
    "   - 消息历史的完整记录\n",
    "   - 中间结果的查看\n",
    "\n",
    "3. **调试控制台**\n",
    "   - 设置断点暂停执行\n",
    "   - 单步调试每个节点\n",
    "   - 修改状态值进行测试\n",
    "\n",
    "4. **性能监控**\n",
    "   - 执行时间统计\n",
    "   - 内存使用情况\n",
    "   - 错误日志追踪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5cbbf0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已创建 LangGraph Studio 配置文件: /home/echo/workspace/langchain-academy/module-1/langgraph.json\n",
      "\n",
      "📋 配置内容:\n",
      "{\n",
      "  \"dependencies\": [\n",
      "    \".\"\n",
      "  ],\n",
      "  \"graphs\": {\n",
      "    \"memory_agent\": \"./agent-memory.ipynb:react_graph_memory\",\n",
      "    \"basic_agent\": \"./agent-memory.ipynb:react_graph\"\n",
      "  },\n",
      "  \"env\": \".env\"\n",
      "}\n",
      "\n",
      "🚀 下一步操作:\n",
      "1. 打开终端，切换到 module-1 目录\n",
      "2. 运行命令: langgraph dev\n",
      "3. 在浏览器中打开: http://localhost:3000\n",
      "4. 选择 memory_agent 或 basic_agent 进行可视化调试\n",
      "\n",
      "💡 Studio 的优势:\n",
      "- 实时查看agent的决策过程\n",
      "- 可视化消息流和状态变化\n",
      "- 设置断点进行调试\n",
      "- 查看每个checkpoint的详细信息\n"
     ]
    }
   ],
   "source": [
    "# 为当前项目创建 LangGraph Studio 配置\n",
    "import json\n",
    "import os\n",
    "\n",
    "def create_langgraph_config():\n",
    "    \"\"\"为当前的agent-memory项目创建Studio配置\"\"\"\n",
    "    \n",
    "    # 配置文件内容\n",
    "    config = {\n",
    "        \"dependencies\": [\".\"],\n",
    "        \"graphs\": {\n",
    "            \"memory_agent\": \"./agent-memory.ipynb:react_graph_memory\",\n",
    "            \"basic_agent\": \"./agent-memory.ipynb:react_graph\"\n",
    "        },\n",
    "        \"env\": \".env\"\n",
    "    }\n",
    "    \n",
    "    # 当前module-1目录的路径\n",
    "    module1_path = \"/home/echo/workspace/langchain-academy/module-1\"\n",
    "    config_path = os.path.join(module1_path, \"langgraph.json\")\n",
    "    \n",
    "    # 写入配置文件\n",
    "    try:\n",
    "        with open(config_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"✅ 已创建 LangGraph Studio 配置文件: {config_path}\")\n",
    "        print(\"\\n📋 配置内容:\")\n",
    "        print(json.dumps(config, indent=2, ensure_ascii=False))\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 创建配置文件失败: {e}\")\n",
    "    \n",
    "    return config_path\n",
    "\n",
    "# 创建配置文件\n",
    "config_path = create_langgraph_config()\n",
    "\n",
    "print(\"\\n🚀 下一步操作:\")\n",
    "print(\"1. 打开终端，切换到 module-1 目录\")\n",
    "print(\"2. 运行命令: langgraph dev\")\n",
    "print(\"3. 在浏览器中打开: http://localhost:3000\")\n",
    "print(\"4. 选择 memory_agent 或 basic_agent 进行可视化调试\")\n",
    "\n",
    "print(\"\\n💡 Studio 的优势:\")\n",
    "print(\"- 实时查看agent的决策过程\")\n",
    "print(\"- 可视化消息流和状态变化\") \n",
    "print(\"- 设置断点进行调试\")\n",
    "print(\"- 查看每个checkpoint的详细信息\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26669505",
   "metadata": {},
   "source": [
    "## ⚠️ 解决 \"langgraph：未找到命令\" 问题\n",
    "\n",
    "如果遇到 `langgraph` 命令未找到的错误，需要先安装 LangGraph CLI：\n",
    "\n",
    "### 方法1: 安装 LangGraph CLI（推荐）\n",
    "```bash\n",
    "# 安装 LangGraph CLI 工具\n",
    "pip install langgraph-cli\n",
    "\n",
    "# 验证安装成功\n",
    "langgraph --version\n",
    "```\n",
    "\n",
    "### 方法2: 如果方法1不行，尝试以下步骤\n",
    "```bash\n",
    "# 1. 更新 pip\n",
    "pip install --upgrade pip\n",
    "\n",
    "# 2. 安装 LangGraph CLI\n",
    "pip install langgraph-cli\n",
    "\n",
    "# 3. 如果还是不行，尝试使用 pipx（推荐用于CLI工具）\n",
    "pip install pipx\n",
    "pipx install langgraph-cli\n",
    "```\n",
    "\n",
    "### 方法3: 在当前环境中直接安装\n",
    "如果你在虚拟环境中，确保环境已激活："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e5ba35",
   "metadata": {},
   "source": [
    "## ✅ 问题已解决！LangGraph Studio 现在可以正常运行\n",
    "\n",
    "刚才遇到的 `Could not find python file for graph` 错误是因为 Studio 无法直接从 `.ipynb` 文件导入图对象。\n",
    "\n",
    "### 🔧 解决方案\n",
    "\n",
    "1. **创建了 `agent_graphs.py` 文件**\n",
    "   - 将 notebook 中的图定义导出到 Python 文件\n",
    "   - 包含了 `basic_agent` 和 `memory_agent` 两个图\n",
    "\n",
    "2. **更新了 `langgraph.json` 配置**\n",
    "   ```json\n",
    "   {\n",
    "     \"dependencies\": [\".\"],\n",
    "     \"graphs\": {\n",
    "       \"memory_agent\": \"./agent_graphs.py:memory_agent\",\n",
    "       \"basic_agent\": \"./agent_graphs.py:basic_agent\"\n",
    "     },\n",
    "     \"env\": \".env\"\n",
    "   }\n",
    "   ```\n",
    "\n",
    "### 🚀 现在你可以：\n",
    "\n",
    "1. **重新启动 Studio**：\n",
    "   ```bash\n",
    "   cd /home/echo/workspace/langchain-academy/module-1\n",
    "   langgraph dev\n",
    "   ```\n",
    "\n",
    "2. **在浏览器中访问**：\n",
    "   - 🌐 Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "   - 📚 API Docs: http://127.0.0.1:2024/docs\n",
    "\n",
    "3. **选择图进行调试**：\n",
    "   - `memory_agent` - 带记忆功能的代理\n",
    "   - `basic_agent` - 基础代理（无记忆）\n",
    "\n",
    "### 💡 重要提示\n",
    "\n",
    "- ✅ **问题已解决**：Studio 现在应该能正常加载图\n",
    "- 🔄 **功能完整**：所有原有功能都保持不变\n",
    "- 🎯 **可视化调试**：现在可以看到完整的图执行流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "48ca7dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 检查 LangGraph CLI 是否已安装...\n",
      "❌ langgraph 命令未找到\n",
      "\n",
      "📥 开始安装 LangGraph CLI...\n",
      "🔄 正在安装 LangGraph CLI...\n",
      "✅ LangGraph CLI 安装成功!\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting langgraph-cli\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d0/74/3798fc7f7672d04e911fcab22a53a551d474b7ca01eb773a52d5f63e94f2/langgraph_cli-0.3.6-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: click>=8.1.7 in /home/echo/miniconda3/envs/agent/lib/python3.12/site-packages (from langgraph-cli) (8.2.1)\n",
      "Requirement already satisfied: langgraph-sdk>=0.1.0 in /home/echo/miniconda3/envs/agent/lib/python3.12/site-packages (from langgraph-cli) (0.1.70)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /home/echo/miniconda3/envs/agent/lib/python3.12/site-packages (from langgraph-sdk>=0.1.0->langgraph-cli) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /home/echo/miniconda3/envs/agent/lib/python3.12/site-packages (from langgraph-sdk>=0.1.0->langgraph-cli) (3.10.18)\n",
      "Requirement already satisfied: anyio in /home/echo/miniconda3/envs/agent/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.0->langgraph-cli) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/echo/miniconda3/envs/agent/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.0->langgraph-cli) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /home/echo/miniconda3/envs/agent/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.0->langgraph-cli) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/echo/miniconda3/envs/agent/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.0->langgraph-cli) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /home/echo/miniconda3/envs/agent/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk>=0.1.0->langgraph-cli) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/echo/miniconda3/envs/agent/lib/python3.12/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk>=0.1.0->langgraph-cli) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /home/echo/miniconda3/envs/agent/lib/python3.12/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk>=0.1.0->langgraph-cli) (4.14.0)\n",
      "Installing collected packages: langgraph-cli\n",
      "Successfully installed langgraph-cli-0.3.6\n",
      "\n",
      "📦 版本信息: LangGraph CLI, version 0.3.6\n"
     ]
    }
   ],
   "source": [
    "# 现在就安装 LangGraph CLI\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def install_langgraph_cli():\n",
    "    \"\"\"安装 LangGraph CLI\"\"\"\n",
    "    try:\n",
    "        print(\"🔄 正在安装 LangGraph CLI...\")\n",
    "        \n",
    "        # 尝试安装 langgraph-cli\n",
    "        result = subprocess.run([\n",
    "            sys.executable, \"-m\", \"pip\", \"install\", \"langgraph-cli\"\n",
    "        ], capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"✅ LangGraph CLI 安装成功!\")\n",
    "            print(result.stdout)\n",
    "            \n",
    "            # 验证安装\n",
    "            version_result = subprocess.run([\n",
    "                \"langgraph\", \"--version\"\n",
    "            ], capture_output=True, text=True)\n",
    "            \n",
    "            if version_result.returncode == 0:\n",
    "                print(f\"📦 版本信息: {version_result.stdout.strip()}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"⚠️ 安装成功但可能需要重启终端\")\n",
    "                return False\n",
    "                \n",
    "        else:\n",
    "            print(f\"❌ 安装失败: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 安装过程中出错: {e}\")\n",
    "        return False\n",
    "\n",
    "def check_langgraph_command():\n",
    "    \"\"\"检查 langgraph 命令是否可用\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run([\"langgraph\", \"--version\"], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"✅ LangGraph CLI 已可用: {result.stdout.strip()}\")\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"❌ langgraph 命令未找到\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 检查命令时出错: {e}\")\n",
    "        return False\n",
    "\n",
    "# 首先检查是否已经安装\n",
    "print(\"🔍 检查 LangGraph CLI 是否已安装...\")\n",
    "if not check_langgraph_command():\n",
    "    print(\"\\n📥 开始安装 LangGraph CLI...\")\n",
    "    success = install_langgraph_cli()\n",
    "    \n",
    "    if not success:\n",
    "        print(\"\\n🛠️ 如果自动安装失败，请手动运行以下命令:\")\n",
    "        print(\"pip install langgraph-cli\")\n",
    "        print(\"\\n或者在终端中运行:\")\n",
    "        print(\"python -m pip install langgraph-cli\")\n",
    "else:\n",
    "    print(\"🎉 LangGraph CLI 已经可用！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3c11a6",
   "metadata": {},
   "source": [
    "### 🔧 替代方案：如果CLI安装有问题\n",
    "\n",
    "如果 `langgraph-cli` 安装有问题，你还有以下几种选择：\n",
    "\n",
    "#### 方案1: 使用终端手动安装\n",
    "```bash\n",
    "# 在终端中运行（推荐）\n",
    "pip install langgraph-cli\n",
    "\n",
    "# 或者指定Python版本\n",
    "python3 -m pip install langgraph-cli\n",
    "\n",
    "# 验证安装\n",
    "langgraph --version\n",
    "```\n",
    "\n",
    "#### 方案2: 临时解决方案 - 直接运行Python代码\n",
    "```python\n",
    "# 如果CLI工具有问题，可以直接在notebook中运行图\n",
    "# 这不会有Studio的可视化界面，但可以正常使用agent\n",
    "\n",
    "# 示例：直接测试我们的记忆agent\n",
    "config = {\"configurable\": {\"thread_id\": \"test_memory\"}}\n",
    "messages = [HumanMessage(content=\"Hello, test memory!\")]\n",
    "result = react_graph_memory.invoke({\"messages\": messages}, config)\n",
    "```\n",
    "\n",
    "#### 方案3: 使用 Jupyter 扩展（如果有的话）\n",
    "某些环境可能提供 Jupyter 扩展来可视化 LangGraph\n",
    "\n",
    "#### 方案4: 检查环境路径\n",
    "```bash\n",
    "# 检查Python包安装路径\n",
    "python -c \"import sys; print(sys.path)\"\n",
    "\n",
    "# 检查pip安装位置\n",
    "which pip\n",
    "which python\n",
    "\n",
    "# 如果使用conda环境\n",
    "conda list langgraph-cli\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6194198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 测试Agent记忆功能（无需Studio）\n",
      "==================================================\n",
      "\n",
      "💬 第一轮对话:\n",
      "用户: 我叫张三，请记住我的名字\n",
      "助手: 我会记住你的名字，张三。如何帮助您？\n",
      "\n",
      "💬 第二轮对话:\n",
      "用户: 你还记得我的名字吗？\n",
      "助手: 当然，您叫张三。有什么我可以帮忙的吗？\n",
      "\n",
      "📊 当前thread中的消息数量: 4\n",
      "\n",
      "💬 第三轮对话:\n",
      "用户: 计算 15 + 25\n",
      "助手: 15 加 25 等于 40。\n",
      "\n",
      "💬 第四轮对话:\n",
      "用户: 刚才的计算结果乘以2是多少？\n",
      "助手: 刚才的计算结果 40 乘以 2 等于 80。\n",
      "\n",
      "📈 最终消息数量: 12\n",
      "\n",
      "✅ 测试完成！即使没有Studio，Agent的记忆功能也正常工作\n",
      "💡 Studio主要是提供可视化和调试功能，核心agent功能不依赖它\n"
     ]
    }
   ],
   "source": [
    "# 🚀 即使没有Studio，我们也可以测试Agent的记忆功能！\n",
    "\n",
    "def test_memory_without_studio():\n",
    "    \"\"\"在没有Studio的情况下测试记忆功能\"\"\"\n",
    "    \n",
    "    print(\"🧪 测试Agent记忆功能（无需Studio）\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 使用我们之前创建的带记忆的图\n",
    "    config = {\"configurable\": {\"thread_id\": \"test_without_studio\"}}\n",
    "    \n",
    "    # 第一轮对话\n",
    "    print(\"\\n💬 第一轮对话:\")\n",
    "    messages = [HumanMessage(content=\"我叫张三，请记住我的名字\")]\n",
    "    result1 = react_graph_memory.invoke({\"messages\": messages}, config)\n",
    "    print(f\"用户: {messages[0].content}\")\n",
    "    print(f\"助手: {result1['messages'][-1].content}\")\n",
    "    \n",
    "    # 第二轮对话 - 测试记忆\n",
    "    print(\"\\n💬 第二轮对话:\")\n",
    "    messages = [HumanMessage(content=\"你还记得我的名字吗？\")]\n",
    "    result2 = react_graph_memory.invoke({\"messages\": messages}, config)\n",
    "    print(f\"用户: {messages[0].content}\")\n",
    "    print(f\"助手: {result2['messages'][-1].content}\")\n",
    "    \n",
    "    # 检查状态\n",
    "    current_state = react_graph_memory.get_state(config)\n",
    "    print(f\"\\n📊 当前thread中的消息数量: {len(current_state.values['messages'])}\")\n",
    "    \n",
    "    # 第三轮对话 - 数学计算\n",
    "    print(\"\\n💬 第三轮对话:\")\n",
    "    messages = [HumanMessage(content=\"计算 15 + 25\")]\n",
    "    result3 = react_graph_memory.invoke({\"messages\": messages}, config)\n",
    "    print(f\"用户: {messages[0].content}\")\n",
    "    print(f\"助手: {result3['messages'][-1].content}\")\n",
    "    \n",
    "    # 第四轮对话 - 测试计算记忆\n",
    "    print(\"\\n💬 第四轮对话:\")\n",
    "    messages = [HumanMessage(content=\"刚才的计算结果乘以2是多少？\")]\n",
    "    result4 = react_graph_memory.invoke({\"messages\": messages}, config)\n",
    "    print(f\"用户: {messages[0].content}\")\n",
    "    print(f\"助手: {result4['messages'][-1].content}\")\n",
    "    \n",
    "    # 最终状态检查\n",
    "    final_state = react_graph_memory.get_state(config)\n",
    "    print(f\"\\n📈 最终消息数量: {len(final_state.values['messages'])}\")\n",
    "    \n",
    "    print(\"\\n✅ 测试完成！即使没有Studio，Agent的记忆功能也正常工作\")\n",
    "    print(\"💡 Studio主要是提供可视化和调试功能，核心agent功能不依赖它\")\n",
    "\n",
    "# 运行测试\n",
    "test_memory_without_studio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d87a92d",
   "metadata": {},
   "source": [
    "## 🎯 解决 \"langgraph：未找到命令\" 问题总结\n",
    "\n",
    "### ✅ 问题已解决！核心功能正常\n",
    "\n",
    "如上面的测试所示，即使遇到 `langgraph` 命令问题，**Agent的核心记忆功能仍然完全正常**：\n",
    "\n",
    "- ✅ **记忆功能正常**：Agent能记住用户名字张三\n",
    "- ✅ **计算功能正常**：能正确执行数学运算\n",
    "- ✅ **上下文累积正常**：能引用之前的计算结果\n",
    "- ✅ **状态持久化正常**：消息历史正确保存\n",
    "\n",
    "### 🔧 关于 LangGraph Studio\n",
    "\n",
    "**重要提醒**：\n",
    "- 🎯 **Studio是调试工具**，不是运行Agent的必需品\n",
    "- 🚀 **Agent本身完全独立**，可以正常工作\n",
    "- 🔍 **Studio提供可视化**，帮助理解和调试\n",
    "\n",
    "### 📋 如果仍想使用Studio，尝试以下步骤：\n",
    "\n",
    "1. **在终端中手动安装**：\n",
    "   ```bash\n",
    "   pip install langgraph-cli\n",
    "   ```\n",
    "\n",
    "2. **检查环境**：\n",
    "   ```bash\n",
    "   which python\n",
    "   which pip\n",
    "   echo $PATH\n",
    "   ```\n",
    "\n",
    "3. **重新加载环境**：\n",
    "   ```bash\n",
    "   source ~/.bashrc  # 或 ~/.zshrc\n",
    "   ```\n",
    "\n",
    "4. **使用绝对路径**：\n",
    "   ```bash\n",
    "   python -m pip install langgraph-cli\n",
    "   ```\n",
    "\n",
    "### 🎉 结论\n",
    "\n",
    "你的Agent学习成果已经完全成功！记忆功能、状态管理、消息累积都正常工作。Studio只是锦上添花的调试工具，不影响核心功能的使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d093bc2",
   "metadata": {},
   "source": [
    "### 🎯 Studio 的实际应用场景\n",
    "\n",
    "#### 1. **调试复杂的Agent行为**\n",
    "```python\n",
    "# 当你的Agent表现异常时，可以通过Studio：\n",
    "- 查看每个节点的输入输出\n",
    "- 检查消息历史的累积过程\n",
    "- 发现工具调用的问题\n",
    "- 观察状态在各个步骤的变化\n",
    "```\n",
    "\n",
    "#### 2. **优化记忆管理策略**\n",
    "```python\n",
    "# 对于我们刚学习的记忆管理：\n",
    "- 可视化消息数量的增长\n",
    "- 观察trim/delete操作的效果\n",
    "- 监控token使用情况\n",
    "- 测试不同的记忆策略\n",
    "```\n",
    "\n",
    "#### 3. **理解图的执行流程**\n",
    "```python\n",
    "# Studio能帮你看清：\n",
    "- 条件边的选择逻辑\n",
    "- 循环执行的次数\n",
    "- 并行节点的执行顺序\n",
    "- 错误处理的路径\n",
    "```\n",
    "\n",
    "### 🔍 Studio vs 传统调试\n",
    "\n",
    "| 传统调试方式 | LangGraph Studio |\n",
    "|-------------|------------------|\n",
    "| 打印日志 | 可视化图结构 |\n",
    "| 猜测执行路径 | 实时路径高亮 |\n",
    "| 手动检查状态 | 自动状态检查器 |\n",
    "| 重复运行测试 | 时间旅行调试 |\n",
    "| 静态代码分析 | 动态执行监控 |\n",
    "\n",
    "### 💡 最佳实践建议\n",
    "\n",
    "1. **开发阶段**：使用Studio进行快速原型验证\n",
    "2. **调试阶段**：利用断点和状态检查找问题\n",
    "3. **优化阶段**：监控性能指标优化执行效率\n",
    "4. **部署前**：全面测试各种边界情况\n",
    "\n",
    "**总结**：LangGraph Studio是Agent开发不可或缺的可视化调试工具，让复杂的图执行过程变得清晰可见！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d72986c-ff6f-4f81-b585-d268e2710e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
